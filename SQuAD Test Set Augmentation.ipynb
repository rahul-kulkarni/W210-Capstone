{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# from transformers import BertForQuestionAnswering, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "# from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "import timeit\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contractions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "# Required for identifying parts of speech. \n",
    "#TODO should we do this using Bert based model?\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use BERT or RoBERTa models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'roberta'\n",
    "\n",
    "# Download pre-trained models from huggingface\n",
    "if model_type == 'bert':\n",
    "    # Download (Using cased to maintain case in output)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-large-cased')\n",
    "    model_mask = '[MASK]'\n",
    "    \n",
    "elif model_type == 'roberta':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "    model_mask = '<mask>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_roberta.RobertaForMaskedLM'>\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_word_count_file = \"/data/distribution_shift/enwiki-20190320-words-frequency.txt\"\n",
    "\n",
    "wiki_freq_dict = {}\n",
    "\n",
    "with open(wiki_word_count_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word_count = line.split(' ')\n",
    "        wiki_freq_dict[word_count[0]] = int(word_count[1][:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_word_prediction(text):\n",
    "    # Text is already masked.  Find masked words, predict replacements and replace them.  \n",
    "    global model_type\n",
    "    predicted_words = []\n",
    "    full_paragraph = ''\n",
    "    \n",
    "    text_token_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    tokenized_text = tokenizer.tokenize(text, return_tensors='pt')\n",
    "\n",
    "    # Nested function so we can break paragraph in to parts when > 512 tokens long. \n",
    "    def predict_part(text_token_ids_part, tokenized_text_part):\n",
    "        # Get and format positions of work masks\n",
    "        mask_positions_2d = (text_token_ids_part.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "        mask_positions = [mask.item() for mask in mask_positions_2d ]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(text_token_ids_part)\n",
    "\n",
    "        last_hidden_state = output[0].squeeze()\n",
    "\n",
    "        token_predictions_list =[]\n",
    "        for mask_index in mask_positions:\n",
    "            mask_hidden_state = last_hidden_state[mask_index]\n",
    "            # This isn't really required unless we want > 1 predicted word per mask. \n",
    "            idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n",
    "\n",
    "            # The different models tokenize differently, so handle based on model type. \n",
    "            words = None\n",
    "            if model_type == 'roberta':\n",
    "                words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "            elif model_type == 'bert':\n",
    "                words = tokenizer.decode(idx).split(' ')\n",
    "                \n",
    "            for i in range(len(words)):\n",
    "                if (words[i][0:2] == '##'):\n",
    "                    words[i] = words[i][2:]\n",
    "\n",
    "            token_predictions_list.append(words)\n",
    "\n",
    "    \n",
    "        # Make sure we have a list of predictions for each mask. \n",
    "        assert len(mask_positions) == len(token_predictions_list)\n",
    "\n",
    "        # Replace masks with predicted words\n",
    "        #   Make a copy so we can calculate shape differences, required for answer_start in QnA.\n",
    "        predicted_text = copy(tokenized_text_part) \n",
    "        for pos, new_word in zip(mask_positions, token_predictions_list):\n",
    "\n",
    "            #TODO This is where we could search for a more appropriate replacement. \n",
    "            # Add the weird G at the front of predicted words, so they look like recognized tokens for the \n",
    "            # detokenization logic used in 'detokenize()'. \n",
    "            if model_type == 'roberta':\n",
    "                predicted_word = 'Ġ'+new_word[0]\n",
    "            else:\n",
    "                predicted_word = new_word[0]\n",
    "\n",
    "            # Ugly hack, not sure why this goes askew\n",
    "            if predicted_text[pos-2] == \"<mask>\":\n",
    "                predicted_text[pos-2] = predicted_word\n",
    "            else:\n",
    "                predicted_text[pos-1] = predicted_word\n",
    "            predicted_words.append(new_word[0])\n",
    "\n",
    "        return detokenize(predicted_text)\n",
    "    \n",
    "\n",
    "    # Split text in multiple parts if text is longer than 512 tokens. \n",
    "    if len(tokenized_text) > 512:\n",
    "        # Let's figure out how many splits.\n",
    "        start_pos = 0\n",
    "        end_pos = 0\n",
    "        piece_size = 0\n",
    "        pieces = 0\n",
    "        \n",
    "        for i in range(100):\n",
    "            denom = i + 1\n",
    "            if int(len(tokenized_text)/denom) < 500:\n",
    "                piece_size = int(len(tokenized_text)/denom)\n",
    "                pieces = denom\n",
    "                break\n",
    "        \n",
    "#         print(\"Going to break the para in to {} pieces. End: {}\".format(pieces, end_pos))\n",
    "        \n",
    "        for i in range(pieces):\n",
    "            end_pos = piece_size * (i+1)\n",
    "            \n",
    "#             print(\"Doing piece {} Start: {} End: {}\".format(i+1, start_pos, end_pos))\n",
    "            sized_text = []\n",
    "            split_idx = 0\n",
    "            \n",
    "            if i == (pieces-1):\n",
    "                # Grab to the end. \n",
    "                split_idx = None\n",
    "            else:\n",
    "                # Look for the first sentence ending after the split position. \n",
    "                split_idx = end_pos\n",
    "                for idx, token in enumerate(tokenized_text[end_pos:], start=end_pos):\n",
    "                    if token in '.!?' or (idx-end_pos) >= 512:\n",
    "                        split_idx = idx+1\n",
    "                        break\n",
    "            \n",
    "#             print(tokenized_text[start_pos:split_idx])\n",
    "#             print(\"start_pos: {} split_idx: {} end_pos: {}\".format(start_pos,split_idx, end_pos))\n",
    "            full_paragraph += predict_part(text_token_ids[0:1,start_pos:split_idx], tokenized_text[start_pos:split_idx])\n",
    "            start_pos = split_idx\n",
    "    else:\n",
    "        full_paragraph += predict_part(text_token_ids, tokenized_text)\n",
    "        \n",
    "    if full_paragraph.find(\"\\<mask\\>\") >= 0:\n",
    "        print(full_paragraph)\n",
    "        \n",
    "    return full_paragraph, predicted_words\n",
    "\n",
    "def detokenize(tokenized_text):\n",
    "    global model_type\n",
    "    postfix_symbols=\"?:!.,;%)\"\n",
    "    prefix_symbols=\"($\"\n",
    "    \n",
    "    detokenized_sentence=''\n",
    "    for idx, token in enumerate(tokenized_text):\n",
    "        if idx == 0:\n",
    "            # Just to ensure we can assume text exists in detokenized_sentence below.  \n",
    "            detokenized_sentence = token\n",
    "        elif model_type == 'roberta' and token[0:1] != 'Ġ': \n",
    "            # RoBERTa adds 'Ġ' at the beginning of tokens if they are in the vocabulary.\n",
    "            # if they are not present, then it has split the word in to multiple tokens \n",
    "            # and need to be reconnected. \n",
    "            detokenized_sentence += token\n",
    "        elif token[0:2] == '##':\n",
    "            # Bert splits unrecognized tokens and prepends '##', this rejoins those.  \n",
    "            detokenized_sentence += token[2:]\n",
    "        elif token in postfix_symbols:\n",
    "            # Don't add space before punctuation. \n",
    "            detokenized_sentence += token\n",
    "        elif  detokenized_sentence[-1:] in prefix_symbols:\n",
    "            # Don't add space after dollar (and things like that. \n",
    "            detokenized_sentence += token\n",
    "        elif detokenized_sentence[-1:] in [\"'\", '\"', '-'] or token[0:1] in [\"'\", '\"', '-']:\n",
    "            # eliminates spaces before or after apostrophe. \n",
    "            detokenized_sentence += token\n",
    "        else:\n",
    "            # Looks like a normal word add a space. \n",
    "            detokenized_sentence += ' '+token\n",
    "        \n",
    "    if model_type == 'roberta':\n",
    "        # Remove the leftover weird Ġ's. \n",
    "        detokenized_sentence = detokenized_sentence.replace('Ġ', '')\n",
    "    \n",
    "    detokenized_sentence = detokenized_sentence.replace(\"` ` \", '\"')\n",
    "    \n",
    "    return detokenized_sentence\n",
    "\n",
    "## NLTK parts of speech \n",
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective 'big'\n",
    "# JJR adjective, comparative 'bigger'\n",
    "# JJS adjective, superlative 'biggest'\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "# NN noun, singular 'desk'\n",
    "# NNS noun plural 'desks'\n",
    "# NNP proper noun, singular 'Harrison'\n",
    "# NNPS proper noun, plural 'Americans'\n",
    "# PDT predeterminer 'all the kids'\n",
    "# POS possessive ending parent's\n",
    "# PRP personal pronoun I, he, she\n",
    "# PRP$ possessive pronoun my, his, hers\n",
    "# RB adverb very, silently,\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO to go 'to' the store.\n",
    "# UH interjection errrrrrrrm\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-abverb where, when\n",
    "\n",
    "# Pass sentence and list of parts of speach to be masked.\n",
    "# Returns masked sentence and list of words replaced by mask. \n",
    "def mask_sentence_by_part(sentence, part):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    word_tags = nltk.pos_tag(tokenized_sentence)\n",
    "    \n",
    "    masked_words = []\n",
    "    \n",
    "    for idx, word_tag in enumerate(word_tags):\n",
    "        if (word_tag[1] in part):\n",
    "            masked_words.append(tokenized_sentence[idx])\n",
    "            tokenized_sentence[idx] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "def expand_contractions(text):\n",
    "    assert len(text[0]) == 1\n",
    "    \n",
    "    return text[0] + contractions.fix(text)[1:]\n",
    "\n",
    "def augment_language_by_part(sentence, part):\n",
    "    masked_sentence, masked_words = mask_sentence_by_part(sentence, part)\n",
    "    new_sentence, predicted_words = masked_word_prediction(masked_sentence)\n",
    "    return new_sentence, masked_words, predicted_words\n",
    "\n",
    "def augment_language_by_frequency(sentence, percentile):\n",
    "    masked_sentence, masked_words = mask_sentence_by_frequency(sentence, percentile)\n",
    "    new_sentence, predicted_words = masked_word_prediction(masked_sentence)\n",
    "    return new_sentence, masked_words, predicted_words\n",
    "\n",
    "\n",
    "# Mask word based on their frequency, making the lower \"percentile\" passed. \n",
    "def mask_sentence_by_frequency(sentence, percentile):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    \n",
    "    masked_words = []\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(tokenized_sentence):\n",
    "        if word in wiki_freq_dict:\n",
    "            word_frequencies[word] = wiki_freq_dict[word]\n",
    "            \n",
    "    sorted_word_frequencies = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item: item[1])}\n",
    "\n",
    "    n=int(len(sorted_word_frequencies)*percentile)\n",
    "\n",
    "    low_frequency_words = {k: sorted_word_frequencies[k] for k in list(sorted_word_frequencies)[:n]}\n",
    "\n",
    "    # mask words, and keep track of what words were masked\n",
    "    for i, word in enumerate(tokenized_sentence):\n",
    "        if word in low_frequency_words:\n",
    "            masked_words.append(word)\n",
    "            tokenized_sentence[i] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "\n",
    "def augment_answers(orig_answer, orig_answer_start, augmented_paragraph, masked_words, predicted_words):\n",
    "    # Find answers in the augmented paragraph.\n",
    "    # Returns new answer and new start position\n",
    "    # Throws assertion if augmented answer isn't found. \n",
    "\n",
    "#     augmented_answer_re = orig_answer\n",
    "    expanded_answer = expand_contractions(orig_answer)\n",
    "    \n",
    "    # Escape some chars\n",
    "#     augmented_answer_re = re.escape(expanded_answer)\n",
    "\n",
    "    # a terrible way to escape to keep spaces unescaped, might instead use re.escape and then unescape spaces. \n",
    "    augmented_answer_re = expanded_answer \\\n",
    "        .replace(\"\\\\\", \"\\\\\\\\\") \\\n",
    "        .replace(\")\", \"\\)\") \\\n",
    "        .replace(\"(\", \"\\(\") \\\n",
    "        .replace(\"]\", \"\\]\") \\\n",
    "        .replace(\"[\", \"\\[\") \\\n",
    "        .replace(\"}\", \"\\}\") \\\n",
    "        .replace(\"{\", \"\\{\") \\\n",
    "        .replace(\">\", \"\\>\") \\\n",
    "        .replace(\"<\", \"\\<\") \\\n",
    "        .replace('^', '\\^') \\\n",
    "        .replace('|', '\\|') \\\n",
    "        .replace('*', '\\*') \\\n",
    "        .replace('?', '\\?') \\\n",
    "        .replace('+', '\\+') \\\n",
    "        .replace(\"``\", '\"')\n",
    "\n",
    "    # Ignore whitespace around some chars\n",
    "    augmented_answer_re = re.sub(r'([.,$=/\\-\"\\'])', r' *\\1 *', augmented_answer_re.strip())\n",
    "    augmented_answer_re = augmented_answer_re.replace('$', '\\$')\n",
    "\n",
    "    already_swapped = []\n",
    "    for masked_word, predicted_word in zip(masked_words, predicted_words):\n",
    "        if re.escape(masked_word) != re.escape(predicted_word) and not masked_word in already_swapped:\n",
    "            already_swapped.append(masked_word)\n",
    "            word_pair_re = \" *(\" + re.escape(masked_word) \n",
    "            \n",
    "            \n",
    "            for sub_masked_word, sub_predicted_word in zip(masked_words, predicted_words):\n",
    "                if sub_masked_word == masked_word:\n",
    "                    word_pair_re += \"|\" + re.escape(sub_predicted_word)\n",
    "\n",
    "            word_pair_re += \") *\"\n",
    "\n",
    "            # Swap out masked words with regex matching either as whole word. \n",
    "            augmented_answer_re = re.sub(r'\\b'+re.escape(masked_word)+r'\\b', word_pair_re, augmented_answer_re)\n",
    "\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * * *', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * *', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * ', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace('  ', ' ')\n",
    "\n",
    "#     print(\"augmented_answer_re:\\n{}\\naugmented_paragraph:\\n{}\\n\\n\\n\".format(r'('+augmented_answer_re+r')', augmented_paragraph))\n",
    "\n",
    "    # Find all the matches\n",
    "    matches = re.finditer(r'('+augmented_answer_re+r')', augmented_paragraph, re.IGNORECASE)\n",
    "    \n",
    "    # Find the match closest in start_position and use that one. \n",
    "    closest_match = None\n",
    "    closest_position_delta = 0\n",
    "    for match in matches:\n",
    "        position_delta = abs(match.span()[0] - orig_answer_start)\n",
    "        \n",
    "        if closest_match is None or position_delta < closest_position_delta:\n",
    "            closest_match = match\n",
    "            closest_position_delta = position_delta\n",
    "            \n",
    "#     assert closest_match, augmented_answer_re\n",
    "    # If there is no match, alert and move on. \n",
    "    if not closest_match:\n",
    "#         print(\"*** Could not find\\nOrig:'{}'\\nAug:'{}' in :\\n{}\\n\".format(orig_answer, augmented_answer_re, augmented_paragraph))\n",
    "        new_answer = None\n",
    "        new_start_pos = None\n",
    "    else:\n",
    "        new_answer = closest_match.group()\n",
    "        new_start_pos = closest_match.span()[0]\n",
    "    \n",
    "    return new_answer, new_start_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/413 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_JJ_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_JJ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_VB_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_RB_RBR_RBZ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_VB_VBD_VBG_VBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_Percentile_0.1.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_Percentile_0.2.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_Percentile_0.3.json\n",
      "Processing file: /data/distribution_shift/new_qa/amazon_reviews_v1.0.json for parts of frequncy percentile: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 145/413 [00:29<00:54,  4.92it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (761 > 512). Running this sequence through the model will result in indexing errors\n",
      " 44%|████▍     | 182/413 [00:36<00:48,  4.73it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (953 > 512). Running this sequence through the model will result in indexing errors\n",
      " 50%|████▉     | 205/413 [00:41<00:40,  5.12it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 512). Running this sequence through the model will result in indexing errors\n",
      " 50%|█████     | 207/413 [00:42<00:43,  4.70it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (675 > 512). Running this sequence through the model will result in indexing errors\n",
      " 53%|█████▎    | 218/413 [00:44<00:40,  4.79it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (711 > 512). Running this sequence through the model will result in indexing errors\n",
      " 70%|██████▉   | 289/413 [00:58<00:22,  5.59it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n",
      " 78%|███████▊  | 324/413 [01:05<00:17,  5.11it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (599 > 512). Running this sequence through the model will result in indexing errors\n",
      " 84%|████████▎ | 345/413 [01:10<00:14,  4.84it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (609 > 512). Running this sequence through the model will result in indexing errors\n",
      " 95%|█████████▌| 393/413 [01:19<00:03,  5.35it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 413/413 [01:23<00:00,  4.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting: /data/distribution_shift/augmented_qa/amazon_reviews_v1_0_roberta_Percentile_0.5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_JJ_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_JJ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_VB_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_RB_RBR_RBZ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_VB_VBD_VBG_VBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_Percentile_0.1.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_Percentile_0.2.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_Percentile_0.3.json\n",
      "Processing file: /data/distribution_shift/new_qa/reddit_v1.0.json for parts of frequncy percentile: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (736 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (731 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (839 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (781 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (618 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (632 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (627 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (728 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (746 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1/1 [01:17<00:00, 77.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting: /data/distribution_shift/augmented_qa/reddit_v1_0_roberta_Percentile_0.5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_JJ_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_JJ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_VB_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_RB_RBR_RBZ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_VB_VBD_VBG_VBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_Percentile_0.1.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_Percentile_0.2.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_Percentile_0.3.json\n",
      "Processing file: /data/distribution_shift/new_qa/new_wiki_v1.0.json for parts of frequncy percentile: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/48 [00:05<00:45,  1.07s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n",
      " 29%|██▉       | 14/48 [00:18<00:49,  1.45s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (536 > 512). Running this sequence through the model will result in indexing errors\n",
      " 31%|███▏      | 15/48 [00:19<00:45,  1.37s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      " 75%|███████▌  | 36/48 [00:43<00:13,  1.08s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (843 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 48/48 [00:52<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting: /data/distribution_shift/augmented_qa/new_wiki_v1.0_roberta_Percentile_0.5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/797 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_JJ_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_JJ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_VB_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_VB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_RB.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_RB_RBR_RBZ.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_VB_VBD_VBG_VBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_Percentile_0.1.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_Percentile_0.2.json\n",
      "Skipping existing output: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_Percentile_0.3.json\n",
      "Processing file: /data/distribution_shift/new_qa/nyt_v1.0.json for parts of frequncy percentile: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 53/797 [00:04<01:10, 10.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      " 10%|▉         | 78/797 [00:06<01:00, 11.85it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      " 12%|█▏        | 98/797 [00:08<01:07, 10.36it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      " 23%|██▎       | 180/797 [00:15<00:43, 14.24it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      " 28%|██▊       | 225/797 [00:19<01:01,  9.31it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      " 42%|████▏     | 336/797 [00:29<00:53,  8.66it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      " 50%|████▉     | 397/797 [00:35<00:40,  9.83it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      " 63%|██████▎   | 504/797 [00:45<00:23, 12.56it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      " 65%|██████▌   | 519/797 [00:47<00:27, 10.09it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      " 72%|███████▏  | 575/797 [00:52<00:27,  7.93it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      " 99%|█████████▉| 790/797 [01:11<00:00, 10.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 797/797 [01:12<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting: /data/distribution_shift/augmented_qa/nyt_v1.0_roberta_Percentile_0.5.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_qa(qa_json, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "\n",
    "    if parts_of_speech:\n",
    "        print(\"Processing file: {} for parts of speech: {}\".format(qa_json, parts_of_speech))\n",
    "    elif frequency_percentile:\n",
    "        print(\"Processing file: {} for parts of frequncy percentile: {}\".format(qa_json, frequency_percentile))\n",
    "        \n",
    "    \n",
    "    total_time = 0\n",
    "    paragraph_count = 0\n",
    "\n",
    "    results = {}\n",
    "    count = 0\n",
    "\n",
    "    augmented_json = {\"data\": []}\n",
    "\n",
    "    paragraphs = []\n",
    "    augmented_json[\"data\"].append(paragraphs)\n",
    "\n",
    "    ## JSON Format\n",
    "    # -data\n",
    "    #   -paragraphs\n",
    "    #     -context\n",
    "    #     -qas\n",
    "    #       -question\n",
    "    #       -id\n",
    "    #       -answers\n",
    "    #          -text\n",
    "    #          -answer_start\n",
    "    #   -title\n",
    "    #   -split\n",
    "\n",
    "    with open(qa_json) as json_file:\n",
    "        qa_json = json.load(json_file)['data']\n",
    "\n",
    "        new_data = []\n",
    "        new_splits = []\n",
    "        new_data.append(new_splits)\n",
    "        new_json = {\"data\": new_data}\n",
    "\n",
    "        new_splits = []\n",
    "        new_json = {\"data\": new_splits}\n",
    "\n",
    "        for splits in tqdm(qa_json):\n",
    "            paragraphs = splits['paragraphs']\n",
    "\n",
    "            new_paragraphs = []\n",
    "            new_split = {\"paragraphs\": new_paragraphs, \"split\": splits['split'], \"title\": splits['title']}\n",
    "            new_splits.append(new_split)\n",
    "\n",
    "\n",
    "            for paragraph in paragraphs:\n",
    "                start = timeit.default_timer()\n",
    "                count += 1\n",
    "\n",
    "                if parts_of_speech:\n",
    "                    augmented_paragraph, masked_words, predicted_words = augment_language_by_part(\n",
    "                        paragraph['context'], parts_of_speech\n",
    "                    )\n",
    "                elif frequency_percentile:\n",
    "                    augmented_paragraph, masked_words, predicted_words = augment_language_by_frequency(\n",
    "                        paragraph['context'], frequency_percentile\n",
    "                    )\n",
    "\n",
    "                new_qas = []\n",
    "                new_paragraph = {\"context\": augmented_paragraph, \"qas\": new_qas}\n",
    "                new_paragraphs.append(new_paragraph)\n",
    "\n",
    "                for qa in paragraph['qas']:\n",
    "                    #TODO Skip any with missing answers\n",
    "                    keep_question = True\n",
    "\n",
    "                    new_answers = []\n",
    "\n",
    "                    for answer in qa['answers']:\n",
    "                        aug_answer_text, aug_answer_start = augment_answers(answer['text'], answer['answer_start'], augmented_paragraph, masked_words, predicted_words)\n",
    "\n",
    "                        if aug_answer_start is None:\n",
    "                            keep_question = False\n",
    "\n",
    "                        new_answer = {\"answer_start\": aug_answer_start, \"text\": aug_answer_text}\n",
    "                        new_answers.append(new_answer)\n",
    "\n",
    "                    if keep_question:\n",
    "                        new_qas.append({\"answers\": new_answers, \"id\": qa['id'], \"question\": qa['question']})\n",
    "\n",
    "\n",
    "\n",
    "                stop = timeit.default_timer()\n",
    "                run_time = stop - start\n",
    "\n",
    "                total_time += run_time\n",
    "                paragraph_count += 1\n",
    "    return new_json\n",
    "\n",
    "def get_augmented_filename(output_dir, question_set, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "    \n",
    "    if parts_of_speech:\n",
    "        filename = output_dir+question_set+\"_\"+model_type+\"_\"+\"_\".join(parts_of_speech)+\".json\"\n",
    "    elif frequency_percentile:\n",
    "        filename = output_dir+question_set+\"_\"+model_type+\"_Percentile_\"+str(frequency_percentile)+\".json\"\n",
    "    \n",
    "    return filename\n",
    "    \n",
    "\n",
    "def write_json(augmented_qa_json, output_dir, question_set, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "    filename = get_augmented_filename(output_dir, question_set, parts_of_speech, frequency_percentile)\n",
    "    \n",
    "    print(\"Writting: {}\".format(filename))\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(augmented_qa_json, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return filename\n",
    "\n",
    "\n",
    "output_dir = '/data/distribution_shift/augmented_qa/'\n",
    "\n",
    "qa_files = {\n",
    "    \"amazon_reviews_v1_0\": '/data/distribution_shift/new_qa/amazon_reviews_v1.0.json',\n",
    "    \"reddit_v1_0\": '/data/distribution_shift/new_qa/reddit_v1.0.json',\n",
    "    \"new_wiki_v1.0\": '/data/distribution_shift/new_qa/new_wiki_v1.0.json',\n",
    "    \"nyt_v1.0\": '/data/distribution_shift/new_qa/nyt_v1.0.json',\n",
    "}\n",
    "\n",
    "parts_of_speech_list = [\n",
    "    ['JJ', 'VB'],\n",
    "    ['JJ'],\n",
    "    ['VB', 'RB'],\n",
    "    ['VB'],\n",
    "    ['RB'],\n",
    "    ['RB', 'RBR', 'RBZ'],\n",
    "    ['VB', 'VBD', 'VBG', 'VBN', 'VBP'],\n",
    "    ['RB', 'RBR', 'RBZ', 'VB', 'VBD', 'VBG' 'VBN', 'VBP']\n",
    "]\n",
    "\n",
    "frequency_percentiles = [\n",
    "    0.10,\n",
    "    0.20,\n",
    "    0.30,\n",
    "    0.50\n",
    "]\n",
    "\n",
    "for quesion_set, filename in qa_files.items():\n",
    "    for parts_of_speech in parts_of_speech_list:\n",
    "        filepath = get_augmented_filename(output_dir, quesion_set, parts_of_speech=parts_of_speech)\n",
    "        if not os.path.exists(filepath):\n",
    "            augmented_qa_json = process_qa(filename, parts_of_speech=parts_of_speech)\n",
    "            write_json(augmented_qa_json, output_dir, quesion_set, parts_of_speech=parts_of_speech)\n",
    "        else:\n",
    "            print(\"Skipping existing output: {}\".format(filepath))\n",
    "        \n",
    "    for frequency_percentile in frequency_percentiles:\n",
    "        filepath = get_augmented_filename(output_dir, quesion_set, frequency_percentile=frequency_percentile)\n",
    "        if not os.path.exists(filepath):\n",
    "            augmented_qa_json = process_qa(filename, frequency_percentile=frequency_percentile)\n",
    "            write_json(augmented_qa_json, output_dir, quesion_set, frequency_percentile=frequency_percentile)\n",
    "        else:\n",
    "            print(\"Skipping existing output: {}\".format(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wikipedia Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_word_count_file = \"/data/distribution_shift/enwiki-20190320-words-frequency.txt\"\n",
    "\n",
    "wiki_freq_dict = {}\n",
    "\n",
    "with open(wiki_word_count_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        word_count = line.split(' ')\n",
    "        wiki_freq_dict[word_count[0]] = int(word_count[1][:-1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First of all, this thing is <mask>' <mask>. My wife put it together while I did some work, she <mask> all the glass <mask> in to the frame . We <mask> it up with 6 BIG 4\"wall <mask> <mask>. On the top <mask> I house an external WD Hdd, a WDTV Live and a Actiontec HDMI WiFi Transmitter . On the second <mask> I have my Dish Network 722 (<mask> <mask> even with all the <mask> coming out the back). On the bottom <mask> I have an old Ken wood Stereo Receive (<mask>, but the front legs come off just a bit but this does not affect <mask>. The <mask> measures 17\"w x 14\"d x 5\"h.The <mask> maintenance was a little <mask>. We used a <mask> coat <mask> and some <mask> tape to <mask> the <mask> <mask> through the back.This unit installed is an <mask> addition to your small living space, I highly <mask> it!\n"
     ]
    }
   ],
   "source": [
    "# Mask word based on their frequency, making the lower \"percentile\" passed. \n",
    "def mask_sentence_frequency(sentence, percentile):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    \n",
    "    masked_words = []\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(tokenized_sentence):\n",
    "        if word in wiki_freq_dict:\n",
    "            word_frequencies[word] = wiki_freq_dict[word]\n",
    "            \n",
    "    sorted_word_frequencies = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item: item[1])}\n",
    "\n",
    "    n=int(len(sorted_word_frequencies)*percentile)\n",
    "\n",
    "    low_frequency_words = {k: sorted_word_frequencies[k] for k in list(sorted_word_frequencies)[:n]}\n",
    "\n",
    "    # mask words, and keep track of what words were masked\n",
    "    for i, word in enumerate(tokenized_sentence):\n",
    "        if word in low_frequency_words:\n",
    "            masked_words.append(word)\n",
    "            tokenized_sentence[i] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "paragraph = \"It's a very nice holder - not too big and not too small. It fits any lipstick, lip gloss, chapstick, etc nicely. I love that I'm able to see what I have and not have to dig through a makeup bag anymore. I would highly recommend.\"\n",
    "paragraph = \"First of all, this thing is freakin' awesome. My wife put it together while I did some work, she assembled all the glass shelves in to the frame. We hung it up with 6 BIG 4\\\" wall toggle bolts. On the top shelf I house an external WD Hdd, a WDTV Live and a Actiontec HDMI WiFi Transmitter. On the second shelf I have my Dish Network 722 (fits perfectly even with all the wires coming out the back). On the bottom shelf I have an old Ken wood Stereo Receive (fits, but the front legs come off just a bit but this doesn't affect stability. The receiver measures 17\\\"w x 14\\\"d x 5\\\"h.The wire maintenance was a little trick. We used a wire coat hanger and some masking tape to thread the bulky wires through the back.This unit installed is an amazing addition to your small living space, I highly recommend it!\"\n",
    "\n",
    "masked_sentence, word_mask = mask_sentence_frequency(paragraph, 0.25)\n",
    "print(masked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"paragraphs\": [\n",
      "        {\n",
      "            \"context\": \"It's a very nice holder - not too big and not too small. It fits any lipstick, lip gloss, chapstick, etc nicely. I love that I'm able to see what I have and not have to dig through a makeup bag anymore. I would highly recommend.\",\n",
      "            \"qas\": [\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 26,\n",
      "                            \"text\": \"not too big and not too small\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 26,\n",
      "                            \"text\": \"not too big and not too small\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 30,\n",
      "                            \"text\": \"too big and not too small\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd465dacc027a086d65bc6c\",\n",
      "                    \"question\": \"What size is the holder?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 65,\n",
      "                            \"text\": \"any lipstick, lip gloss, chapstick\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 65,\n",
      "                            \"text\": \"any lipstick, lip gloss, chapstick, etc\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 60,\n",
      "                            \"text\": \"fits any lipstick, lip gloss, chapstick, etc nicely\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd465dacc027a086d65bc6d\",\n",
      "                    \"question\": \"What does it fit nicely?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 203,\n",
      "                            \"text\": \"I would highly recommend.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 211,\n",
      "                            \"text\": \"highly\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 211,\n",
      "                            \"text\": \"highly recommend.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd465dacc027a086d65bc6e\",\n",
      "                    \"question\": \"Does the writer recommend the holder?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 157,\n",
      "                            \"text\": \"not have to dig through a makeup bag anymore.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 157,\n",
      "                            \"text\": \"not\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 157,\n",
      "                            \"text\": \"not have to dig\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd465dacc027a086d65bc6f\",\n",
      "                    \"question\": \"Do you have to dig through a makeup bag?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 17,\n",
      "                            \"text\": \"holder\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 17,\n",
      "                            \"text\": \"holder\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 17,\n",
      "                            \"text\": \"holder\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd465dacc027a086d65bc70\",\n",
      "                    \"question\": \"What is the product?\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"context\": \"First of all, this thing is freakin' awesome. My wife put it together while I did some work, she assembled all the glass shelves in to the frame. We hung it up with 6 BIG 4\\\" wall toggle bolts. On the top shelf I house an external WD Hdd, a WDTV Live and a Actiontec HDMI WiFi Transmitter. On the second shelf I have my Dish Network 722 (fits perfectly even with all the wires coming out the back). On the bottom shelf I have an old Ken wood Stereo Receive (fits, but the front legs come off just a bit but this doesn't affect stability. The receiver measures 17\\\"w x 14\\\"d x 5\\\"h.The wire maintenance was a little trick. We used a wire coat hanger and some masking tape to thread the bulky wires through the back.This unit installed is an amazing addition to your small living space, I highly recommend it!\",\n",
      "            \"qas\": [\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 46,\n",
      "                            \"text\": \"My wife\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 49,\n",
      "                            \"text\": \"wife\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 49,\n",
      "                            \"text\": \"wife\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4662ccc027a086d65bc7e\",\n",
      "                    \"question\": \"Who assembled the glass shelves?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 319,\n",
      "                            \"text\": \"Dish Network 722\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 319,\n",
      "                            \"text\": \"Dish Network 722\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 319,\n",
      "                            \"text\": \"Dish Network 722\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4662ccc027a086d65bc7f\",\n",
      "                    \"question\": \"What's on the second shelf?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 428,\n",
      "                            \"text\": \"old Ken wood Stereo Receive\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 432,\n",
      "                            \"text\": \"Ken wood Stereo Receive\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 432,\n",
      "                            \"text\": \"Ken wood Stereo Receive\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4662ccc027a086d65bc80\",\n",
      "                    \"question\": \"What's on the bottom shelf?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 559,\n",
      "                            \"text\": \"17\\\"w x 14\\\"d x 5\\\"h\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 559,\n",
      "                            \"text\": \"17\\\"w x 14\\\"d x 5\\\"h\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 559,\n",
      "                            \"text\": \"17\\\"w x 14\\\"d x 5\\\"h\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4662ccc027a086d65bc81\",\n",
      "                    \"question\": \"What are the dimensions of the receiver?\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"context\": \"The presto my pod makes, to me, a very weak and watery cup of coffee. It could be just me, I prefer using the medium or dark roast pods, I like strong coffee. I tried using a few different types of coffee in the presto my pod, but it didn't seem to make a difference. I'm not convinced that the pressure is strong enough to brew a decent cup.don't buy this... heck I'll sell you mine for a dollar :-) I'll never use the dang thing again.I followed up by buying the brown eco pod, and THAT worked decently and made a nice cup of coffee by my standards.\",\n",
      "            \"qas\": [\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 401,\n",
      "                            \"text\": \"I'll never use the dang thing again\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 272,\n",
      "                            \"text\": \"not\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 342,\n",
      "                            \"text\": \"don't buy this\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd466d7cc027a086d65bcc0\",\n",
      "                    \"question\": \"Does the writer recommend the pod?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 227,\n",
      "                            \"text\": \"but it didn't seem to make a difference.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 231,\n",
      "                            \"text\": \"it didn't seem to\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 234,\n",
      "                            \"text\": \"didn't seem to make a difference\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd466d7cc027a086d65bcc3\",\n",
      "                    \"question\": \"Did several pods make a difference?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 272,\n",
      "                            \"text\": \"not convinced that the pressure is strong enough\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 272,\n",
      "                            \"text\": \"not convinced\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 272,\n",
      "                            \"text\": \"not convinced that the pressure is strong enough to brew a decent cup\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd466d7cc027a086d65bcc1\",\n",
      "                    \"question\": \"Is the pressure strong enough?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 137,\n",
      "                            \"text\": \"I like strong coffee\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 137,\n",
      "                            \"text\": \"I like strong coffee\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 144,\n",
      "                            \"text\": \"strong\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd466d7cc027a086d65bcc2\",\n",
      "                    \"question\": \"Does the writer like strong coffee?\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"context\": \"This product takes 10 minutes to setup 4. It is super easy. Very easy on the eyes. It makes the room look like you have a sound system. I used my simple SONY HTIB.The product is stong on the bottom so it's not going to tilt over. The extension part is a bit weak. I have it fully extended as I guess most people would like them. The product because to lose strength and stand a bit off centered. It leans to a side. I wouldn't return them! Just keep in mind that they are a great value and look great. Just has a clumsy feel to it. I tighten them every week when i see them leaning too much.\",\n",
      "            \"qas\": [\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 19,\n",
      "                            \"text\": \"10 minutes\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 19,\n",
      "                            \"text\": \"10 minutes\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 19,\n",
      "                            \"text\": \"10 minutes\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd46770cc027a086d65bd08\",\n",
      "                    \"question\": \"How long does the product take to set up?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 198,\n",
      "                            \"text\": \"so it's not going to tilt over.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 163,\n",
      "                            \"text\": \"The product is stong\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 167,\n",
      "                            \"text\": \"product is stong on the bottom\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd46770cc027a086d65bd09\",\n",
      "                    \"question\": \"is the product strong on the bottom?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 234,\n",
      "                            \"text\": \"extension part is a bit weak.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 252,\n",
      "                            \"text\": \"a bit\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 230,\n",
      "                            \"text\": \"The extension part is a bit weak\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd46770cc027a086d65bd0a\",\n",
      "                    \"question\": \"is the extension part weak?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 396,\n",
      "                            \"text\": \"It leans to a side.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 396,\n",
      "                            \"text\": \"It leans to a side\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 396,\n",
      "                            \"text\": \"It leans to a side\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd46770cc027a086d65bd0b\",\n",
      "                    \"question\": \"does it lean to the side?\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"context\": \"I have always kept a dustbuster in my kitchen for quick clean ups. They are always charged. Not with this new 18 volt model. Not only does is not have much suction power, you can't keep the battery plugged in so it is always charged and ready to go. If you don't use it for 4-5 days, the charge depletes! So then you have to recharge it for 14-16 hours. Not convenient at all. So I returned the dustbuster. I then ordered a Eureka that works great and can be left plugged in, permanently. The model number is H96. It is not as large as it looks in the photos and is a great buy; not to mention it costs less than the inefficient new dustbusters. The new dustbuster models are now just cheap knock offs of the original great machines. Black and Decker in their attempt to make a higher profit by cheapening a previously great product, destroyed a really good thing.I strongly recommend the Eureka below:Eureka Quick Up Cordless 2 in 1, 96Hproduct.\",\n",
      "            \"qas\": [\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 274,\n",
      "                            \"text\": \"4-5 days\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 274,\n",
      "                            \"text\": \"4-5 days\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 274,\n",
      "                            \"text\": \"4-5\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4680fcc027a086d65bd96\",\n",
      "                    \"question\": \"how many days until the charge depletes?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 341,\n",
      "                            \"text\": \"14-16 hours.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 341,\n",
      "                            \"text\": \"14-16 hours\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 341,\n",
      "                            \"text\": \"14-16 hours\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4680fcc027a086d65bd97\",\n",
      "                    \"question\": \"how long do you have to recharge it?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 354,\n",
      "                            \"text\": \"Not convenient at all.\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 354,\n",
      "                            \"text\": \"Not\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 354,\n",
      "                            \"text\": \"Not convenient at all.\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4680fcc027a086d65bd98\",\n",
      "                    \"question\": \"Is it convenient?\"\n",
      "                },\n",
      "                {\n",
      "                    \"answers\": [\n",
      "                        {\n",
      "                            \"answer_start\": 509,\n",
      "                            \"text\": \"H96\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 509,\n",
      "                            \"text\": \"H96\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"answer_start\": 509,\n",
      "                            \"text\": \"H96\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"id\": \"5dd4680fcc027a086d65bd99\",\n",
      "                    \"question\": \"What is the model number?\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"split\": \"amazon_reviews\",\n",
      "    \"title\": \"Amazon_Reviews_2030\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "qa_json = '/data/distribution_shift/new_qa/amazon_reviews_v1.0.json'\n",
    "with open(qa_json) as json_file:\n",
    "    qa_json = json.load(json_file)['data']\n",
    "    for splits in qa_json:\n",
    "        print(json.dumps(splits, indent=4, sort_keys=True))\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
