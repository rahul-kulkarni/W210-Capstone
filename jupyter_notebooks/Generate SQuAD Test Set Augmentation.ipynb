{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# from transformers import BertForQuestionAnswering, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "# from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "import timeit\n",
    "from copy import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import contractions\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from proj_utils import *\n",
    "from proj_config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location for augmented test files\n",
    "output_dir = '/data/augmentation/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Required for identifying parts of speech. \n",
    "#TODO should we do this using Bert based model?\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Use BERT or RoBERTa models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_type = 'roberta'\n",
    "\n",
    "# Download pre-trained models from huggingface\n",
    "if model_type == 'bert':\n",
    "    # Download (Using cased to maintain case in output)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-large-cased')\n",
    "    model_mask = '[MASK]'\n",
    "    \n",
    "elif model_type == 'roberta':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "    model_mask = '<mask>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_roberta.RobertaForMaskedLM'>\n",
      "<class 'torch.nn.parallel.data_parallel.DataParallel'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# if torch.cuda.device_count() > 1:\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Wiki word frequencies in to json.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wiki_word_count_file = \"/data/misc/enwiki-20190320-words-frequency.txt\"\n",
    "\n",
    "# wiki_freq_dict = {}\n",
    "\n",
    "# with open(wiki_word_count_file, 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         word_count = line.split(' ')\n",
    "#         wiki_freq_dict[word_count[0]] = int(word_count[1][:-1])\n",
    "\n",
    "# write_gzip_json(\"/data/misc/enwiki-20190320-words-frequency.json.gz\", wiki_freq_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://nlp-distribution.s3.ca-central-1.amazonaws.com/misc/enwiki-20190320-words-frequency.json.gz\n"
     ]
    }
   ],
   "source": [
    "wiki_freq_dict = get_gzip_json_url('https://nlp-distribution.s3.ca-central-1.amazonaws.com/misc/enwiki-20190320-words-frequency.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_word_prediction(text):\n",
    "    # Text is already masked.  Find masked words, predict replacements and replace them.  \n",
    "    global model_type\n",
    "    predicted_words = []\n",
    "    full_paragraph = ''\n",
    "    \n",
    "#     text_token_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "#     tokenized_text = tokenizer.tokenize(text, return_tensors='pt')\n",
    "\n",
    "    text_token_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Nested function so we can break paragraph in to parts when > 512 tokens long. \n",
    "    def predict_part(text_token_ids_part, tokenized_text_part):\n",
    "        # Get and format positions of work masks\n",
    "        mask_positions_2d = (text_token_ids_part.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "        mask_positions = [mask.item() for mask in mask_positions_2d ]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(text_token_ids_part)\n",
    "\n",
    "        last_hidden_state = output[0].squeeze()\n",
    "\n",
    "        token_predictions_list =[]\n",
    "        for mask_index in mask_positions:\n",
    "            mask_hidden_state = last_hidden_state[mask_index]\n",
    "            # This isn't really required unless we want > 1 predicted word per mask. \n",
    "            idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n",
    "\n",
    "            # The different models tokenize differently, so handle based on model type. \n",
    "            words = None\n",
    "            if model_type == 'roberta':\n",
    "                words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "            elif model_type == 'bert':\n",
    "                words = tokenizer.decode(idx).split(' ')\n",
    "                \n",
    "            for i in range(len(words)):\n",
    "                if (words[i][0:2] == '##'):\n",
    "                    words[i] = words[i][2:]\n",
    "\n",
    "            token_predictions_list.append(words)\n",
    "\n",
    "    \n",
    "        # Make sure we have a list of predictions for each mask. \n",
    "        assert len(mask_positions) == len(token_predictions_list)\n",
    "\n",
    "        # Replace masks with predicted words\n",
    "        #   Make a copy so we can calculate shape differences, required for answer_start in QnA.\n",
    "        predicted_text = copy(tokenized_text_part) \n",
    "        for pos, new_word in zip(mask_positions, token_predictions_list):\n",
    "\n",
    "            #TODO This is where we could search for a more appropriate replacement. \n",
    "            # Add the weird G at the front of predicted words, so they look like recognized tokens for the \n",
    "            # detokenization logic used in 'detokenize()'. \n",
    "            if model_type == 'roberta':\n",
    "                predicted_word = 'Ä '+new_word[0]\n",
    "            else:\n",
    "                predicted_word = new_word[0]\n",
    "\n",
    "            # Ugly hack, not sure why this goes askew\n",
    "            if predicted_text[pos-2] == \"<mask>\":\n",
    "                predicted_text[pos-2] = predicted_word\n",
    "            else:\n",
    "                predicted_text[pos-1] = predicted_word\n",
    "            predicted_words.append(new_word[0])\n",
    "\n",
    "        return detokenize(predicted_text)\n",
    "    \n",
    "\n",
    "    # Split text in multiple parts if text is longer than 512 tokens. \n",
    "    if len(tokenized_text) > 512:\n",
    "        # Let's figure out how many splits.\n",
    "        start_pos = 0\n",
    "        end_pos = 0\n",
    "        piece_size = 0\n",
    "        pieces = 0\n",
    "        \n",
    "        for i in range(100):\n",
    "            denom = i + 1\n",
    "            if int(len(tokenized_text)/denom) < 500:\n",
    "                piece_size = int(len(tokenized_text)/denom)\n",
    "                pieces = denom\n",
    "                break\n",
    "        \n",
    "#         print(\"Going to break the para in to {} pieces. End: {}\".format(pieces, end_pos))\n",
    "        \n",
    "        for i in range(pieces):\n",
    "            end_pos = piece_size * (i+1)\n",
    "            \n",
    "#             print(\"Doing piece {} Start: {} End: {}\".format(i+1, start_pos, end_pos))\n",
    "            sized_text = []\n",
    "            split_idx = 0\n",
    "            \n",
    "            if i == (pieces-1):\n",
    "                # Grab to the end. \n",
    "                split_idx = None\n",
    "            else:\n",
    "                # Look for the first sentence ending after the split position. \n",
    "                split_idx = end_pos\n",
    "                for idx, token in enumerate(tokenized_text[end_pos:], start=end_pos):\n",
    "                    if token in '.!?' or (idx-end_pos) >= 512:\n",
    "                        split_idx = idx+1\n",
    "                        break\n",
    "            \n",
    "#             print(tokenized_text[start_pos:split_idx])\n",
    "#             print(\"start_pos: {} split_idx: {} end_pos: {}\".format(start_pos,split_idx, end_pos))\n",
    "            full_paragraph += predict_part(text_token_ids[0:1,start_pos:split_idx], tokenized_text[start_pos:split_idx])\n",
    "            start_pos = split_idx\n",
    "    else:\n",
    "        full_paragraph += predict_part(text_token_ids, tokenized_text)\n",
    "        \n",
    "    if full_paragraph.find(\"\\<mask\\>\") >= 0:\n",
    "        print(full_paragraph)\n",
    "        \n",
    "    return full_paragraph, predicted_words\n",
    "\n",
    "def detokenize(tokenized_text):\n",
    "    global model_type\n",
    "    postfix_symbols=\"?:!.,;%)\"\n",
    "    prefix_symbols=\"($\"\n",
    "    \n",
    "    detokenized_sentence=''\n",
    "    for idx, token in enumerate(tokenized_text):\n",
    "        if idx == 0:\n",
    "            # Just to ensure we can assume text exists in detokenized_sentence below.  \n",
    "            detokenized_sentence = token\n",
    "        elif model_type == 'roberta' and token[0:1] != 'Ä ': \n",
    "            # RoBERTa adds 'Ä ' at the beginning of tokens if they are in the vocabulary.\n",
    "            # if they are not present, then it has split the word in to multiple tokens \n",
    "            # and need to be reconnected. \n",
    "            detokenized_sentence += token\n",
    "        elif token[0:2] == '##':\n",
    "            # Bert splits unrecognized tokens and prepends '##', this rejoins those.  \n",
    "            detokenized_sentence += token[2:]\n",
    "        elif token in postfix_symbols:\n",
    "            # Don't add space before punctuation. \n",
    "            detokenized_sentence += token\n",
    "        elif  detokenized_sentence[-1:] in prefix_symbols:\n",
    "            # Don't add space after dollar (and things like that. \n",
    "            detokenized_sentence += token\n",
    "        elif detokenized_sentence[-1:] in [\"'\", '\"', '-'] or token[0:1] in [\"'\", '\"', '-']:\n",
    "            # eliminates spaces before or after apostrophe. \n",
    "            detokenized_sentence += token\n",
    "        else:\n",
    "            # Looks like a normal word add a space. \n",
    "            detokenized_sentence += ' '+token\n",
    "        \n",
    "    if model_type == 'roberta':\n",
    "        # Remove the leftover weird Ä 's. \n",
    "        detokenized_sentence = detokenized_sentence.replace('Ä ', '')\n",
    "    \n",
    "    detokenized_sentence = detokenized_sentence.replace(\"` ` \", '\"')\n",
    "    \n",
    "    return detokenized_sentence\n",
    "\n",
    "## NLTK parts of speech \n",
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective 'big'\n",
    "# JJR adjective, comparative 'bigger'\n",
    "# JJS adjective, superlative 'biggest'\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "# NN noun, singular 'desk'\n",
    "# NNS noun plural 'desks'\n",
    "# NNP proper noun, singular 'Harrison'\n",
    "# NNPS proper noun, plural 'Americans'\n",
    "# PDT predeterminer 'all the kids'\n",
    "# POS possessive ending parent's\n",
    "# PRP personal pronoun I, he, she\n",
    "# PRP$ possessive pronoun my, his, hers\n",
    "# RB adverb very, silently,\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO to go 'to' the store.\n",
    "# UH interjection errrrrrrrm\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-abverb where, when\n",
    "\n",
    "# Pass sentence and list of parts of speach to be masked.\n",
    "# Returns masked sentence and list of words replaced by mask. \n",
    "def mask_sentence_by_part(sentence, part):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    word_tags = nltk.pos_tag(tokenized_sentence)\n",
    "    \n",
    "    masked_words = []\n",
    "    \n",
    "    for idx, word_tag in enumerate(word_tags):\n",
    "        if (word_tag[1] in part):\n",
    "            masked_words.append(tokenized_sentence[idx])\n",
    "            tokenized_sentence[idx] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "def expand_contractions(text):\n",
    "    assert len(text[0]) == 1\n",
    "    \n",
    "    return text[0] + contractions.fix(text)[1:]\n",
    "\n",
    "def augment_language_by_part(sentence, part):\n",
    "    masked_sentence, masked_words = mask_sentence_by_part(sentence, part)\n",
    "    new_sentence, predicted_words = masked_word_prediction(masked_sentence)\n",
    "    return new_sentence, masked_words, predicted_words\n",
    "\n",
    "def augment_language_by_frequency(sentence, percentile):\n",
    "    masked_sentence, masked_words = mask_sentence_by_frequency(sentence, percentile)\n",
    "    new_sentence, predicted_words = masked_word_prediction(masked_sentence)\n",
    "    return new_sentence, masked_words, predicted_words\n",
    "\n",
    "\n",
    "# Mask word based on their frequency, making the lower \"percentile\" passed. \n",
    "def mask_sentence_by_frequency(sentence, percentile):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    \n",
    "    masked_words = []\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(tokenized_sentence):\n",
    "        if word in wiki_freq_dict:\n",
    "            word_frequencies[word] = wiki_freq_dict[word]\n",
    "            \n",
    "    sorted_word_frequencies = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item: item[1])}\n",
    "\n",
    "    n=int(len(sorted_word_frequencies)*percentile)\n",
    "\n",
    "    low_frequency_words = {k: sorted_word_frequencies[k] for k in list(sorted_word_frequencies)[:n]}\n",
    "\n",
    "    # mask words, and keep track of what words were masked\n",
    "    for i, word in enumerate(tokenized_sentence):\n",
    "        if word in low_frequency_words:\n",
    "            masked_words.append(word)\n",
    "            tokenized_sentence[i] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "\n",
    "def augment_answers(orig_answer, orig_answer_start, augmented_paragraph, masked_words, predicted_words):\n",
    "    # Find answers in the augmented paragraph.\n",
    "    # Returns new answer and new start position\n",
    "    # Throws assertion if augmented answer isn't found. \n",
    "\n",
    "#     augmented_answer_re = orig_answer\n",
    "    expanded_answer = expand_contractions(orig_answer)\n",
    "    \n",
    "    # Escape some chars\n",
    "#     augmented_answer_re = re.escape(expanded_answer)\n",
    "\n",
    "    # a terrible way to escape to keep spaces unescaped, might instead use re.escape and then unescape spaces. \n",
    "    augmented_answer_re = expanded_answer \\\n",
    "        .replace(\"\\\\\", \"\\\\\\\\\") \\\n",
    "        .replace(\")\", \"\\)\") \\\n",
    "        .replace(\"(\", \"\\(\") \\\n",
    "        .replace(\"]\", \"\\]\") \\\n",
    "        .replace(\"[\", \"\\[\") \\\n",
    "        .replace(\"}\", \"\\}\") \\\n",
    "        .replace(\"{\", \"\\{\") \\\n",
    "        .replace(\">\", \"\\>\") \\\n",
    "        .replace(\"<\", \"\\<\") \\\n",
    "        .replace('^', '\\^') \\\n",
    "        .replace('|', '\\|') \\\n",
    "        .replace('*', '\\*') \\\n",
    "        .replace('?', '\\?') \\\n",
    "        .replace('+', '\\+') \\\n",
    "        .replace(\"``\", '\"')\n",
    "\n",
    "    # Ignore whitespace around some chars\n",
    "    augmented_answer_re = re.sub(r'([.,$=/\\-\"\\'])', r' *\\1 *', augmented_answer_re.strip())\n",
    "    augmented_answer_re = augmented_answer_re.replace('$', '\\$')\n",
    "\n",
    "    already_swapped = []\n",
    "    for masked_word, predicted_word in zip(masked_words, predicted_words):\n",
    "        if re.escape(masked_word) != re.escape(predicted_word) and not masked_word in already_swapped:\n",
    "            already_swapped.append(masked_word)\n",
    "            word_pair_re = \" *(\" + re.escape(masked_word) \n",
    "            \n",
    "            \n",
    "            for sub_masked_word, sub_predicted_word in zip(masked_words, predicted_words):\n",
    "                if sub_masked_word == masked_word:\n",
    "                    word_pair_re += \"|\" + re.escape(sub_predicted_word)\n",
    "\n",
    "            word_pair_re += \") *\"\n",
    "\n",
    "            # Swap out masked words with regex matching either as whole word. \n",
    "            augmented_answer_re = re.sub(r'\\b'+re.escape(masked_word)+r'\\b', word_pair_re, augmented_answer_re)\n",
    "\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * * *', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * *', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace(' * ', ' *')\n",
    "    augmented_answer_re = augmented_answer_re.replace('  ', ' ')\n",
    "\n",
    "#     print(\"augmented_answer_re:\\n{}\\naugmented_paragraph:\\n{}\\n\\n\\n\".format(r'('+augmented_answer_re+r')', augmented_paragraph))\n",
    "\n",
    "    # Find all the matches\n",
    "    matches = re.finditer(r'('+augmented_answer_re+r')', augmented_paragraph, re.IGNORECASE)\n",
    "    \n",
    "    # Find the match closest in start_position and use that one. \n",
    "    closest_match = None\n",
    "    closest_position_delta = 0\n",
    "    for match in matches:\n",
    "        position_delta = abs(match.span()[0] - orig_answer_start)\n",
    "        \n",
    "        if closest_match is None or position_delta < closest_position_delta:\n",
    "            closest_match = match\n",
    "            closest_position_delta = position_delta\n",
    "            \n",
    "#     assert closest_match, augmented_answer_re\n",
    "    # If there is no match, alert and move on. \n",
    "    if not closest_match:\n",
    "#         print(\"*** Could not find\\nOrig:'{}'\\nAug:'{}' in :\\n{}\\n\".format(orig_answer, augmented_answer_re, augmented_paragraph))\n",
    "        new_answer = None\n",
    "        new_start_pos = None\n",
    "    else:\n",
    "        new_answer = closest_match.group()\n",
    "        new_start_pos = closest_match.span()[0]\n",
    "    \n",
    "    return new_answer, new_start_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching: https://ndownloader.figshare.com/files/21500109?private_link=2f119bea3e8d711047ec\n",
      "Fetching: https://ndownloader.figshare.com/files/21500112?private_link=2f119bea3e8d711047ec\n",
      "Fetching: https://ndownloader.figshare.com/files/21500115?private_link=2f119bea3e8d711047ec\n",
      "Fetching: https://ndownloader.figshare.com/files/21500118?private_link=2f119bea3e8d711047ec\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_JJ_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_JJ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_VB_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_RB_RBR_RBZ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_VB_VBD_VBG_VBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_Percentile_0.1.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_Percentile_0.2.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_Percentile_0.3.json.gz\n",
      "Skipping existing output: /data/augmentation/test/amazon_reviews_v1_0_roberta_Percentile_0.5.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_JJ_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_JJ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_VB_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_RB_RBR_RBZ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_VB_VBD_VBG_VBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_Percentile_0.1.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_Percentile_0.2.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_Percentile_0.3.json.gz\n",
      "Skipping existing output: /data/augmentation/test/reddit_v1_0_roberta_Percentile_0.5.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_JJ_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_JJ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_VB_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_RB_RBR_RBZ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_VB_VBD_VBG_VBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_Percentile_0.1.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_Percentile_0.2.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_Percentile_0.3.json.gz\n",
      "Skipping existing output: /data/augmentation/test/new_wiki_v1.0_roberta_Percentile_0.5.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_JJ_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_JJ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_VB_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_VB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_RB.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_RB_RBR_RBZ.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_VB_VBD_VBG_VBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_RB_RBR_RBZ_VB_VBD_VBGVBN_VBP.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_Percentile_0.1.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_Percentile_0.2.json.gz\n",
      "Skipping existing output: /data/augmentation/test/nyt_v1.0_roberta_Percentile_0.3.json.gz\n",
      "Processing frequncy percentile: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (735 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (572 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (653 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (741 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (614 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (659 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (674 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1001 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writting: /data/augmentation/test/nyt_v1.0_roberta_Percentile_0.5.json.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def process_qa(qa_json, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "\n",
    "    if parts_of_speech:\n",
    "        print(f\"Processing parts of speech: {parts_of_speech}\".format(qa_json, parts_of_speech))\n",
    "    elif frequency_percentile:\n",
    "        print(f\"Processing frequncy percentile: {frequency_percentile}\")\n",
    "        \n",
    "    \n",
    "    total_time = 0\n",
    "    paragraph_count = 0\n",
    "\n",
    "    results = {}\n",
    "    count = 0\n",
    "\n",
    "    augmented_json = {\"data\": []}\n",
    "\n",
    "    paragraphs = []\n",
    "    augmented_json[\"data\"].append(paragraphs)\n",
    "\n",
    "    ## JSON Format\n",
    "    # -data\n",
    "    #   -paragraphs\n",
    "    #     -context\n",
    "    #     -qas\n",
    "    #       -question\n",
    "    #       -id\n",
    "    #       -answers\n",
    "    #          -text\n",
    "    #          -answer_start\n",
    "    #   -title\n",
    "    #   -split\n",
    "\n",
    "#     with open(qa_json) as json_file:\n",
    "#         qa_json = json.load(json_file)['data']\n",
    "#     qa_json = get_json_url(qa_json)['data']\n",
    "\n",
    "\n",
    "    new_data = []\n",
    "    new_splits = []\n",
    "    new_data.append(new_splits)\n",
    "    new_json = {\"data\": new_data}\n",
    "\n",
    "    new_splits = []\n",
    "    new_json = {\"data\": new_splits}\n",
    "\n",
    "    for splits in qa_json:\n",
    "        paragraphs = splits['paragraphs']\n",
    "\n",
    "        new_paragraphs = []\n",
    "        new_split = {\"paragraphs\": new_paragraphs, \"split\": splits['split'], \"title\": splits['title']}\n",
    "        new_splits.append(new_split)\n",
    "\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            start = timeit.default_timer()\n",
    "            count += 1\n",
    "\n",
    "            if parts_of_speech:\n",
    "                augmented_paragraph, masked_words, predicted_words = augment_language_by_part(\n",
    "                    paragraph['context'], parts_of_speech\n",
    "                )\n",
    "            elif frequency_percentile:\n",
    "                augmented_paragraph, masked_words, predicted_words = augment_language_by_frequency(\n",
    "                    paragraph['context'], frequency_percentile\n",
    "                )\n",
    "\n",
    "            new_qas = []\n",
    "            new_paragraph = {\"context\": augmented_paragraph, \"qas\": new_qas}\n",
    "            new_paragraphs.append(new_paragraph)\n",
    "\n",
    "            for qa in paragraph['qas']:\n",
    "                #TODO Skip any with missing answers\n",
    "                keep_question = True\n",
    "\n",
    "                new_answers = []\n",
    "\n",
    "                for answer in qa['answers']:\n",
    "                    aug_answer_text, aug_answer_start = augment_answers(answer['text'], answer['answer_start'], augmented_paragraph, masked_words, predicted_words)\n",
    "\n",
    "                    if aug_answer_start is None:\n",
    "                        keep_question = False\n",
    "\n",
    "                    new_answer = {\"answer_start\": aug_answer_start, \"text\": aug_answer_text}\n",
    "                    new_answers.append(new_answer)\n",
    "\n",
    "                if keep_question:\n",
    "                    new_qas.append({\"answers\": new_answers, \"id\": qa['id'], \"question\": qa['question']})\n",
    "\n",
    "\n",
    "\n",
    "            stop = timeit.default_timer()\n",
    "            run_time = stop - start\n",
    "\n",
    "            total_time += run_time\n",
    "            paragraph_count += 1\n",
    "    return new_json\n",
    "\n",
    "def get_augmented_filename(output_dir, question_set, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "    \n",
    "    if parts_of_speech:\n",
    "        filename = output_dir+question_set+\"_\"+model_type+\"_\"+\"_\".join(parts_of_speech)+\".json.gz\"\n",
    "    elif frequency_percentile:\n",
    "        filename = output_dir+question_set+\"_\"+model_type+\"_Percentile_\"+str(frequency_percentile)+\".json.gz\"\n",
    "    \n",
    "    return filename\n",
    "    \n",
    "\n",
    "def write_json(augmented_qa_json, output_dir, question_set, parts_of_speech=None, frequency_percentile=None):\n",
    "    assert bool(parts_of_speech) ^ bool(frequency_percentile), \"Can only pass one of parts_of_speech and frequency_percentile\"\n",
    "    filename = get_augmented_filename(output_dir, question_set, parts_of_speech, frequency_percentile)\n",
    "\n",
    "    return write_gzip_json(filename)\n",
    "\n",
    "\n",
    "## These dicts are stored in proj_config.py.  Uncomment here to override and try different variations. \n",
    "# qa_urls = {\n",
    "#     \"amazon_reviews_v1_0\": 'https://ndownloader.figshare.com/files/21500109?private_link=2f119bea3e8d711047ec',\n",
    "#     \"reddit_v1_0\": 'https://ndownloader.figshare.com/files/21500112?private_link=2f119bea3e8d711047ec',\n",
    "#     \"new_wiki_v1.0\": 'https://ndownloader.figshare.com/files/21500115?private_link=2f119bea3e8d711047ec',\n",
    "#     \"nyt_v1.0\": 'https://ndownloader.figshare.com/files/21500118?private_link=2f119bea3e8d711047ec',\n",
    "# }\n",
    "\n",
    "# parts_of_speech_list = [\n",
    "#     ['JJ', 'VB'],\n",
    "#     ['JJ'],\n",
    "#     ['VB', 'RB'],\n",
    "#     ['VB'],\n",
    "#     ['RB'],\n",
    "#     ['RB', 'RBR', 'RBZ'],\n",
    "#     ['VB', 'VBD', 'VBG', 'VBN', 'VBP'],\n",
    "#     ['RB', 'RBR', 'RBZ', 'VB', 'VBD', 'VBG' 'VBN', 'VBP']\n",
    "# ]\n",
    "\n",
    "# frequency_percentiles = [\n",
    "#     0.10,\n",
    "#     0.20,\n",
    "#     0.30,\n",
    "#     0.50\n",
    "# ]\n",
    "\n",
    "qa_files = {}\n",
    "\n",
    "# Cache the test sets\n",
    "for name, url in qa_urls.items():\n",
    "    qa_files[name] = get_json_url(url)['data']\n",
    "\n",
    "for question_set, filename in qa_files.items():\n",
    "    for parts_of_speech in parts_of_speech_list:\n",
    "        filepath = get_augmented_filename(output_dir, question_set, parts_of_speech=parts_of_speech)\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            augmented_qa_json = process_qa(filename, parts_of_speech=parts_of_speech)\n",
    "#             write_json(augmented_qa_json, output_dir, question_set, parts_of_speech=parts_of_speech)\n",
    "            filename = get_augmented_filename(output_dir, question_set, parts_of_speech=parts_of_speech)\n",
    "            write_gzip_json(filename, augmented_qa_json)\n",
    "        else:\n",
    "            print(\"Skipping existing output: {}\".format(filepath))\n",
    "        \n",
    "    for frequency_percentile in frequency_percentiles:\n",
    "        filepath = get_augmented_filename(output_dir, question_set, frequency_percentile=frequency_percentile)\n",
    "        if not os.path.exists(filepath):\n",
    "            augmented_qa_json = process_qa(filename, frequency_percentile=frequency_percentile)\n",
    "#             write_json(augmented_qa_json, output_dir, question_set, frequency_percentile=frequency_percentile)\n",
    "            filename = get_augmented_filename(output_dir, question_set, frequency_percentile=frequency_percentile)\n",
    "            write_gzip_json(filename, augmented_qa_json)\n",
    "        else:\n",
    "            print(\"Skipping existing output: {}\".format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask word based on their frequency, making the lower \"percentile\" passed. \n",
    "def mask_sentence_frequency(sentence, percentile):\n",
    "    tokenized_sentence = word_tokenize(expand_contractions(sentence))\n",
    "    \n",
    "    masked_words = []\n",
    "    word_frequencies = {}\n",
    "    \n",
    "    \n",
    "    for idx, word in enumerate(tokenized_sentence):\n",
    "        if word in wiki_freq_dict:\n",
    "            word_frequencies[word] = wiki_freq_dict[word]\n",
    "            \n",
    "    sorted_word_frequencies = {k: v for k, v in sorted(word_frequencies.items(), key=lambda item: item[1])}\n",
    "\n",
    "    n=int(len(sorted_word_frequencies)*percentile)\n",
    "\n",
    "    low_frequency_words = {k: sorted_word_frequencies[k] for k in list(sorted_word_frequencies)[:n]}\n",
    "\n",
    "    # mask words, and keep track of what words were masked\n",
    "    for i, word in enumerate(tokenized_sentence):\n",
    "        if word in low_frequency_words:\n",
    "            masked_words.append(word)\n",
    "            tokenized_sentence[i] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence), masked_words\n",
    "\n",
    "paragraph = \"It's a very nice holder - not too big and not too small. It fits any lipstick, lip gloss, chapstick, etc nicely. I love that I'm able to see what I have and not have to dig through a makeup bag anymore. I would highly recommend.\"\n",
    "paragraph = \"First of all, this thing is freakin' awesome. My wife put it together while I did some work, she assembled all the glass shelves in to the frame. We hung it up with 6 BIG 4\\\" wall toggle bolts. On the top shelf I house an external WD Hdd, a WDTV Live and a Actiontec HDMI WiFi Transmitter. On the second shelf I have my Dish Network 722 (fits perfectly even with all the wires coming out the back). On the bottom shelf I have an old Ken wood Stereo Receive (fits, but the front legs come off just a bit but this doesn't affect stability. The receiver measures 17\\\"w x 14\\\"d x 5\\\"h.The wire maintenance was a little trick. We used a wire coat hanger and some masking tape to thread the bulky wires through the back.This unit installed is an amazing addition to your small living space, I highly recommend it!\"\n",
    "\n",
    "masked_sentence, word_mask = mask_sentence_frequency(paragraph, 0.25)\n",
    "print(masked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_json = '/data/distribution_shift/new_qa/amazon_reviews_v1.0.json'\n",
    "with open(qa_json) as json_file:\n",
    "    qa_json = json.load(json_file)['data']\n",
    "    \n",
    "for splits in qa_json:\n",
    "    print(json.dumps(splits, indent=4, sort_keys=True))\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
