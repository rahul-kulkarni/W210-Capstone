{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "# from transformers import BertForQuestionAnswering, AutoModelForQuestionAnswering, AutoTokenizer\n",
    "# from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BERT or RoBERTa models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'bert'\n",
    "\n",
    "if model_type == 'bert':\n",
    "    # Download (Using cased to maintain case in output)\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    model = BertForMaskedLM.from_pretrained('bert-large-cased')\n",
    "    model_mask = '[MASK]'\n",
    "    \n",
    "elif model_type == 'roberta':\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "    model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "    model_mask = '<mask>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "def bert_named_entity(sentence):\n",
    "    global model_type\n",
    "    \n",
    "    sentence_token_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "    tonized_sentence = tokenizer.tokenize(sentence, return_tensors='pt')\n",
    "\n",
    "    # Get and format positions of work masks\n",
    "    mask_positions_2d = (sentence_token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
    "    mask_positions = [mask.item() for mask in mask_positions_2d ]\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sentence_token_ids)\n",
    "\n",
    "    last_hidden_state = output[0].squeeze()\n",
    "\n",
    "    \n",
    "    token_predictions_list =[]\n",
    "    for mask_index in mask_positions:\n",
    "        mask_hidden_state = last_hidden_state[mask_index]\n",
    "        # This isn't really required unless we want > 1 predicted word per mask. \n",
    "        idx = torch.topk(mask_hidden_state, k=5, dim=0)[1]\n",
    "        \n",
    "        words = None\n",
    "        if model_type == 'roberta':\n",
    "            words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
    "        elif model_type == 'bert':\n",
    "            words = tokenizer.decode(idx).split(' ')\n",
    "\n",
    "        token_predictions_list.append(words)\n",
    "        \n",
    "    # Make sure we have a list of predictions for each mask. \n",
    "    assert len(mask_positions) == len(token_predictions_list)\n",
    "\n",
    "    # Replace masks with predicted words\n",
    "    #   Make a copy so we can calculate shape differences, required for answer_start in QnA.\n",
    "    predicted_sentence = copy(tonized_sentence) \n",
    "    for pos, new_word in zip(mask_positions, token_predictions_list):\n",
    "        \n",
    "        # Add the weird G at the front of predicted words, so they look like recognized tokens for detokenization. \n",
    "        if model_type == 'roberta':\n",
    "            predicted_word = 'Ġ'+new_word[0]\n",
    "        else:\n",
    "            predicted_word = new_word[0]\n",
    "        \n",
    "        predicted_sentence[pos-1] = predicted_word\n",
    "\n",
    "    return detokenize(predicted_sentence)\n",
    "\n",
    "def detokenize(tokenized_text):\n",
    "    global model_type\n",
    "    punctuation=\"?:!.,;\"\n",
    "    \n",
    "    detokenized_sentence=''\n",
    "    for idx, token in enumerate(tokenized_text):\n",
    "        if idx == 0:\n",
    "            detokenized_sentence = token\n",
    "        elif model_type == 'roberta' and token[0:1] != 'Ġ':\n",
    "            detokenized_sentence += token\n",
    "        elif token[0:2] == '##':\n",
    "            detokenized_sentence += token[2:]\n",
    "        elif token in punctuation:\n",
    "            detokenized_sentence += token\n",
    "        elif detokenized_sentence[-1:] == \"'\" or token[0:1] == \"'\":\n",
    "            detokenized_sentence += token\n",
    "        else:\n",
    "            detokenized_sentence += ' '+token\n",
    "        \n",
    "    if model_type == 'roberta':\n",
    "        detokenized_sentence = detokenized_sentence.replace('Ġ', '')\n",
    "    \n",
    "    return detokenized_sentence\n",
    "\n",
    "\n",
    "# CC coordinating conjunction\n",
    "# CD cardinal digit\n",
    "# DT determiner\n",
    "# EX existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "# FW foreign word\n",
    "# IN preposition/subordinating conjunction\n",
    "# JJ adjective 'big'\n",
    "# JJR adjective, comparative 'bigger'\n",
    "# JJS adjective, superlative 'biggest'\n",
    "# LS list marker 1)\n",
    "# MD modal could, will\n",
    "# NN noun, singular 'desk'\n",
    "# NNS noun plural 'desks'\n",
    "# NNP proper noun, singular 'Harrison'\n",
    "# NNPS proper noun, plural 'Americans'\n",
    "# PDT predeterminer 'all the kids'\n",
    "# POS possessive ending parent's\n",
    "# PRP personal pronoun I, he, she\n",
    "# PRP$ possessive pronoun my, his, hers\n",
    "# RB adverb very, silently,\n",
    "# RBR adverb, comparative better\n",
    "# RBS adverb, superlative best\n",
    "# RP particle give up\n",
    "# TO to go 'to' the store.\n",
    "# UH interjection errrrrrrrm\n",
    "# VB verb, base form take\n",
    "# VBD verb, past tense took\n",
    "# VBG verb, gerund/present participle taking\n",
    "# VBN verb, past participle taken\n",
    "# VBP verb, sing. present, non-3d take\n",
    "# VBZ verb, 3rd person sing. present takes\n",
    "# WDT wh-determiner which\n",
    "# WP wh-pronoun who, what\n",
    "# WP$ possessive wh-pronoun whose\n",
    "# WRB wh-abverb where, when\n",
    "\n",
    "# Pass sentence and list of parts of speach to be masked.\n",
    "def mask_sentence_by_part(sentence, part):\n",
    "    tokenized_sentence = word_tokenize(sentence)\n",
    "    word_tags = nltk.pos_tag(tokenized_sentence)\n",
    "    \n",
    "    for idx, word_tag in enumerate(word_tags):\n",
    "        if (word_tag[1] in part):\n",
    "            tokenized_sentence[idx] = model_mask\n",
    "\n",
    "    return TreebankWordDetokenizer().detokenize(tokenized_sentence)\n",
    "\n",
    "def augment_language(sentence, part):\n",
    "    masked_sentence = mask_sentence_by_part(sentence, part)\n",
    "    return bert_named_entity(masked_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: It's a very nice holder - not too big and not too small.\n",
      " after: It's a very good holder - not too big and not too small.\n",
      "\n",
      "before: It fits any lipstick, lip gloss, chapstick, etc nicely.\n",
      " after: It fits any lipstick, lip gloss, chapstick, etc nicely.\n",
      "\n",
      "before: I love that I'm able to see what I have and not have to dig through a makeup bag anymore.\n",
      " after: I love that I'm able to enjoy what I have and not have to go through a makeup bag anymore.\n",
      "\n",
      "before: I would highly recommend.\n",
      " after: I would highly disagree.\n",
      "\n",
      "before: This product takes 10 minutes to setup 4.\n",
      " after: This product takes 10 minutes to $. 4.\n",
      "\n",
      "before: It is super easy.\n",
      " after: It is very rare.\n",
      "\n",
      "before: Very easy on the eyes.\n",
      " after: Very easy on the eyes.\n",
      "\n",
      "before: It makes the room look like you have a sound system.\n",
      " after: It makes the room look like you have a sound system.\n",
      "\n",
      "before: I used my simple SONY HTIB.The product is stong on the bottom so it's not going to tilt over.\n",
      " after: I used my favorite SONY HTIB. The product is sealed on the bottom so it's not going to spill over.\n",
      "\n",
      "before: The extension part is a bit weak.\n",
      " after: The extension part is a bit shorter.\n",
      "\n",
      "before: I have it fully extended as I guess most people would like them.\n",
      " after: I have it fully extended as I guess most people would have them.\n",
      "\n",
      "before: The product because to lose strength and stand a bit off centered.\n",
      " after: The product because to its strength and is a bit off centered.\n",
      "\n",
      "before: It leans to a side.\n",
      " after: It leans to a side.\n",
      "\n",
      "before: I wouldn't return them!\n",
      " after: I wouldn't let them!\n",
      "\n",
      "before: Just keep in mind that they are a great value and look great.\n",
      " after: Just bear in mind that they are a good value and a resource.\n",
      "\n",
      "before: Just has a clumsy feel to it.\n",
      " after: Just has a different feel to it.\n",
      "\n",
      "before: I tighten them every week when i see them leaning too much.\n",
      " after: I tighten them every week when i see them leaning too far.\n",
      "\n",
      "before: I have always kept a dustbuster in my kitchen for quick clean ups.\n",
      " after: I have always kept a dustbuster in my kitchen for clean clean ups.\n",
      "\n",
      "before: They are always charged.\n",
      " after: They are always charged.\n",
      "\n",
      "before: Not with this new 18 volt model.\n",
      " after: Not with this Model 18 \" model.\n",
      "\n",
      "before: Not only does is not have much suction power, you can't keep the battery plugged in so it is always charged and ready to go.\n",
      " after: Not only does is not have have suction power, you can't get the battery plugged in so it is always charged and ready to charge.\n",
      "\n",
      "before: If you don't use it for 4-5 days, the charge depletes!\n",
      " after: If you don't use it for 30 days, the charge depletes!\n",
      "\n",
      "before: So then you have to recharge it for 14-16 hours.\n",
      " after: So then you have to do it for two hours.\n",
      "\n",
      "before: Not convenient at all.\n",
      " after: Not convenient at all.\n",
      "\n",
      "before: So I returned the dustbuster.\n",
      " after: So I returned the dustbuster.\n",
      "\n",
      "before: I then ordered a Eureka that works great and can be left plugged in, permanently.\n",
      " after: I then ordered a Eureka that works, and can be left plugged in, permanently.\n",
      "\n",
      "before: The model number is H96.\n",
      " after: The model number is H96.\n",
      "\n",
      "before: It is not as large as it looks in the photos and is a great buy; not to mention it costs less than the inefficient new dustbusters.\n",
      " after: It is not as good as it looks in the photos and is a cheap buy; not to mention it costs less than the best best dustbusters.\n",
      "\n",
      "before: The new dustbuster models are now just cheap knock offs of the original great machines.\n",
      " after: The original dustbuster models are now just cheap knock offs of the original original machines.\n",
      "\n",
      "before: Black and Decker in their attempt to make a higher profit by cheapening a previously great product, destroyed a really good thing.I strongly recommend the Eureka below:Eureka Quick Up Cordless 2 in 1, 96Hproduct.\n",
      " after: Black and Decker in their attempt to make a higher profit by cheapening a previously profitable product, destroyed a really good thing. I strongly recommend the Eureka below: Eureka Quick Up Cordless 2 in 1, 96Hproduct.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "paragraph = \"It's a very nice holder - not too big and not too small. It fits any lipstick, lip gloss, chapstick, etc nicely. I love that I'm able to see what I have and not have to dig through a makeup bag anymore. I would highly recommend.\"\n",
    "paragraph += \" This product takes 10 minutes to setup 4. It is super easy. Very easy on the eyes. It makes the room look like you have a sound system. I used my simple SONY HTIB.The product is stong on the bottom so it's not going to tilt over. The extension part is a bit weak. I have it fully extended as I guess most people would like them. The product because to lose strength and stand a bit off centered. It leans to a side. I wouldn't return them! Just keep in mind that they are a great value and look great. Just has a clumsy feel to it. I tighten them every week when i see them leaning too much.\"\n",
    "paragraph += \" I have always kept a dustbuster in my kitchen for quick clean ups. They are always charged. Not with this new 18 volt model. Not only does is not have much suction power, you can't keep the battery plugged in so it is always charged and ready to go. If you don't use it for 4-5 days, the charge depletes! So then you have to recharge it for 14-16 hours. Not convenient at all. So I returned the dustbuster. I then ordered a Eureka that works great and can be left plugged in, permanently. The model number is H96. It is not as large as it looks in the photos and is a great buy; not to mention it costs less than the inefficient new dustbusters. The new dustbuster models are now just cheap knock offs of the original great machines. Black and Decker in their attempt to make a higher profit by cheapening a previously great product, destroyed a really good thing.I strongly recommend the Eureka below:Eureka Quick Up Cordless 2 in 1, 96Hproduct.\"\n",
    "sentences = nltk.tokenize.sent_tokenize(paragraph)\n",
    "\n",
    "for sentence in sentences:\n",
    "     print(\"before: {}\\n after: {}\\n\".format(sentence, augment_language(sentence, ['JJ', 'VB'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at tag types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('It', 'PRP')\n",
      "(\"'s\", 'VBZ')\n",
      "('a', 'DT')\n",
      "('very', 'RB')\n",
      "('nice', 'JJ')\n",
      "('holder', 'NN')\n",
      "('-', ':')\n",
      "('not', 'RB')\n",
      "('too', 'RB')\n",
      "('big', 'JJ')\n",
      "('and', 'CC')\n",
      "('not', 'RB')\n",
      "('too', 'RB')\n",
      "('small', 'JJ')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('fits', 'VBZ')\n",
      "('any', 'DT')\n",
      "('lipstick', 'NN')\n",
      "(',', ',')\n",
      "('lip', 'NN')\n",
      "('gloss', 'NN')\n",
      "(',', ',')\n",
      "('chapstick', 'NN')\n",
      "(',', ',')\n",
      "('etc', 'FW')\n",
      "('nicely', 'RB')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('love', 'VBP')\n",
      "('that', 'IN')\n",
      "('I', 'PRP')\n",
      "(\"'m\", 'VBP')\n",
      "('able', 'JJ')\n",
      "('to', 'TO')\n",
      "('see', 'VB')\n",
      "('what', 'WP')\n",
      "('I', 'PRP')\n",
      "('have', 'VBP')\n",
      "('and', 'CC')\n",
      "('not', 'RB')\n",
      "('have', 'VB')\n",
      "('to', 'TO')\n",
      "('dig', 'VB')\n",
      "('through', 'IN')\n",
      "('a', 'DT')\n",
      "('makeup', 'NN')\n",
      "('bag', 'NN')\n",
      "('anymore', 'RB')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('would', 'MD')\n",
      "('highly', 'RB')\n",
      "('recommend', 'VB')\n",
      "('.', '.')\n",
      "('This', 'DT')\n",
      "('product', 'NN')\n",
      "('takes', 'VBZ')\n",
      "('10', 'CD')\n",
      "('minutes', 'NNS')\n",
      "('to', 'TO')\n",
      "('setup', 'VB')\n",
      "('4', 'CD')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('super', 'JJ')\n",
      "('easy', 'JJ')\n",
      "('.', '.')\n",
      "('Very', 'RB')\n",
      "('easy', 'JJ')\n",
      "('on', 'IN')\n",
      "('the', 'DT')\n",
      "('eyes', 'NNS')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('makes', 'VBZ')\n",
      "('the', 'DT')\n",
      "('room', 'NN')\n",
      "('look', 'NN')\n",
      "('like', 'IN')\n",
      "('you', 'PRP')\n",
      "('have', 'VBP')\n",
      "('a', 'DT')\n",
      "('sound', 'NN')\n",
      "('system', 'NN')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('used', 'VBD')\n",
      "('my', 'PRP$')\n",
      "('simple', 'JJ')\n",
      "('SONY', 'NNP')\n",
      "('HTIB.The', 'NNP')\n",
      "('product', 'NN')\n",
      "('is', 'VBZ')\n",
      "('stong', 'JJ')\n",
      "('on', 'IN')\n",
      "('the', 'DT')\n",
      "('bottom', 'NN')\n",
      "('so', 'IN')\n",
      "('it', 'PRP')\n",
      "(\"'s\", 'VBZ')\n",
      "('not', 'RB')\n",
      "('going', 'VBG')\n",
      "('to', 'TO')\n",
      "('tilt', 'VB')\n",
      "('over', 'IN')\n",
      "('.', '.')\n",
      "('The', 'DT')\n",
      "('extension', 'NN')\n",
      "('part', 'NN')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('bit', 'NN')\n",
      "('weak', 'JJ')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('have', 'VBP')\n",
      "('it', 'PRP')\n",
      "('fully', 'RB')\n",
      "('extended', 'VBD')\n",
      "('as', 'IN')\n",
      "('I', 'PRP')\n",
      "('guess', 'VBP')\n",
      "('most', 'JJS')\n",
      "('people', 'NNS')\n",
      "('would', 'MD')\n",
      "('like', 'VB')\n",
      "('them', 'PRP')\n",
      "('.', '.')\n",
      "('The', 'DT')\n",
      "('product', 'NN')\n",
      "('because', 'IN')\n",
      "('to', 'TO')\n",
      "('lose', 'VB')\n",
      "('strength', 'NN')\n",
      "('and', 'CC')\n",
      "('stand', 'VB')\n",
      "('a', 'DT')\n",
      "('bit', 'NN')\n",
      "('off', 'RP')\n",
      "('centered', 'VBN')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('leans', 'VBZ')\n",
      "('to', 'TO')\n",
      "('a', 'DT')\n",
      "('side', 'NN')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('would', 'MD')\n",
      "(\"n't\", 'RB')\n",
      "('return', 'VB')\n",
      "('them', 'PRP')\n",
      "('!', '.')\n",
      "('Just', 'RB')\n",
      "('keep', 'VB')\n",
      "('in', 'IN')\n",
      "('mind', 'NN')\n",
      "('that', 'IN')\n",
      "('they', 'PRP')\n",
      "('are', 'VBP')\n",
      "('a', 'DT')\n",
      "('great', 'JJ')\n",
      "('value', 'NN')\n",
      "('and', 'CC')\n",
      "('look', 'VB')\n",
      "('great', 'JJ')\n",
      "('.', '.')\n",
      "('Just', 'NNP')\n",
      "('has', 'VBZ')\n",
      "('a', 'DT')\n",
      "('clumsy', 'JJ')\n",
      "('feel', 'NN')\n",
      "('to', 'TO')\n",
      "('it', 'PRP')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('tighten', 'VBP')\n",
      "('them', 'PRP')\n",
      "('every', 'DT')\n",
      "('week', 'NN')\n",
      "('when', 'WRB')\n",
      "('i', 'NN')\n",
      "('see', 'VBP')\n",
      "('them', 'PRP')\n",
      "('leaning', 'VBG')\n",
      "('too', 'RB')\n",
      "('much', 'JJ')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('have', 'VBP')\n",
      "('always', 'RB')\n",
      "('kept', 'VBN')\n",
      "('a', 'DT')\n",
      "('dustbuster', 'NN')\n",
      "('in', 'IN')\n",
      "('my', 'PRP$')\n",
      "('kitchen', 'NN')\n",
      "('for', 'IN')\n",
      "('quick', 'JJ')\n",
      "('clean', 'JJ')\n",
      "('ups', 'NNS')\n",
      "('.', '.')\n",
      "('They', 'PRP')\n",
      "('are', 'VBP')\n",
      "('always', 'RB')\n",
      "('charged', 'VBN')\n",
      "('.', '.')\n",
      "('Not', 'RB')\n",
      "('with', 'IN')\n",
      "('this', 'DT')\n",
      "('new', 'JJ')\n",
      "('18', 'CD')\n",
      "('volt', 'JJ')\n",
      "('model', 'NN')\n",
      "('.', '.')\n",
      "('Not', 'RB')\n",
      "('only', 'RB')\n",
      "('does', 'VBZ')\n",
      "('is', 'VBZ')\n",
      "('not', 'RB')\n",
      "('have', 'VB')\n",
      "('much', 'JJ')\n",
      "('suction', 'NN')\n",
      "('power', 'NN')\n",
      "(',', ',')\n",
      "('you', 'PRP')\n",
      "('ca', 'MD')\n",
      "(\"n't\", 'RB')\n",
      "('keep', 'VB')\n",
      "('the', 'DT')\n",
      "('battery', 'NN')\n",
      "('plugged', 'VBD')\n",
      "('in', 'IN')\n",
      "('so', 'IN')\n",
      "('it', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('always', 'RB')\n",
      "('charged', 'VBN')\n",
      "('and', 'CC')\n",
      "('ready', 'JJ')\n",
      "('to', 'TO')\n",
      "('go', 'VB')\n",
      "('.', '.')\n",
      "('If', 'IN')\n",
      "('you', 'PRP')\n",
      "('do', 'VBP')\n",
      "(\"n't\", 'RB')\n",
      "('use', 'VB')\n",
      "('it', 'PRP')\n",
      "('for', 'IN')\n",
      "('4-5', 'JJ')\n",
      "('days', 'NNS')\n",
      "(',', ',')\n",
      "('the', 'DT')\n",
      "('charge', 'NN')\n",
      "('depletes', 'VBZ')\n",
      "('!', '.')\n",
      "('So', 'RB')\n",
      "('then', 'RB')\n",
      "('you', 'PRP')\n",
      "('have', 'VBP')\n",
      "('to', 'TO')\n",
      "('recharge', 'VB')\n",
      "('it', 'PRP')\n",
      "('for', 'IN')\n",
      "('14-16', 'JJ')\n",
      "('hours', 'NNS')\n",
      "('.', '.')\n",
      "('Not', 'RB')\n",
      "('convenient', 'JJ')\n",
      "('at', 'IN')\n",
      "('all', 'DT')\n",
      "('.', '.')\n",
      "('So', 'CC')\n",
      "('I', 'PRP')\n",
      "('returned', 'VBD')\n",
      "('the', 'DT')\n",
      "('dustbuster', 'NN')\n",
      "('.', '.')\n",
      "('I', 'PRP')\n",
      "('then', 'RB')\n",
      "('ordered', 'VBD')\n",
      "('a', 'DT')\n",
      "('Eureka', 'NNP')\n",
      "('that', 'WDT')\n",
      "('works', 'VBZ')\n",
      "('great', 'JJ')\n",
      "('and', 'CC')\n",
      "('can', 'MD')\n",
      "('be', 'VB')\n",
      "('left', 'VBN')\n",
      "('plugged', 'VBN')\n",
      "('in', 'IN')\n",
      "(',', ',')\n",
      "('permanently', 'RB')\n",
      "('.', '.')\n",
      "('The', 'DT')\n",
      "('model', 'NN')\n",
      "('number', 'NN')\n",
      "('is', 'VBZ')\n",
      "('H96', 'NNP')\n",
      "('.', '.')\n",
      "('It', 'PRP')\n",
      "('is', 'VBZ')\n",
      "('not', 'RB')\n",
      "('as', 'RB')\n",
      "('large', 'JJ')\n",
      "('as', 'IN')\n",
      "('it', 'PRP')\n",
      "('looks', 'VBZ')\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "('photos', 'NN')\n",
      "('and', 'CC')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('great', 'JJ')\n",
      "('buy', 'NN')\n",
      "(';', ':')\n",
      "('not', 'RB')\n",
      "('to', 'TO')\n",
      "('mention', 'VB')\n",
      "('it', 'PRP')\n",
      "('costs', 'VBZ')\n",
      "('less', 'JJR')\n",
      "('than', 'IN')\n",
      "('the', 'DT')\n",
      "('inefficient', 'JJ')\n",
      "('new', 'JJ')\n",
      "('dustbusters', 'NNS')\n",
      "('.', '.')\n",
      "('The', 'DT')\n",
      "('new', 'JJ')\n",
      "('dustbuster', 'NN')\n",
      "('models', 'NNS')\n",
      "('are', 'VBP')\n",
      "('now', 'RB')\n",
      "('just', 'RB')\n",
      "('cheap', 'JJ')\n",
      "('knock', 'NN')\n",
      "('offs', 'NNS')\n",
      "('of', 'IN')\n",
      "('the', 'DT')\n",
      "('original', 'JJ')\n",
      "('great', 'JJ')\n",
      "('machines', 'NNS')\n",
      "('.', '.')\n",
      "('Black', 'NNP')\n",
      "('and', 'CC')\n",
      "('Decker', 'NNP')\n",
      "('in', 'IN')\n",
      "('their', 'PRP$')\n",
      "('attempt', 'NN')\n",
      "('to', 'TO')\n",
      "('make', 'VB')\n",
      "('a', 'DT')\n",
      "('higher', 'JJR')\n",
      "('profit', 'NN')\n",
      "('by', 'IN')\n",
      "('cheapening', 'VBG')\n",
      "('a', 'DT')\n",
      "('previously', 'RB')\n",
      "('great', 'JJ')\n",
      "('product', 'NN')\n",
      "(',', ',')\n",
      "('destroyed', 'VBD')\n",
      "('a', 'DT')\n",
      "('really', 'RB')\n",
      "('good', 'JJ')\n",
      "('thing.I', 'NN')\n",
      "('strongly', 'RB')\n",
      "('recommend', 'VB')\n",
      "('the', 'DT')\n",
      "('Eureka', 'NNP')\n",
      "('below', 'IN')\n",
      "(':', ':')\n",
      "('Eureka', 'NN')\n",
      "('Quick', 'NNP')\n",
      "('Up', 'NNP')\n",
      "('Cordless', 'NNP')\n",
      "('2', 'CD')\n",
      "('in', 'IN')\n",
      "('1', 'CD')\n",
      "(',', ',')\n",
      "('96Hproduct', 'CD')\n",
      "('.', '.')\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentence = word_tokenize(paragraph)\n",
    "word_tags = nltk.pos_tag(tokenized_sentence)\n",
    "\n",
    "for foo in word_tags:\n",
    "    print(foo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
