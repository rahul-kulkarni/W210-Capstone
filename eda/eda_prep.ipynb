{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Stanford CoreNLP server\n",
    "`java -Xmx16g -cp C:\\stanford-corenlp-latest\\stanford-corenlp-4.0.0\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 600 -threads 5 -maxCharLength 100000 -quiet False -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Justin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from hyphen import Hyphenator\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import stanfordnlp\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Uncomment if needed to fix this error:\n",
    "# OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "NLP_PORT = 9002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_PATH = './predictions'\n",
    "TEST_SETS_PATH = './test_sets'\n",
    "MODEL_EVALS_URL = 'https://squad-model-evals.s3-us-west-2.amazonaws.com/model_db.json'\n",
    "\n",
    "#SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT', 'dev-v1.1']\n",
    "SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eval_file(eval_file_path, model_evals_url, overwrite=False):\n",
    "    if (not os.path.exists(eval_file_path)) or overwrite:\n",
    "        r = requests.get(model_evals_url)\n",
    "                        \n",
    "        with open(eval_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(r.text)\n",
    "\n",
    "    else:\n",
    "        print('File Exists')\n",
    "    \n",
    "    \n",
    "\n",
    "def write_output(output_file_path, list_to_write):\n",
    "    fields = list_to_write[0].keys()\n",
    "    \n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, \n",
    "                                    fieldnames=fields,\n",
    "                                    delimiter=',', \n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_MINIMAL )\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(list_to_write)\n",
    "\n",
    "def parse_predictions(prediction_file_path, download=False):\n",
    "    \n",
    "    with open(prediction_file_path) as f:\n",
    "      predictions = json.load(f)\n",
    "\n",
    "\n",
    "    pred_list_test = [{ 'model_display_name': x['name'], \n",
    "      'model_name': x['metadata']['name'], \n",
    "      'description': x['metadata']['description'], \n",
    "      'uuid': x['metadata']['uuid'],\n",
    "      'testbed': x['testbed'],\n",
    "      'predictions': x['predictions']\n",
    "\n",
    "     } for x in predictions]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for r in predictions:\n",
    "\n",
    "      display_name = r['name']\n",
    "      model_name = r['metadata']['name']\n",
    "      description = r['metadata']['description']\n",
    "      uuid = r['metadata']['uuid']\n",
    "      testbed = r['testbed']\n",
    "\n",
    "      for k1, v1 in r['predictions'].items():\n",
    "        if k1 in (SET_NAMES):\n",
    "          if 'bundle' in v1.keys():\n",
    "            test_set = k1\n",
    "            bundle = v1['bundle']\n",
    "\n",
    "            for k2, v2 in v1['data'].items():\n",
    "              qid = k2\n",
    "              predicted_answer = v2\n",
    "              exact_match = v1['scores'][qid]['exact_match']\n",
    "              f1 = v1['scores'][qid]['f1']\n",
    "\n",
    "              pred_list.append( {\n",
    "                'display_name': display_name,\n",
    "                'model_name': model_name,\n",
    "                'description': description,\n",
    "                'uuid': uuid,\n",
    "                'testbed': testbed,\n",
    "                'test_set': test_set,\n",
    "                'qid': qid,\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': float(f1)\n",
    "              })\n",
    "   \n",
    "    return pred_list\n",
    "\n",
    "def load_data(input_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        return [r for r in csv_reader]\n",
    "\n",
    "def parse_answers(answer_file_path):\n",
    "    test_set_answers = [a for a in os.listdir(answer_file_path) if not os.path.isdir('/'.join([answer_file_path, a]))]\n",
    "    answers_list = []\n",
    "    \n",
    "    for f in test_set_answers:\n",
    "      with open('/'.join([TEST_SETS_PATH, f])) as fh:\n",
    "          test_set = f.split('.')[0]\n",
    "          \n",
    "          answers = json.load(fh)['data']\n",
    "          for x in answers:\n",
    "              title = x['title']\n",
    "    \n",
    "              for p in x['paragraphs']:\n",
    "                  context = p['context']\n",
    "    \n",
    "                  for qa in p['qas']:\n",
    "                      question = qa['question']\n",
    "                      question_id = qa['id']\n",
    "    \n",
    "                      for a in qa['answers']:\n",
    "                          answers_list.append(\n",
    "                                  {\n",
    "                                      'test_set': test_set,\n",
    "                                      'question_id': question_id,\n",
    "                                      'title': title,\n",
    "                                      'context': context,\n",
    "                                      'question_text': question,\n",
    "                                      'answer_text': a['text'],\n",
    "                                      'answer_start': a['answer_start']\n",
    "                                  }\n",
    "                              )\n",
    "    return answers_list\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(question_id, predicted_answer, all_answers):\n",
    "    gold_answers = [normalize_answer(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    return max((int(normalize_answer(predicted_answer) == a) for a in gold_answers))\n",
    "\n",
    "def compute_f1(question_id, predicted_answer, all_answers):\n",
    "    gold_toks = [get_tokens(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    pred_toks = get_tokens(predicted_answer)\n",
    "    \n",
    "    f1s = []\n",
    "  \n",
    "    for answer_toks in gold_toks:\n",
    "        common = collections.Counter(answer_toks) & collections.Counter(pred_toks)\n",
    "        num_same = sum(common.values())\n",
    "      \n",
    "        if len(answer_toks) == 0 or len(pred_toks) == 0:\n",
    "            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "            f1s.append(float(int(answer_toks == pred_toks)))\n",
    "            continue\n",
    "        if num_same == 0:\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(answer_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return float(max(f1s))\n",
    "\n",
    "def print_answer(qid, all_answers):\n",
    "    question = [q for q in all_answers if q['question_id'] == qid]\n",
    "    answers = [a['answer_text'] for a in question]\n",
    "    \n",
    "    if question:\n",
    "        print('Test Set:', question[0]['test_set'])\n",
    "        print('Context:', question[0]['context'])\n",
    "        print('Question:', question[0]['question_text'])\n",
    "        print('Answers:', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running. Found 2 sentences\n"
     ]
    }
   ],
   "source": [
    "# Test Server\n",
    "try:\n",
    "    txt = 'This is a test sentence. So is this.'\n",
    "    with CoreNLPClient(endpoint='http://localhost:{}'.format(NLP_PORT), start_server=False, timeout=30000) as client:\n",
    "        ann = client.annotate(txt)\n",
    "        print('Server running. Found {} sentences'.format(len(ann.sentence)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n"
     ]
    }
   ],
   "source": [
    "# Download the model_db.json file that contains all the pre-evaluated and scored questions\n",
    "# from the previous groups' work, if it doesn't exist yet.\n",
    "\n",
    "fetch_eval_file(PREDICTION_PATH + '/model_db.json', MODEL_EVALS_URL, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If predictions and/or answer files don't exist, uncomment these to recreate them\n",
    "\n",
    "predictions = parse_predictions(PREDICTION_PATH + '/model_db.json')\n",
    "answers = parse_answers(TEST_SETS_PATH)\n",
    "\n",
    "write_output(PREDICTION_PATH + '/all_predictions.csv', predictions)\n",
    "write_output(PREDICTION_PATH + '/all_answers.csv', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, load from files\n",
    "predictions = load_data(PREDICTION_PATH + '/all_predictions.csv')\n",
    "answers = load_data(PREDICTION_PATH + '/all_answers.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into Pandas dataframes\n",
    "\n",
    "df_pred = pd.DataFrame(predictions)\n",
    "df_answers = pd.DataFrame(answers)\n",
    "\n",
    "df_pred = df_pred.astype({'f1': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set</th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>91</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106622</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9c9bbd8ae5305bc982f410</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>Sales Professional Development Help? Hello All...</td>\n",
       "      <td>How many people does the company that I work f...</td>\n",
       "      <td>8</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106688</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9ca3b18ae5305bc982f454</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...</td>\n",
       "      <td>How many did he play in?</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106689</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9ca3b18ae5305bc982f454</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...</td>\n",
       "      <td>How many did he play in?</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106760</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9cae448ae5305bc982f492</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>[AMA Request] A scalper / secondary-market tic...</td>\n",
       "      <td>What's the total number of questions the autho...</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106761</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9cae448ae5305bc982f492</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>[AMA Request] A scalper / secondary-market tic...</td>\n",
       "      <td>What's the total number of questions the autho...</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4795 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_set               question_id                     title  \\\n",
       "66      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "67      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "68      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "151     amazon_reviews_v1  5dd4673dcc027a086d65bcec        Amazon_Reviews_295   \n",
       "152     amazon_reviews_v1  5dd4673dcc027a086d65bcec        Amazon_Reviews_295   \n",
       "...                   ...                       ...                       ...   \n",
       "106622          reddit_v1  5d9c9bbd8ae5305bc982f410  Filtered_Reddit_Comments   \n",
       "106688          reddit_v1  5d9ca3b18ae5305bc982f454  Filtered_Reddit_Comments   \n",
       "106689          reddit_v1  5d9ca3b18ae5305bc982f454  Filtered_Reddit_Comments   \n",
       "106760          reddit_v1  5d9cae448ae5305bc982f492  Filtered_Reddit_Comments   \n",
       "106761          reddit_v1  5d9cae448ae5305bc982f492  Filtered_Reddit_Comments   \n",
       "\n",
       "                                                  context  \\\n",
       "66      i wanted an electric kettle, but landed up ord...   \n",
       "67      i wanted an electric kettle, but landed up ord...   \n",
       "68      i wanted an electric kettle, but landed up ord...   \n",
       "151     I ordered these sheets and must say was a bit ...   \n",
       "152     I ordered these sheets and must say was a bit ...   \n",
       "...                                                   ...   \n",
       "106622  Sales Professional Development Help? Hello All...   \n",
       "106688  David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...   \n",
       "106689  David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...   \n",
       "106760  [AMA Request] A scalper / secondary-market tic...   \n",
       "106761  [AMA Request] A scalper / secondary-market tic...   \n",
       "\n",
       "                                            question_text answer_text  \\\n",
       "66                        How many irritations are there?           2   \n",
       "67                        How many irritations are there?           2   \n",
       "68                        How many irritations are there?           2   \n",
       "151               what is the thread count on the sheets?        1500   \n",
       "152               what is the thread count on the sheets?        1500   \n",
       "...                                                   ...         ...   \n",
       "106622  How many people does the company that I work f...           8   \n",
       "106688                           How many did he play in?           4   \n",
       "106689                           How many did he play in?           4   \n",
       "106760  What's the total number of questions the autho...           5   \n",
       "106761  What's the total number of questions the autho...           5   \n",
       "\n",
       "       answer_start  is_numeric  \n",
       "66              169        True  \n",
       "67              169        True  \n",
       "68              169        True  \n",
       "151              91        True  \n",
       "152             126        True  \n",
       "...             ...         ...  \n",
       "106622          302        True  \n",
       "106688           99        True  \n",
       "106689           99        True  \n",
       "106760           60        True  \n",
       "106761           60        True  \n",
       "\n",
       "[4795 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers['is_numeric'] = df_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)\n",
    "df_answers[df_answers['is_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\justin.stanley\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors='tokenize', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stanford_metrics(txt):\n",
    "    subtree_value = ''\n",
    "    ner = '_NO_NER'\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        with CoreNLPClient(endpoint='http://localhost:{}'.format(NLP_PORT), start_server=False, timeout=30000) as client:\n",
    "\n",
    "            ann = client.annotate(txt)\n",
    "            \n",
    "            sentence_count = len(ann.sentence)\n",
    "            words = [x.word for s in ann.sentence for x in s.token if x.word not in string.punctuation]\n",
    "            word_count = len(words)\n",
    "            character_count = sum([len(x) for x in words])\n",
    "            \n",
    "            sentence = ann.sentence[0]\n",
    "            if sentence.mentions:\n",
    "                ner = sentence.mentions[0].entityType\n",
    "            \n",
    "            constituency_parse = sentence.parseTree\n",
    "            subtree_value = constituency_parse.child[0].value\n",
    "        \n",
    "        return subtree_value, ner, sentence_count, word_count, character_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0],e.args[0], e.args[0], e.args[0],e.args[0]\n",
    "    \n",
    "def get_stanford_counts(txt):\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "        sentence_count = len(doc.sentences)\n",
    "        words = [w.text for s in doc.sentences for w in s.words if w.text not in string.punctuation]\n",
    "        word_count = len(words)\n",
    "        character_count = sum([len(x) for x in words])\n",
    "        \n",
    "        return sentence_count, word_count, character_count, words\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0], e.args[0], e.args[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_answers = pd.DataFrame({'answer_text': df_answers['answer_text'].unique()})\n",
    "df_distinct_answers[['first_parse', 'first_ner', 'sentence_count', 'word_count', 'word_character_count', ]] = df_distinct_answers.apply(lambda row: get_all_stanford_metrics(row['answer_text']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_text</th>\n",
       "      <th>first_parse</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_character_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_ner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CAUSE_OF_DEATH</th>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY</th>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTRY</th>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIMINAL_CHARGE</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoreNLP request timed out. Your document may be too long.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>1934</td>\n",
       "      <td>1934</td>\n",
       "      <td>1934</td>\n",
       "      <td>1934</td>\n",
       "      <td>1934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DURATION</th>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "      <td>1232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDLE</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDEOLOGY</th>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONEY</th>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATIONALITY</th>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMBER</th>\n",
       "      <td>4523</td>\n",
       "      <td>4523</td>\n",
       "      <td>4523</td>\n",
       "      <td>4523</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDINAL</th>\n",
       "      <td>505</td>\n",
       "      <td>505</td>\n",
       "      <td>505</td>\n",
       "      <td>505</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>1763</td>\n",
       "      <td>1763</td>\n",
       "      <td>1763</td>\n",
       "      <td>1763</td>\n",
       "      <td>1763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT</th>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>4895</td>\n",
       "      <td>4895</td>\n",
       "      <td>4895</td>\n",
       "      <td>4895</td>\n",
       "      <td>4895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELIGION</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SET</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE_OR_PROVINCE</th>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <td>1752</td>\n",
       "      <td>1752</td>\n",
       "      <td>1752</td>\n",
       "      <td>1752</td>\n",
       "      <td>1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_NO_NER</th>\n",
       "      <td>31038</td>\n",
       "      <td>31038</td>\n",
       "      <td>31038</td>\n",
       "      <td>31038</td>\n",
       "      <td>31038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list index out of range</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    answer_text  first_parse  \\\n",
       "first_ner                                                                      \n",
       "CAUSE_OF_DEATH                                              386          386   \n",
       "CITY                                                        581          581   \n",
       "COUNTRY                                                     604          604   \n",
       "CRIMINAL_CHARGE                                             166          166   \n",
       "CoreNLP request timed out. Your document may be...            1            1   \n",
       "DATE                                                       1934         1934   \n",
       "DURATION                                                   1232         1232   \n",
       "HANDLE                                                        6            6   \n",
       "IDEOLOGY                                                    264          264   \n",
       "LOCATION                                                    610          610   \n",
       "MISC                                                        635          635   \n",
       "MONEY                                                       640          640   \n",
       "NATIONALITY                                                 676          676   \n",
       "NUMBER                                                     4523         4523   \n",
       "ORDINAL                                                     505          505   \n",
       "ORGANIZATION                                               1763         1763   \n",
       "PERCENT                                                     369          369   \n",
       "PERSON                                                     4895         4895   \n",
       "RELIGION                                                    140          140   \n",
       "SET                                                         182          182   \n",
       "STATE_OR_PROVINCE                                           302          302   \n",
       "TIME                                                        214          214   \n",
       "TITLE                                                      1752         1752   \n",
       "URL                                                           3            3   \n",
       "_NO_NER                                                   31038        31038   \n",
       "list index out of range                                       1            1   \n",
       "\n",
       "                                                    sentence_count  \\\n",
       "first_ner                                                            \n",
       "CAUSE_OF_DEATH                                                 386   \n",
       "CITY                                                           581   \n",
       "COUNTRY                                                        604   \n",
       "CRIMINAL_CHARGE                                                166   \n",
       "CoreNLP request timed out. Your document may be...               1   \n",
       "DATE                                                          1934   \n",
       "DURATION                                                      1232   \n",
       "HANDLE                                                           6   \n",
       "IDEOLOGY                                                       264   \n",
       "LOCATION                                                       610   \n",
       "MISC                                                           635   \n",
       "MONEY                                                          640   \n",
       "NATIONALITY                                                    676   \n",
       "NUMBER                                                        4523   \n",
       "ORDINAL                                                        505   \n",
       "ORGANIZATION                                                  1763   \n",
       "PERCENT                                                        369   \n",
       "PERSON                                                        4895   \n",
       "RELIGION                                                       140   \n",
       "SET                                                            182   \n",
       "STATE_OR_PROVINCE                                              302   \n",
       "TIME                                                           214   \n",
       "TITLE                                                         1752   \n",
       "URL                                                              3   \n",
       "_NO_NER                                                      31038   \n",
       "list index out of range                                          1   \n",
       "\n",
       "                                                    word_count  \\\n",
       "first_ner                                                        \n",
       "CAUSE_OF_DEATH                                             386   \n",
       "CITY                                                       581   \n",
       "COUNTRY                                                    604   \n",
       "CRIMINAL_CHARGE                                            166   \n",
       "CoreNLP request timed out. Your document may be...           1   \n",
       "DATE                                                      1934   \n",
       "DURATION                                                  1232   \n",
       "HANDLE                                                       6   \n",
       "IDEOLOGY                                                   264   \n",
       "LOCATION                                                   610   \n",
       "MISC                                                       635   \n",
       "MONEY                                                      640   \n",
       "NATIONALITY                                                676   \n",
       "NUMBER                                                    4523   \n",
       "ORDINAL                                                    505   \n",
       "ORGANIZATION                                              1763   \n",
       "PERCENT                                                    369   \n",
       "PERSON                                                    4895   \n",
       "RELIGION                                                   140   \n",
       "SET                                                        182   \n",
       "STATE_OR_PROVINCE                                          302   \n",
       "TIME                                                       214   \n",
       "TITLE                                                     1752   \n",
       "URL                                                          3   \n",
       "_NO_NER                                                  31038   \n",
       "list index out of range                                      1   \n",
       "\n",
       "                                                    word_character_count  \n",
       "first_ner                                                                 \n",
       "CAUSE_OF_DEATH                                                       386  \n",
       "CITY                                                                 581  \n",
       "COUNTRY                                                              604  \n",
       "CRIMINAL_CHARGE                                                      166  \n",
       "CoreNLP request timed out. Your document may be...                     1  \n",
       "DATE                                                                1934  \n",
       "DURATION                                                            1232  \n",
       "HANDLE                                                                 6  \n",
       "IDEOLOGY                                                             264  \n",
       "LOCATION                                                             610  \n",
       "MISC                                                                 635  \n",
       "MONEY                                                                640  \n",
       "NATIONALITY                                                          676  \n",
       "NUMBER                                                              4523  \n",
       "ORDINAL                                                              505  \n",
       "ORGANIZATION                                                        1763  \n",
       "PERCENT                                                              369  \n",
       "PERSON                                                              4895  \n",
       "RELIGION                                                             140  \n",
       "SET                                                                  182  \n",
       "STATE_OR_PROVINCE                                                    302  \n",
       "TIME                                                                 214  \n",
       "TITLE                                                               1752  \n",
       "URL                                                                    3  \n",
       "_NO_NER                                                            31038  \n",
       "list index out of range                                                1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct_answers.fillna(value = {'first_ner':'_NO_NER'}).groupby(['first_ner']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context = df_answers[['test_set','context']].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context[['sentence_count', 'word_count', 'word_character_count', 'words']] = df_distinct_context.apply(lambda row: get_stanford_counts(row['context']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_set</th>\n",
       "      <th>context</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_character_count</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>175</td>\n",
       "      <td>[It, 's, a, very, nice, holder, not, too, big,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>First of all, this thing is freakin' awesome. ...</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>630</td>\n",
       "      <td>[First, of, all, this, thing, is, freakin', aw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>The presto my pod makes, to me, a very weak an...</td>\n",
       "      <td>8</td>\n",
       "      <td>117</td>\n",
       "      <td>431</td>\n",
       "      <td>[The, presto, my, pod, makes, to, me, a, very,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>This product takes 10 minutes to setup 4. It i...</td>\n",
       "      <td>14</td>\n",
       "      <td>123</td>\n",
       "      <td>458</td>\n",
       "      <td>[This, product, takes, 10, minutes, to, setup,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>I have always kept a dustbuster in my kitchen ...</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>749</td>\n",
       "      <td>[I, have, always, kept, a, dustbuster, in, my,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index           test_set  \\\n",
       "0      0  amazon_reviews_v1   \n",
       "1     15  amazon_reviews_v1   \n",
       "2     27  amazon_reviews_v1   \n",
       "3     39  amazon_reviews_v1   \n",
       "4     51  amazon_reviews_v1   \n",
       "\n",
       "                                             context  sentence_count  \\\n",
       "0  It's a very nice holder - not too big and not ...               4   \n",
       "1  First of all, this thing is freakin' awesome. ...               9   \n",
       "2  The presto my pod makes, to me, a very weak an...               8   \n",
       "3  This product takes 10 minutes to setup 4. It i...              14   \n",
       "4  I have always kept a dustbuster in my kitchen ...              15   \n",
       "\n",
       "   word_count  word_character_count  \\\n",
       "0          47                   175   \n",
       "1         159                   630   \n",
       "2         117                   431   \n",
       "3         123                   458   \n",
       "4         182                   749   \n",
       "\n",
       "                                               words  \n",
       "0  [It, 's, a, very, nice, holder, not, too, big,...  \n",
       "1  [First, of, all, this, thing, is, freakin', aw...  \n",
       "2  [The, presto, my, pod, makes, to, me, a, very,...  \n",
       "3  [This, product, takes, 10, minutes, to, setup,...  \n",
       "4  [I, have, always, kept, a, dustbuster, in, my,...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct_context[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_df = dd.from_pandas(df_distinct_context, npartitions = 2*multiprocessing.cpu_count()) \\\n",
    "            .map_partitions(lambda df: df.apply(lambda row: [max(1, len(Hyphenator('en_US').syllables(x))) if len(str(x)) < 100 else -1 for x in row['words'] ], axis = 1)) \\\n",
    "            .compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['syllables_per_word'] = syll_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['polysyllable_count'] = df_distinct_context.apply(lambda row: len([x for x in row['syllables_per_word'] if x > 1]), axis = 1)\n",
    "df_distinct_context['avg_word_length'] = df_distinct_context.apply(lambda row: sum([len(x) for x in row['words']])/row['word_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['avg_sentence_length_in_words'] = df_distinct_context['word_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['context_character_count'] = df_distinct_context.apply(lambda row: len(row['context']), axis=1)\n",
    "df_distinct_context['avg_sentence_length_in_characters'] = df_distinct_context['context_character_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['syllables_per_word'] = df_distinct_context.apply(lambda row: sum([x for x in row['syllables_per_word'] if x > 0])/ len([x for x in row['syllables_per_word'] if x > 0]) , axis=1)\n",
    "df_distinct_context['flesch-kincaid_grade_level'] = df_distinct_context.apply(lambda row: (0.39 * row['avg_sentence_length_in_words']) + (11.8 * row['syllables_per_word']) - 15.59, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['coleman-liau'] = df_distinct_context.apply(lambda row: (0.0588 * (row['avg_word_length']) * 100) - (0.296 * (100/row['avg_sentence_length_in_words'])) - 15.8, axis=1)\n",
    "df_distinct_context['gunning-fog'] = df_distinct_context.apply(lambda row: 0.4 * ((row['word_count'] / row['sentence_count']) + ((row['polysyllable_count'] / row['word_count']) * 100)), axis=1)\n",
    "df_distinct_context['automated-readability'] = df_distinct_context.apply(lambda row: 4.71 * (row['context_character_count'] / row['word_count']) + 0.5 * (row['word_count'] / row['sentence_count']) - 21.43, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers = df_answers.merge(df_distinct_answers, on=['answer_text'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers['is_numeric'] = df_merged_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers_and_context = df_merged_answers.merge(df_distinct_context, on=['context'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context = df_pred.merge(df_merged_answers_and_context, left_on=['qid'], right_on=['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context['exact_match'] = df_pred_answers_context['exact_match'].map({'True':True, 'False':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers.to_csv('answers.csv', index=False)\n",
    "df_distinct_answers.to_csv('distinct_answers.csv', index=False)\n",
    "df_distinct_context.to_csv('distinct_context.csv', index=False)\n",
    "df_merged_answers.to_csv('merged_answers.csv', index=False)\n",
    "df_merged_answers_and_context.to_csv('merged_answers_and_context.csv', index = False)\n",
    "df_pred_answers_context.to_csv('pred_answers_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>display_name</th>\n",
       "      <th>model_name</th>\n",
       "      <th>description</th>\n",
       "      <th>uuid</th>\n",
       "      <th>testbed</th>\n",
       "      <th>test_set</th>\n",
       "      <th>qid</th>\n",
       "      <th>predicted_answer</th>\n",
       "      <th>exact_match</th>\n",
       "      <th>f1</th>\n",
       "      <th>...</th>\n",
       "      <th>syllables_per_word</th>\n",
       "      <th>polysyllable_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length_in_words</th>\n",
       "      <th>context_character_count</th>\n",
       "      <th>avg_sentence_length_in_characters</th>\n",
       "      <th>flesch-kincaid_grade_level</th>\n",
       "      <th>coleman-liau</th>\n",
       "      <th>gunning-fog</th>\n",
       "      <th>automated-readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>xlnet-123(singlemodel)</td>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>0x8d330a</td>\n",
       "      <td>John</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.75</td>\n",
       "      <td>228</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>xlnet-123(singlemodel)</td>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>0x8d330a</td>\n",
       "      <td>John</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.75</td>\n",
       "      <td>228</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>xlnet-123(singlemodel)</td>\n",
       "      <td>XLNET-123 (single model)</td>\n",
       "      <td>0x8d330a</td>\n",
       "      <td>John</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.75</td>\n",
       "      <td>228</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tuned BERT-1seq Large Cased (single model)</td>\n",
       "      <td>tunedbert-1seqlargecased(singlemodel)</td>\n",
       "      <td>Tuned BERT-1seq Large Cased (single model)</td>\n",
       "      <td>0xf776d7</td>\n",
       "      <td>John</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.75</td>\n",
       "      <td>228</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tuned BERT-1seq Large Cased (single model)</td>\n",
       "      <td>tunedbert-1seqlargecased(singlemodel)</td>\n",
       "      <td>Tuned BERT-1seq Large Cased (single model)</td>\n",
       "      <td>0xf776d7</td>\n",
       "      <td>John</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.75</td>\n",
       "      <td>228</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 display_name  \\\n",
       "0                    XLNET-123 (single model)   \n",
       "1                    XLNET-123 (single model)   \n",
       "2                    XLNET-123 (single model)   \n",
       "3  Tuned BERT-1seq Large Cased (single model)   \n",
       "4  Tuned BERT-1seq Large Cased (single model)   \n",
       "\n",
       "                              model_name  \\\n",
       "0                 xlnet-123(singlemodel)   \n",
       "1                 xlnet-123(singlemodel)   \n",
       "2                 xlnet-123(singlemodel)   \n",
       "3  tunedbert-1seqlargecased(singlemodel)   \n",
       "4  tunedbert-1seqlargecased(singlemodel)   \n",
       "\n",
       "                                  description      uuid testbed test_set  \\\n",
       "0                    XLNET-123 (single model)  0x8d330a    John   Amazon   \n",
       "1                    XLNET-123 (single model)  0x8d330a    John   Amazon   \n",
       "2                    XLNET-123 (single model)  0x8d330a    John   Amazon   \n",
       "3  Tuned BERT-1seq Large Cased (single model)  0xf776d7    John   Amazon   \n",
       "4  Tuned BERT-1seq Large Cased (single model)  0xf776d7    John   Amazon   \n",
       "\n",
       "                        qid               predicted_answer  exact_match   f1  \\\n",
       "0  5dd465dacc027a086d65bc6c  not too big and not too small         True  1.0   \n",
       "1  5dd465dacc027a086d65bc6c  not too big and not too small         True  1.0   \n",
       "2  5dd465dacc027a086d65bc6c  not too big and not too small         True  1.0   \n",
       "3  5dd465dacc027a086d65bc6c  not too big and not too small         True  1.0   \n",
       "4  5dd465dacc027a086d65bc6c  not too big and not too small         True  1.0   \n",
       "\n",
       "   ... syllables_per_word polysyllable_count avg_word_length  \\\n",
       "0  ...           1.106383                  4        3.723404   \n",
       "1  ...           1.106383                  4        3.723404   \n",
       "2  ...           1.106383                  4        3.723404   \n",
       "3  ...           1.106383                  4        3.723404   \n",
       "4  ...           1.106383                  4        3.723404   \n",
       "\n",
       "  avg_sentence_length_in_words context_character_count  \\\n",
       "0                        11.75                     228   \n",
       "1                        11.75                     228   \n",
       "2                        11.75                     228   \n",
       "3                        11.75                     228   \n",
       "4                        11.75                     228   \n",
       "\n",
       "  avg_sentence_length_in_characters flesch-kincaid_grade_level  coleman-liau  \\\n",
       "0                              57.0                   2.047819      3.574468   \n",
       "1                              57.0                   2.047819      3.574468   \n",
       "2                              57.0                   2.047819      3.574468   \n",
       "3                              57.0                   2.047819      3.574468   \n",
       "4                              57.0                   2.047819      3.574468   \n",
       "\n",
       "  gunning-fog automated-readability  \n",
       "0    8.104255              7.293511  \n",
       "1    8.104255              7.293511  \n",
       "2    8.104255              7.293511  \n",
       "3    8.104255              7.293511  \n",
       "4    8.104255              7.293511  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred_answers_context[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\Anaconda3\\envs\\w210_capstone\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (20,21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_answers = pd.read_csv('answers.csv')\n",
    "df_distinct_answers = pd.read_csv('distinct_answers.csv')\n",
    "df_distinct_context = pd.read_csv('distinct_context.csv')\n",
    "df_merged_answers = pd.read_csv('merged_answers.csv')\n",
    "df_merged_answers_and_context = pd.read_csv('merged_answers_and_context.csv')\n",
    "df_pred_answers_context = pd.read_csv('pred_answers_context.csv')\n",
    "df_pred = pd.DataFrame(load_data(PREDICTION_PATH + '/all_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_set</th>\n",
       "      <th>context</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_character_count</th>\n",
       "      <th>words</th>\n",
       "      <th>syllables_per_word</th>\n",
       "      <th>polysyllable_count</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>avg_sentence_length_in_words</th>\n",
       "      <th>context_character_count</th>\n",
       "      <th>avg_sentence_length_in_characters</th>\n",
       "      <th>flesch-kincaid_grade_level</th>\n",
       "      <th>coleman-liau</th>\n",
       "      <th>gunning-fog</th>\n",
       "      <th>automated-readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>175</td>\n",
       "      <td>['It', \"'s\", 'a', 'very', 'nice', 'holder', 'n...</td>\n",
       "      <td>1.106383</td>\n",
       "      <td>4</td>\n",
       "      <td>3.723404</td>\n",
       "      <td>11.750000</td>\n",
       "      <td>228</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>2.047819</td>\n",
       "      <td>3.574468</td>\n",
       "      <td>8.104255</td>\n",
       "      <td>7.293511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>First of all, this thing is freakin' awesome. ...</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>630</td>\n",
       "      <td>['First', 'of', 'all', 'this', 'thing', 'is', ...</td>\n",
       "      <td>1.207547</td>\n",
       "      <td>25</td>\n",
       "      <td>3.962264</td>\n",
       "      <td>17.666667</td>\n",
       "      <td>803</td>\n",
       "      <td>89.222222</td>\n",
       "      <td>5.549057</td>\n",
       "      <td>5.822642</td>\n",
       "      <td>13.355975</td>\n",
       "      <td>11.190314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>The presto my pod makes, to me, a very weak an...</td>\n",
       "      <td>8</td>\n",
       "      <td>117</td>\n",
       "      <td>431</td>\n",
       "      <td>['The', 'presto', 'my', 'pod', 'makes', 'to', ...</td>\n",
       "      <td>1.170940</td>\n",
       "      <td>18</td>\n",
       "      <td>3.683761</td>\n",
       "      <td>14.625000</td>\n",
       "      <td>551</td>\n",
       "      <td>68.875000</td>\n",
       "      <td>3.930844</td>\n",
       "      <td>3.836581</td>\n",
       "      <td>12.003846</td>\n",
       "      <td>8.063782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>This product takes 10 minutes to setup 4. It i...</td>\n",
       "      <td>14</td>\n",
       "      <td>123</td>\n",
       "      <td>458</td>\n",
       "      <td>['This', 'product', 'takes', '10', 'minutes', ...</td>\n",
       "      <td>1.146341</td>\n",
       "      <td>17</td>\n",
       "      <td>3.723577</td>\n",
       "      <td>8.785714</td>\n",
       "      <td>591</td>\n",
       "      <td>42.214286</td>\n",
       "      <td>1.363258</td>\n",
       "      <td>2.725528</td>\n",
       "      <td>9.042741</td>\n",
       "      <td>5.593833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51</td>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>I have always kept a dustbuster in my kitchen ...</td>\n",
       "      <td>15</td>\n",
       "      <td>182</td>\n",
       "      <td>749</td>\n",
       "      <td>['I', 'have', 'always', 'kept', 'a', 'dustbust...</td>\n",
       "      <td>1.214286</td>\n",
       "      <td>31</td>\n",
       "      <td>4.115385</td>\n",
       "      <td>12.133333</td>\n",
       "      <td>946</td>\n",
       "      <td>63.066667</td>\n",
       "      <td>3.470571</td>\n",
       "      <td>5.958901</td>\n",
       "      <td>11.666520</td>\n",
       "      <td>9.118315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index           test_set  \\\n",
       "0      0  amazon_reviews_v1   \n",
       "1     15  amazon_reviews_v1   \n",
       "2     27  amazon_reviews_v1   \n",
       "3     39  amazon_reviews_v1   \n",
       "4     51  amazon_reviews_v1   \n",
       "\n",
       "                                             context  sentence_count  \\\n",
       "0  It's a very nice holder - not too big and not ...               4   \n",
       "1  First of all, this thing is freakin' awesome. ...               9   \n",
       "2  The presto my pod makes, to me, a very weak an...               8   \n",
       "3  This product takes 10 minutes to setup 4. It i...              14   \n",
       "4  I have always kept a dustbuster in my kitchen ...              15   \n",
       "\n",
       "   word_count  word_character_count  \\\n",
       "0          47                   175   \n",
       "1         159                   630   \n",
       "2         117                   431   \n",
       "3         123                   458   \n",
       "4         182                   749   \n",
       "\n",
       "                                               words  syllables_per_word  \\\n",
       "0  ['It', \"'s\", 'a', 'very', 'nice', 'holder', 'n...            1.106383   \n",
       "1  ['First', 'of', 'all', 'this', 'thing', 'is', ...            1.207547   \n",
       "2  ['The', 'presto', 'my', 'pod', 'makes', 'to', ...            1.170940   \n",
       "3  ['This', 'product', 'takes', '10', 'minutes', ...            1.146341   \n",
       "4  ['I', 'have', 'always', 'kept', 'a', 'dustbust...            1.214286   \n",
       "\n",
       "   polysyllable_count  avg_word_length  avg_sentence_length_in_words  \\\n",
       "0                   4         3.723404                     11.750000   \n",
       "1                  25         3.962264                     17.666667   \n",
       "2                  18         3.683761                     14.625000   \n",
       "3                  17         3.723577                      8.785714   \n",
       "4                  31         4.115385                     12.133333   \n",
       "\n",
       "   context_character_count  avg_sentence_length_in_characters  \\\n",
       "0                      228                          57.000000   \n",
       "1                      803                          89.222222   \n",
       "2                      551                          68.875000   \n",
       "3                      591                          42.214286   \n",
       "4                      946                          63.066667   \n",
       "\n",
       "   flesch-kincaid_grade_level  coleman-liau  gunning-fog  \\\n",
       "0                    2.047819      3.574468     8.104255   \n",
       "1                    5.549057      5.822642    13.355975   \n",
       "2                    3.930844      3.836581    12.003846   \n",
       "3                    1.363258      2.725528     9.042741   \n",
       "4                    3.470571      5.958901    11.666520   \n",
       "\n",
       "   automated-readability  \n",
       "0               7.293511  \n",
       "1              11.190314  \n",
       "2               8.063782  \n",
       "3               5.593833  \n",
       "4               9.118315  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct_context[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets = df_distinct_context.groupby(['test_set'], as_index=False).agg({'context': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['freqdist'] = df_test_sets.apply(lambda row: nltk.FreqDist(nltk.tokenize.word_tokenize(row['context'])), axis = 1)\n",
    "df_test_sets['freqdist_lower'] = df_test_sets.apply(lambda row: nltk.FreqDist(nltk.tokenize.word_tokenize(row['context'].lower())), axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['lexical_diversity'] = df_test_sets.apply(lambda row: len(set(nltk.tokenize.word_tokenize(row['context'])))/len(nltk.tokenize.word_tokenize(row['context'])), axis = 1)\n",
    "df_test_sets['lexical_diversity_lower'] = df_test_sets.apply(lambda row: len(set(nltk.tokenize.word_tokenize(row['context'].lower())))/len(nltk.tokenize.word_tokenize(row['context'].lower())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['nltk_sentence_count'] = df_distinct_context.apply(lambda row: len(nltk.FreqDist(nltk.tokenize.sent_tokenize(row['context'].lower()))), axis=1)\n",
    "\n",
    "df_distinct_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TEST_SETS_PATH + '/train-v2.0.json') as f:\n",
    "    s_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_squad_text = ' '.join([x['context'] for y in s_json['data'] for x in y['paragraphs']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_freqdist = nltk.FreqDist(nltk.tokenize.word_tokenize(full_squad_text))\n",
    "squad_freqdist_lower = nltk.FreqDist(nltk.tokenize.word_tokenize(full_squad_text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD Training Lexical Diversity: 0.043178504959439146\n"
     ]
    }
   ],
   "source": [
    "# SQuAD lexical diversity\n",
    "print('SQuAD Training Lexical Diversity:', len(set(nltk.tokenize.word_tokenize(full_squad_text)))/len(nltk.tokenize.word_tokenize(full_squad_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD Training Lexical Diversity (lower case): 0.03877918893738047\n"
     ]
    }
   ],
   "source": [
    "print('SQuAD Training Lexical Diversity (lower case):', len(set(nltk.tokenize.word_tokenize(full_squad_text.lower())))/len(nltk.tokenize.word_tokenize(full_squad_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['training_overlap'] = df_test_sets.apply(lambda row: len(set.intersection( set(row['freqdist'].keys()), set(squad_freqdist.keys()) ))/len(set(row['freqdist'].keys())), axis = 1)\n",
    "df_test_sets['training_overlap_lower'] = df_test_sets.apply(lambda row: len(set.intersection( set(row['freqdist_lower'].keys()), set(squad_freqdist_lower.keys() )))/len(set(row['freqdist_lower'].keys())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['vocab_size'] = df_test_sets.apply(lambda row: len(row['freqdist']), axis = 1)\n",
    "df_test_sets['vocab_size_lower'] = df_test_sets.apply(lambda row: len(row['freqdist_lower']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['distinct_in_test'] = df_test_sets.apply(lambda row: set(row['freqdist'].keys()).difference(set(squad_freqdist.keys())), axis = 1)\n",
    "df_test_sets['distinct_in_test_lower'] = df_test_sets.apply(lambda row: set(row['freqdist_lower'].keys()).difference(set(squad_freqdist_lower.keys())), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['distinct_in_test_size'] = df_test_sets.apply(lambda row: len(row['distinct_in_test']), axis = 1)\n",
    "df_test_sets['distinct_in_test_size_lower'] = df_test_sets.apply(lambda row: len(row['distinct_in_test_lower']), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sets['pct_distinct'] = df_test_sets.apply(lambda row: row['distinct_in_test_size']/row['vocab_size'] , axis = 1)\n",
    "df_test_sets['pct_distinct_lower'] = df_test_sets.apply(lambda row: row['distinct_in_test_size_lower']/row['vocab_size_lower'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datafiles\\wordsapi_sample.json', encoding='utf8') as f:\n",
    "    wordsapi_dict = json.load(f)\n",
    "\n",
    "def getwords(word_set):\n",
    "    results = []\n",
    "    word_set = set(word_set).difference(set(wordsapi_dict.keys()))\n",
    "    for w in word_set:\n",
    "        try:\n",
    "            response = requests.request(\"GET\", url.format(w), headers=headers)\n",
    "            result_json = response.json()\n",
    "            result_json['word'] = w\n",
    "            results.append(result_json)\n",
    "        except Exception as e:\n",
    "            response = {'word': w, 'success':  False, 'message': str(e)}\n",
    "            print(w)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/10/13\n",
      "8/23/09\n",
      "on//off\n",
      "blankets/quilts/comforters\n",
      "5/10/2014\n",
      "1/21/10the\n",
      "12/27/12\n",
      "/an\n",
      "travel/back-up/whatever\n",
      "1/19/2011i\n",
      "restaurant/bar/commercial\n",
      "dishwasher/microwave/college\n",
      "11/16/11i\n",
      "roll/slide/move\n",
      "//www.amazon.com/oreck-professional-purifier-airpb-technology/dp/b002vsafp4/ref=sr_1_7\n",
      "traffic/petstains/hair/dirt\n",
      "money.08/22/13not\n",
      "website.09/10/13after\n",
      "5/31/13the\n",
      "12/17/12\n",
      "up/down/straight\n",
      "12/14/13\n",
      "on/off/tare\n",
      "9/4/13\n",
      "husbands/wives/significant\n",
      "sewing/crochet/art\n",
      "12/03/2011\n",
      "7/1/2014\n",
      "10/10/12\n",
      "side/back/stomach\n",
      "papers/purses/lunchboxes\n",
      "1/4/14\n",
      "07/22/13\n",
      "usage.11/25/13\n",
      "//www.amazon.com/bob-wardens-slow-food-fast/dp/0984188711/ref=sr_1_1\n",
      "//www.amazon.com/calphalon-unison-nonstick-10-piece-cookware/dp/b0028s7r58/ref=cm_cr_pr_product_topthese\n",
      "cracking/breaking/explosions\n",
      "1/10/2013i\n",
      "12/27/11\n",
      "//smile.amazon.com/gp/product/b000hm83x2/ref=oh_details_o02_s00_i00\n",
      "08/02/13i\n",
      "3/28/2014\n",
      "tang/shoulder/blade\n",
      "11/15/13\n",
      "//www.amazon.com/gp/product/b0091xnl0i/ref=oh_details_o01_s00_i00\n",
      "luck9/21/13\n",
      "//www.amazon.com/gp/product/b005wxo984/ref=oh_details_o00_s00_i00\n",
      "good/durable/functional\n",
      "top/floor/rug\n",
      "//www.amazon.com/knorr-homestyle-stock-vegetable-4-66ounce/dp/b008oukitg/ref=sr_1_1\n",
      "06/24/13\n",
      "lid/handle/drip\n",
      ".4/21/2014an\n",
      "bakes/broils/toasts\n",
      "4/19/2012\n",
      "next.11/17/13i\n",
      "6/27/14\n",
      "shakes/smoothies/frappachinos\n",
      "10/12/11\n",
      "11/29/13\n",
      "md/ms/mds\n",
      "bs/mbchb/mb\n",
      "/kəˈnɛt\n",
      "/ˌeɪtʃˌtiːˌɛmˈɛl/\n",
      "md/ms/dnb\n",
      "/j\n",
      "/ˌbɑːrɪˈliːf/\n",
      "/ˈrɒtərdæm/\n",
      "/ˌɛmɛsˈdɒs/\n",
      "/ˌiːstɑːnˈbuːl/\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/4/4c/wikisource-logo.svg/12px-wikisource-logo.svg.png\n",
      "/ˌɪstænˈbuːl/\n",
      "/ˈdʒeɪpɛɡ/\n",
      "stop/period/point\n",
      "29/08/2012\n",
      "/ˈaɪərə/\n",
      "/ref\n",
      "/ˌˈsiːˌɑːrˌtiː/\n",
      "achievement/aptitude/assessment\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/6/65/lock-green.svg/9px-lock-green.svg.png\n",
      "/ˌjuːˌɑːrˈɛl/\n",
      "aglp/agdlp/agudlp\n",
      "/ˌɛfˌbiːˈaɪ/\n",
      "/ˌaɪˌɑːrˈeɪ/\n",
      "/ˈneɪtoʊ/\n",
      "/50\n",
      "/ˈɜːrl/\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/d/d6/lock-gray-alt-2.svg/9px-lock-gray-alt-2.svg.png\n",
      "//upload.wikimedia.org/wikipedia/commons/thumb/a/aa/lock-red-alt-2.svg/9px-lock-red-alt-2.svg.png\n",
      "/ˈskuːbə/\n",
      "/ˌrɒtərˈdæm/\n",
      "/ˈɒksfərd/\n",
      "md/ms/fcps/dnb\n",
      "/ˈreɪdɑːr/\n",
      "cse.edu/about-cse/arts\n",
      "poet/rocker/visual\n",
      "s/a/c/\n",
      "site/blog/youtube\n",
      "12/31/15\n",
      "/r/anxiety\n",
      "/r/androidapps\n",
      "/u/3classy5me\n",
      "/u/catsnpokemon\n",
      "12/04/2015\n",
      "//rungame/730/76561202255233023/+csgodownloadmatch\n",
      "3/29/16\n",
      "23/m/6\n",
      "/u/livviet\n",
      "1/4x1/4\n",
      "01/01/16\n",
      "10/7/15\n",
      "/online\n",
      "/r/motorcycles\n",
      "/u/aravar27\n",
      "/r/asktechnology\n",
      "1/4/16\n",
      "/renew\n",
      "/u/lilusa\n",
      "designer/artist/director\n",
      "/u/errantrailer\n",
      "/u/akimbogogurts\n",
      "/r/leathercraft\n",
      "/r/backpacking\n",
      "//quitloginitem.\n",
      "9/25/16\n",
      "/home/csgowin/nodemodules/steam/lib/steamclient.js:192:10\n",
      "bot/tank/sup\n",
      "ceramic/tourmaline/ionic\n",
      "diamond/master/challenger\n",
      "/u/pickledseacat\n",
      "/u/unanimatedfelix\n",
      "/u/notnath14\n",
      "/r/custommagic\n",
      "/r/gamecube\n",
      "/r/kerbalacademy\n",
      "7/28/16\n",
      "/r/spikes\n",
      "10/100/1000\n",
      "10/07/2011\n",
      "/r/notredamesports|\n",
      "/u/bk1689\n",
      "abns/gst/invoices\n",
      "logo/web/illustrations/media/print\n",
      "jaw/neck/chin\n",
      "19/f/cst\n",
      "/u/tubbyturbo13\n",
      "mechanical/electrical/cosmetic\n",
      "/u/cobakid\n",
      "b/b/p/b\n",
      "wallet/venmo/bitcoin\n",
      "oes/rangefinder/perfect\n",
      "//connect/158.69.227.90:18616\n",
      "/u/6210classick\n",
      "/r/legaladvice\n",
      "/r/topmindsofreddit\n",
      "/home/ubuntu/workspace/copy.c\n",
      "/r/sondheim\n",
      "/u/atooz\n",
      "a/k/a\n",
      "/sfc\n",
      "/r/mylittlepony\n",
      "/u/summonerrin\n",
      "/r/pipetobacco\n",
      "/r/rocketleague\n",
      "/u/doldenberg\n",
      "/so\n",
      "/home/csgowin/nodemodules/steam/lib/steamclient.js:106:26\n",
      "/r/politics\n",
      "/r/indieheads\n",
      "/u/citiral\n",
      "/u/firebolt60\n",
      "js/app/widget/twitchbadgeicon.js\n",
      "/r/gamedev\n",
      "/u/bose-einstein\n",
      "finance/economics/business\n",
      "rebates/shipping/taxes\n",
      "/r/apple\n",
      "host/join/watch\n",
      "wheight/hp/speed/melee\n",
      "ham/sausage/bacon/whatever\n",
      "/r/hearthstone\n",
      "/r/paintball\n",
      "/r/hiphop101\n",
      "'/usr/local/portage/x11-terms/termite/termite-10.ebuild\n",
      "work/school/manic\n",
      "1/25/16\n",
      "27/m/gmt+1\n",
      "-/u/snapcaster-bolt\n",
      "ham/sausage/whatever\n",
      "/r/wheelanddeal\n",
      "/r/ironthronepowers\n",
      "/r/getoutofbed\n",
      "/u/doodlebugboodles\n",
      "/u/576875\n",
      "/r/learnprogramming\n",
      "/r/skyrim\n",
      "/dev/uinput\n",
      "/3\n",
      "/r/hutrep\n",
      "50k/115k/215k\n",
      "/u/snowchill\n",
      "/r/edmontonoilers\n",
      "/r/lifeofnorman\n",
      "/r/itdept\n",
      "/r/reddevils\n",
      "/r/fitness\n",
      "red/yellow/orange\n",
      "/u/cyung\n",
      "white/black/slight\n",
      "program/app/algorithm\n",
      "/r/swtor\n",
      "/u/lilmcc89\n",
      "/u/kaslaps\n",
      "1/2/16\n",
      "/scannow\n",
      "score/wins/kills\n",
      "/r/thebuttonminecraft\n",
      "/r/justgamestuff\n",
      "/r/bluejackets\n",
      "/r/keywest\n",
      "/r/fashionsouls\n",
      "potion/skill/item\n",
      "tower/os/monitor/keyboard/mouse/etc\n",
      "/u/cmpukahi\n",
      "driver/horn/10\n",
      "fire/smallbore/exhumed\n",
      "/u/eckido\n",
      "trading/selling/buying\n",
      "/u/radar_x\n",
      "/€\n",
      "7/17/2015\n",
      "/r/resissues\n",
      "itx/microatx/mid-tower/full-tower\n",
      "/u/vgspanky\n",
      "/u/kwamzilla\n",
      "/10\n",
      "50k/75k/175k\n",
      "/r/werealive\n",
      "block/pump/res\n",
      "/r/porto\n",
      "/r/fallout\n",
      "str/lvl/flat\n",
      "league/dota/starcraft\n",
      "/r/cars\n",
      "skeletons/clanrats/zombies\n",
      "/u/night28\n",
      "19/01/2016\n",
      "3.3/5/12\n",
      "/r/computers\n",
      "/r/hockey\n",
      "/u/barrett777\n",
      "/u/saucedpanda\n",
      "10/18/16\n",
      "house/trees/etc\n",
      "xp/tame/gather\n",
      "/r/healthyfood\n",
      "/r/gamedevscreens\n",
      "/u/anklestraps\n",
      "windows/mac/linux\n",
      "/week\n",
      "1/4/2016\n",
      "/u/tacoslim\n",
      "10/1/2016\n",
      "walking/hiking/working\n",
      "/lib/udev/rules.d/99-steam-controller-perms.rules\n",
      "/r/indiegames\n",
      "/r/heroesofthestorm\n",
      "1000/1000/1000\n",
      "clans/friends/friendlyfire/live\n",
      "/home/ubuntu/workspace/copy\n",
      "listener/initiator/supporter\n",
      "/keyboard/mouse/etc\n",
      "/u/alltattedupjay\n",
      "base/default/contactsformcaptcha/form.phtml\n",
      "5/23/16\n",
      "/u/bcalmkid\n",
      "/u/goran_dragic\n",
      "/r/twitch\n",
      "/shamelessplug\n",
      "pause/end/resume\n",
      "/release\n",
      "1/1/16\n",
      "pure/play/style\n",
      "/pedometer\n",
      "/u/twotecs\n",
      "/r/stunfisk\n",
      "/r/coffee\n",
      "/dev/sdb\n",
      "/r/seahawks\n",
      "/u/sreyz\n",
      "21/f/fr\n",
      "/u/spawnofyanni\n",
      "//the\n",
      "/r/darksouls2/wiki\n",
      "/r/lgg4\n",
      "/u/tennisace0227\n",
      "/r/loseit\n",
      "//\n",
      "/cleanup-image\n",
      "1/2/2016\n",
      "/r/mariomaker\n",
      "/u/turnshroud\n",
      "/r/teraonline\n",
      "performers/musicians/related\n",
      "teebird/thunderbird/westside\n",
      "/r/nhlhut\n",
      "/u/ibswimmin\n",
      "/r/uva\n",
      "/u/bobbymercer1730\n",
      "/5\n",
      "/spell\n",
      "/r/hardwareswap\n",
      "/r/javascript\n",
      "windows/linux/sql\n",
      "fha/va/fannie/freddie\n",
      "17/m/us-looking\n",
      "1/3/16\n",
      "/r/mini\n",
      "clean/whole30/paleo\n",
      "/u/dudewynaut\n",
      "/u/eaglebloo\n",
      "/r/summonerschool\n",
      "12/31/2015\n",
      "/u/pdcolemanjr\n",
      "skype/ts/discord\n",
      "/r/irishtourism\n",
      "/r/giftcardexchange\n",
      "/r/nottheonion\n",
      "3/4/14\n",
      "/home/csgowin/nodemodules/steam/lib/steam_client.js:106:26\n",
      "/r/software\n",
      "/r/noveltranslations\n",
      "/r/darksouls2\n",
      "/restorehealth\n",
      "6/28/16\n",
      "/u/r3xjm\n",
      "/r/nba2k\n",
      "/r/smashbros\n",
      "/u/strangepineapple\n",
      "/r/anime\n",
      "90/90/90\n",
      "/r/sandersforpresident\n",
      "/r/patientgamers\n",
      "jackrabbit/mario/duke\n",
      "13/9/6mm\n",
      "/u/hubife13\n",
      "framboos/hommage/cantillon\n",
      "/m\n",
      "/usr/local/portage/x11-terms/termite\n",
      "/u/brokestupidlonely\n",
      "food/architecture/museums\n",
      "hester/rice/smith\n",
      "/r/minecraft\n",
      "belgium/denmark/germany/netherlands\n",
      "/r/bookclub\n",
      "/r/cassetteculture\n",
      "/r/mechmarket\n",
      "/u/jastone96\n",
      "/home/csgowin/nodemodules/steam/lib/handlers/user.js:178:11\n",
      "/r/instagramshots\n",
      "/u/randomnerdgeek\n",
      "/u/jaclokickflip\n",
      "/r/europe\n",
      "2015/2014/2013/etc\n",
      "time/lap/hr\n",
      "2015/cologne/dreamhack14\n",
      "/r/india\n",
      "/tell\n",
      "b/b/bb/b\n",
      "16/m/canada\n",
      "dc/nova/md\n",
      "12/28-12/30\n",
      "~/downloads/crouton\n",
      "interesting/fun/unknown/welknown/moderately-enjoyable\n",
      "free/white/green\n",
      "/r/totalwar\n",
      "producer/singer/actor/dj/graphic\n",
      "802.11b/g/n\n",
      "2/22/16\n",
      "keyboard/touchpad/audio\n",
      "light/spv/hosted\n",
      "site/blog/video/works\n",
      "360/ps3/vita\n",
      "/r/gradadmissions\n",
      "/pset4/bmp\n"
     ]
    }
   ],
   "source": [
    "for test_set in df_test_sets['test_set'].unique():\n",
    "    word_list = list(df_test_sets[df_test_sets['test_set'] == test_set]['distinct_in_test_lower'].values[0])\n",
    "    results = getwords(word_list)\n",
    "    with open('datafiles/wordsapi_{}.json'.format(test_set), 'w', encoding='utf8') as f_out:\n",
    "        json.dump(results, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two-bath',\n",
       " 'outspoken.',\n",
       " 'c.i.a.-sponsored',\n",
       " 'mozaffor',\n",
       " 'ethan',\n",
       " 'chipper',\n",
       " 'non-american',\n",
       " '88-page',\n",
       " 'fingerpicked',\n",
       " 'mean.',\n",
       " 'guessing',\n",
       " 'limped',\n",
       " 'retires',\n",
       " 'florida.',\n",
       " 'pulpy',\n",
       " 'h.d.p',\n",
       " 'galeotti',\n",
       " 'mutates',\n",
       " 'fair-market',\n",
       " 'acar',\n",
       " 'gilbertville',\n",
       " 'mcconnell',\n",
       " 'happy-slappy',\n",
       " 'xiaochuan',\n",
       " 'marktwainhouse.org',\n",
       " 'most-searched',\n",
       " 'humanize',\n",
       " 'malty',\n",
       " 'telegraphing',\n",
       " 'meaty',\n",
       " 'well-trodden',\n",
       " 'holtby',\n",
       " 'deavere',\n",
       " 'nonwhite',\n",
       " 'freda',\n",
       " 'mermelstein',\n",
       " 'flagstaff',\n",
       " 'djemel',\n",
       " 'pre-recession',\n",
       " 'balducci',\n",
       " 'crazy.',\n",
       " 'flirty',\n",
       " 'andino',\n",
       " 'bilkent',\n",
       " 'birthrates',\n",
       " 'coy',\n",
       " 'recker',\n",
       " 'hothouse',\n",
       " 'worst-hit',\n",
       " 'trivially',\n",
       " 'sugary',\n",
       " 'ibargüen',\n",
       " 'message.',\n",
       " 'chernow',\n",
       " 'pask',\n",
       " 'expressionless',\n",
       " 'g.m.',\n",
       " 'aksyonov',\n",
       " 'serena',\n",
       " 'disdainful',\n",
       " 'layup',\n",
       " 'inception.',\n",
       " 'desmondfishlibrary.org',\n",
       " 'open-water',\n",
       " 'greater.',\n",
       " 'carmona',\n",
       " 'bread-and-butter',\n",
       " 'goan',\n",
       " 'snows',\n",
       " 'daata',\n",
       " 'bretas',\n",
       " 'nicolai',\n",
       " 'mondella',\n",
       " '€100',\n",
       " 'catsoulis',\n",
       " 'mind-share',\n",
       " '1893-1983',\n",
       " '34,540',\n",
       " 'low-key',\n",
       " 'souchon',\n",
       " 'abiquiu',\n",
       " 'expecting.',\n",
       " '498,000',\n",
       " 'christines-store.com',\n",
       " 'quaint',\n",
       " 'cockadoodleq',\n",
       " 'folkified',\n",
       " 'spinster',\n",
       " '203-222-7070',\n",
       " '211,000',\n",
       " 'self-portrait',\n",
       " '212-505-5181',\n",
       " '3,380',\n",
       " 'soy-lacquered',\n",
       " 'sustainability.',\n",
       " 'gadsby',\n",
       " 'leftovers',\n",
       " 'disheartening',\n",
       " 'made.',\n",
       " 'sopacnow.org',\n",
       " 'last-2',\n",
       " 'primped',\n",
       " 'haven.',\n",
       " 'vidinli',\n",
       " 'asians.',\n",
       " 'bad-mouth',\n",
       " 'lenson.',\n",
       " 'charli',\n",
       " 'much-promoted',\n",
       " 'wisteria',\n",
       " '1-2',\n",
       " 'chesters',\n",
       " 'high.',\n",
       " 'self-revealing',\n",
       " 'song-and-dance',\n",
       " 'montpelier',\n",
       " 'amaury',\n",
       " 'cornejo',\n",
       " 'banged-up',\n",
       " 'sneer',\n",
       " 'fishkill',\n",
       " 'areola',\n",
       " 'sanctions.',\n",
       " 'sternheimer',\n",
       " 'bare-knuckle',\n",
       " 'authenticate',\n",
       " 'mermaid',\n",
       " 'chromecast',\n",
       " 'double-double',\n",
       " 'realistic.',\n",
       " 'fact-obsessed',\n",
       " 'melgen',\n",
       " 'starts-win-place-show',\n",
       " 'treetop',\n",
       " '22-5',\n",
       " 'ellenville',\n",
       " 'system.',\n",
       " 'stagy',\n",
       " 'swindlers',\n",
       " 'catheter',\n",
       " 'slights',\n",
       " 'nuggets',\n",
       " 'musicmountain.org',\n",
       " 'resurrectionists',\n",
       " 'mumbled',\n",
       " 'hypochondriac',\n",
       " 'cobbling',\n",
       " 'estell',\n",
       " 'rafters',\n",
       " 'stanikzai',\n",
       " 'worshipers',\n",
       " 'markov',\n",
       " 'carma',\n",
       " 'shyly',\n",
       " 'chauffeur.',\n",
       " 'encore',\n",
       " 'engendering',\n",
       " 'burfoot',\n",
       " 'gizmodo',\n",
       " 'lamberton',\n",
       " 'tsipras',\n",
       " 'least.',\n",
       " 'beeping',\n",
       " 'heffner',\n",
       " 'matchup',\n",
       " 'tub',\n",
       " 'cinephile',\n",
       " 'lytham',\n",
       " 'anticorruption',\n",
       " 'expletive',\n",
       " 'ganek',\n",
       " 'badstuber',\n",
       " 'overregulation',\n",
       " 'omissions',\n",
       " 'shanleys',\n",
       " 'kathak',\n",
       " 'fleetingly',\n",
       " '240,091',\n",
       " 'deferments',\n",
       " 'esopus',\n",
       " 'banstetter',\n",
       " 'fizzy',\n",
       " 'mennonite',\n",
       " 'evgeni',\n",
       " 'street-prospect',\n",
       " 'cash-and-stock',\n",
       " 'chatting',\n",
       " 'namechecked',\n",
       " 'artists.',\n",
       " 'theaterworks',\n",
       " 'burkburnett',\n",
       " 'challenging.',\n",
       " 'corner.',\n",
       " 'draft-eligible',\n",
       " 'self.',\n",
       " 'carefree',\n",
       " 'thingamajigs',\n",
       " 'incomprehensible',\n",
       " 'lipton',\n",
       " 'gardened',\n",
       " 'cast-metal',\n",
       " 'gibneydance.org',\n",
       " 'woolliness',\n",
       " 'bookends',\n",
       " 'all-cause',\n",
       " 'company.',\n",
       " 'yeses',\n",
       " 'late-october',\n",
       " 'eun-me',\n",
       " 'stretcher',\n",
       " 'everdeen',\n",
       " 'chafee',\n",
       " 'whit',\n",
       " 'mcilroy',\n",
       " 'drags',\n",
       " '11-6-3',\n",
       " 'beef-filled',\n",
       " 'adriane',\n",
       " 'incontinent',\n",
       " 'pagliuca',\n",
       " 'plaats',\n",
       " 'affability',\n",
       " 'caballero',\n",
       " '★',\n",
       " 'giacometti',\n",
       " 'bathers',\n",
       " 'anti-israel',\n",
       " 'cregg',\n",
       " '877-840-0457',\n",
       " 'hand-built',\n",
       " 'mazzoleni',\n",
       " 'o.k.',\n",
       " 'pixar',\n",
       " 'visuals',\n",
       " 'treason.',\n",
       " '5,000-per-unit',\n",
       " 'codpieces',\n",
       " 'mahottari',\n",
       " 'ernesta',\n",
       " 'well-intentioned',\n",
       " 'rezoning',\n",
       " 'redemption.',\n",
       " 'daffy',\n",
       " 'mournfully',\n",
       " 'hrabal',\n",
       " 'motherless',\n",
       " 'flat-screen',\n",
       " 'hollander',\n",
       " 'stromboli',\n",
       " '2:28',\n",
       " 'high-sticking',\n",
       " 'repaved',\n",
       " 'tricky',\n",
       " 'draymond',\n",
       " 'new-media',\n",
       " 'aarp',\n",
       " 'rwby.',\n",
       " 'exhumes',\n",
       " '91-year-old',\n",
       " 'delgado',\n",
       " 'heights.',\n",
       " 'scrubby',\n",
       " 'terranova',\n",
       " 'patted',\n",
       " 'marea',\n",
       " 'andrzej',\n",
       " 'views.',\n",
       " 'trice',\n",
       " 'undercooked',\n",
       " 'moskowitz',\n",
       " 'shape-up',\n",
       " 'vote16usa',\n",
       " 'pappalardo',\n",
       " '914-271-6612.',\n",
       " 'fyi',\n",
       " 'b.y.u',\n",
       " 'memento',\n",
       " 'samour',\n",
       " '2:35',\n",
       " 'adiba',\n",
       " '226,000',\n",
       " 'ripostes',\n",
       " 'aundrea',\n",
       " '6:45.',\n",
       " 'unnaturally',\n",
       " 'spirit.',\n",
       " 'newhavenmuseum.org',\n",
       " 'gold-framed',\n",
       " 'beaumont-la-ronce.',\n",
       " 'water.',\n",
       " 'portia',\n",
       " 'folk-art',\n",
       " 'reeboks',\n",
       " 'shinehee',\n",
       " 'sentlij',\n",
       " 'legit',\n",
       " 'impressionable',\n",
       " 'vengeful',\n",
       " 'talladega',\n",
       " 'haberstock',\n",
       " 'angriness',\n",
       " 'peacekeeper',\n",
       " 're-sign',\n",
       " 'bankruptcies',\n",
       " 'macklin',\n",
       " 'lacava',\n",
       " 'vitello',\n",
       " 'judd',\n",
       " 'year.',\n",
       " 'ratmansky',\n",
       " 'grier',\n",
       " 'sauna',\n",
       " 'stringing',\n",
       " 'oncologist',\n",
       " 'sorrows.',\n",
       " 'tomei',\n",
       " 'macgyver',\n",
       " 'ghani',\n",
       " 'overcost',\n",
       " 'bluedoorartcenter.org',\n",
       " 'satchmo',\n",
       " 'jouret-epstein',\n",
       " 'dispiriting',\n",
       " 'different.',\n",
       " '44.0.',\n",
       " 'bellocq',\n",
       " 'ursula',\n",
       " 'artswestchester',\n",
       " 'accident-prediction',\n",
       " 'set.',\n",
       " 'routers',\n",
       " 'chukwu',\n",
       " '1,087.70',\n",
       " 'tiebreak',\n",
       " 'parlor-gallery.com',\n",
       " 's.u.v',\n",
       " 'one.',\n",
       " 'beast.',\n",
       " 'sidley',\n",
       " 'reassessing',\n",
       " 'moto',\n",
       " 'shabab',\n",
       " 'buzzfeed',\n",
       " 'bests',\n",
       " 'councilman',\n",
       " 'delves',\n",
       " 'biggs',\n",
       " 'lower-body',\n",
       " 'fourth-quarter',\n",
       " 'used-books',\n",
       " 'wander',\n",
       " 'playsam',\n",
       " 'soft-focus',\n",
       " 'life-enhancing',\n",
       " 'barthelme',\n",
       " '802.11n',\n",
       " 'johanna',\n",
       " 'andrii',\n",
       " 'puffy',\n",
       " 'blalock',\n",
       " 'rainey',\n",
       " 'cholesterol-lowering',\n",
       " 'strouse',\n",
       " 'showthe',\n",
       " 'airbag',\n",
       " 'républicain',\n",
       " 'rediscover',\n",
       " 'unmendable',\n",
       " 'bushel',\n",
       " 'naomi',\n",
       " 'disney-owned',\n",
       " '32-34',\n",
       " 'rubinmuseum.org',\n",
       " 'chiquita',\n",
       " 'angelika',\n",
       " 'closets',\n",
       " 'r.f.a.',\n",
       " 'constrict',\n",
       " 'showrunners',\n",
       " 'strode',\n",
       " 'front-desk',\n",
       " 'bharara',\n",
       " 'helpful.',\n",
       " 'operation.',\n",
       " 'for-profits',\n",
       " 'baby.',\n",
       " 'wonks',\n",
       " 'infiltrate',\n",
       " 'republic.',\n",
       " 'tradicionales',\n",
       " 'e.t.f.s',\n",
       " 'church.',\n",
       " '28-0',\n",
       " 'ticked',\n",
       " 'in-person',\n",
       " '201-227-1030',\n",
       " 'haddock',\n",
       " '8-year-old',\n",
       " 'icing',\n",
       " 'attired',\n",
       " 'glassed-in',\n",
       " 'man.',\n",
       " 'sofa',\n",
       " 'level.',\n",
       " 'concussion',\n",
       " 'parthé',\n",
       " 'keyhole',\n",
       " 'fdrlibrary.marist.edu',\n",
       " 'wags',\n",
       " 'yevgeny',\n",
       " '45-year-old',\n",
       " 'marshmallow',\n",
       " '212-924-7771',\n",
       " 'heinrichs',\n",
       " 'wither',\n",
       " 'high-testosterone',\n",
       " 'carbonell',\n",
       " 'openhanded',\n",
       " 'heupel',\n",
       " 'resident./now',\n",
       " 'defenseman',\n",
       " 'granderson',\n",
       " 'quarts',\n",
       " 'jacint',\n",
       " 'prowling',\n",
       " 'véra',\n",
       " 'on.',\n",
       " 'bicker.',\n",
       " 'mid-eastern',\n",
       " '69,260',\n",
       " 'punts',\n",
       " 'riotously',\n",
       " 'snarkiness',\n",
       " 'calif.',\n",
       " 'morgantown',\n",
       " 'interlocking',\n",
       " 'altogether.',\n",
       " 'pavéd',\n",
       " 'entwining',\n",
       " 'are.',\n",
       " 'finney',\n",
       " 'rubel',\n",
       " 'true-to-life',\n",
       " 'fieilo',\n",
       " 'sliminess',\n",
       " 'chettinadu',\n",
       " 'sieff',\n",
       " 'capito',\n",
       " 'disinfect',\n",
       " 'mouthwash',\n",
       " 'toes',\n",
       " 'nook',\n",
       " 'brookfieldplaceny.com/eventscalendar',\n",
       " 'gaines',\n",
       " 'clowney',\n",
       " 'sonatina',\n",
       " 'ketchup',\n",
       " 'neto',\n",
       " 'gesticulations',\n",
       " '914-375-5100',\n",
       " 'kelley',\n",
       " 'rap-star',\n",
       " 'podcaster',\n",
       " 'fix.',\n",
       " 'repeatedly.',\n",
       " 'landon',\n",
       " 'sondos',\n",
       " 'empire.',\n",
       " 'eyebrows',\n",
       " 'geli',\n",
       " 'wayfair.',\n",
       " 'wouter',\n",
       " 'bargained',\n",
       " '\\xadböttger',\n",
       " 'fierstein',\n",
       " 'dollop',\n",
       " 'drumheller',\n",
       " 'unkempt',\n",
       " 'straw-man',\n",
       " 'nazar',\n",
       " 'recasting',\n",
       " 're-emerging',\n",
       " 'bowl-eligible',\n",
       " 'md.',\n",
       " 'enchanting',\n",
       " '0.434',\n",
       " 'chiasson',\n",
       " 'springer',\n",
       " 'abdul-hamed',\n",
       " 'gravy',\n",
       " '616,000',\n",
       " 'responsibility.',\n",
       " 'cuatro',\n",
       " 'uncontrollable',\n",
       " 'catholic-jewish',\n",
       " 'often-derided',\n",
       " 'dollhouses',\n",
       " 'casserole',\n",
       " 'culpable',\n",
       " 'then-record',\n",
       " 'brainstormed',\n",
       " 'unfancy',\n",
       " 'ijmeer',\n",
       " 'résumés',\n",
       " 'dez',\n",
       " 'lasermania',\n",
       " 'batsman',\n",
       " 'satisfactorily',\n",
       " 'foursomes',\n",
       " 'demaryius',\n",
       " 'abby',\n",
       " 'overseas.',\n",
       " 'front-load',\n",
       " 'undress',\n",
       " 'pierre-laurent',\n",
       " 'soybean',\n",
       " '7,135',\n",
       " 'n.s.a.',\n",
       " 'oladipo',\n",
       " 'pamper',\n",
       " 'rejoining',\n",
       " 'ratpac',\n",
       " 'máirtín',\n",
       " 'tex',\n",
       " 'fllac.vassar.edu',\n",
       " 'hildrew',\n",
       " '5.35',\n",
       " '\\xadconfirm',\n",
       " 'asbury',\n",
       " 'towpath',\n",
       " 'hallowed',\n",
       " 'apatow',\n",
       " 'closed-door',\n",
       " 'harf',\n",
       " 'metzitzah',\n",
       " 'schenn',\n",
       " 'audacious',\n",
       " 'moby-dick',\n",
       " 'bitcoins',\n",
       " 'n.b.a',\n",
       " 'wirth',\n",
       " 'half-bird',\n",
       " 'carbapenem-resistant',\n",
       " 'in-room',\n",
       " 'missive',\n",
       " 'yekaterinburg',\n",
       " 'rosenthal',\n",
       " 'fla.',\n",
       " 'ilit',\n",
       " 'azoulay',\n",
       " 'hadera',\n",
       " 'blue-blooded',\n",
       " 'half-dollar-size',\n",
       " 'independent-minded',\n",
       " 'unintelligibly',\n",
       " 'gilead',\n",
       " 'sapphires',\n",
       " '9-month-old',\n",
       " 'scrivener',\n",
       " 'five-bedroom',\n",
       " '28-year-old',\n",
       " 'tokarski',\n",
       " 'unseasonable',\n",
       " 'porzingis',\n",
       " 'premium.',\n",
       " 'vessantara',\n",
       " 'deconstruct',\n",
       " 'bengals',\n",
       " 'hrm.org',\n",
       " 'gamers.',\n",
       " 'paint.',\n",
       " 'debated.',\n",
       " 'race.',\n",
       " 'chaudhry',\n",
       " 'berate',\n",
       " 'pricking',\n",
       " 'mastodons',\n",
       " 'cruised',\n",
       " '111-11',\n",
       " 'rainstick',\n",
       " 'gorman',\n",
       " 'swiveled',\n",
       " '7:30',\n",
       " 'farnham',\n",
       " 'anthologyfilmarchives.org',\n",
       " '518-822-0510.',\n",
       " 'drywall',\n",
       " 'landfills',\n",
       " 'mantel',\n",
       " 'flier',\n",
       " 'harlen',\n",
       " 'khdeir',\n",
       " 'season-long',\n",
       " 'utopianist',\n",
       " 'cryptolocker',\n",
       " 'barely-hidden',\n",
       " 'paneling',\n",
       " 'faranda',\n",
       " 'ill-looking',\n",
       " 'dwindle',\n",
       " 'engage.',\n",
       " 'pentangelo',\n",
       " 'shape-shifting',\n",
       " 'cuddyer',\n",
       " 'hashed',\n",
       " 'concedes',\n",
       " 'd.i.y',\n",
       " 'swaddled',\n",
       " 'marriage.',\n",
       " 'dorsey',\n",
       " 'plaintively',\n",
       " 'cranston',\n",
       " 'summonses',\n",
       " 'trucker',\n",
       " 'sprinted',\n",
       " 'stop-and-frisk',\n",
       " 'engles',\n",
       " 'drawer',\n",
       " 'mosaab',\n",
       " 'reda',\n",
       " 'laudomia',\n",
       " 'purée½',\n",
       " 'happiness.',\n",
       " 'bessire',\n",
       " 'stoudemire',\n",
       " 'conceptualism',\n",
       " 'piranha',\n",
       " 'unlocking',\n",
       " 'raylan',\n",
       " 'code.',\n",
       " 'maryursula',\n",
       " 'field-goal',\n",
       " 'wiseman',\n",
       " 'supermodel',\n",
       " 'valbuena',\n",
       " 'comforted',\n",
       " 'seven-foot',\n",
       " 'joc',\n",
       " 'bluster',\n",
       " 'surt',\n",
       " 'multicolored',\n",
       " 'fraschilla',\n",
       " 'blissed-out',\n",
       " 'stefanelli',\n",
       " 'to12',\n",
       " 'city-designated',\n",
       " 'a2020',\n",
       " 'fahs',\n",
       " 'bezy',\n",
       " '50-year-old',\n",
       " 'anti-poverty',\n",
       " 'dallagnol',\n",
       " '21-minute',\n",
       " 'al-mukarama',\n",
       " 'mindfulness-based',\n",
       " 'kreider',\n",
       " 'slimane',\n",
       " 'movement/we',\n",
       " 'bergoglio',\n",
       " 'kareem',\n",
       " 'looney',\n",
       " 'buffoonish',\n",
       " 'de-stress.',\n",
       " 'malick',\n",
       " 'zeroed',\n",
       " 'salih',\n",
       " 'unassailable',\n",
       " 'chocolate-infused',\n",
       " 'merkin',\n",
       " 'birther',\n",
       " 'héctor',\n",
       " '97-mile-per-hour',\n",
       " 'rankled',\n",
       " 'pekka',\n",
       " 'el-abidine',\n",
       " 'maese',\n",
       " 'girardi',\n",
       " 'history.',\n",
       " 'hill-rom',\n",
       " 'poggenpohl',\n",
       " 'nusra',\n",
       " 'julavits',\n",
       " 'krista',\n",
       " 'yolo',\n",
       " 'tencent',\n",
       " 'boire',\n",
       " 'mommie',\n",
       " 'ragged',\n",
       " 'splendidly',\n",
       " 'sitter',\n",
       " 'gretzky',\n",
       " 'ménage',\n",
       " 'coughlin',\n",
       " 'dannemora',\n",
       " 'mcginley',\n",
       " 'vashti',\n",
       " 'protopopov',\n",
       " 'm.r.i',\n",
       " 'corruption-related',\n",
       " 'almar',\n",
       " 'renovating',\n",
       " 'south.',\n",
       " 'double-minor',\n",
       " 'dilallo',\n",
       " 'lalas',\n",
       " 'internment.',\n",
       " 'whatsapp',\n",
       " 'coalesces',\n",
       " 'khodorkovsky',\n",
       " 'madarious',\n",
       " 'eccentrics',\n",
       " 'hypertrophied',\n",
       " 'vertiginous',\n",
       " 'golgohar',\n",
       " 'roundly',\n",
       " 'dunkers',\n",
       " '0.34',\n",
       " '€135,000',\n",
       " 'live.',\n",
       " 'nivas',\n",
       " '212-423-3337',\n",
       " 'medication.',\n",
       " '914-937-4126',\n",
       " 'cowl-neck',\n",
       " '107-year',\n",
       " 'overconsumption.',\n",
       " 'squalor',\n",
       " 'feisty',\n",
       " 'dropbox',\n",
       " 'squirmy',\n",
       " 'fiala',\n",
       " 'brucemuseum.org',\n",
       " 'time-stamped',\n",
       " '24-14',\n",
       " 'swartz',\n",
       " 'sprawled',\n",
       " 'tragic.',\n",
       " '855-17-543-541',\n",
       " 'status.',\n",
       " 'merrie',\n",
       " 'caftans',\n",
       " 'career-high',\n",
       " 'scoured',\n",
       " 'backboard-shattering',\n",
       " 'irons',\n",
       " 'bluetec',\n",
       " 'cannonball',\n",
       " '.350',\n",
       " 'relative.',\n",
       " 'monza',\n",
       " 'neitzel',\n",
       " 'platter',\n",
       " 'kway',\n",
       " 'rubies',\n",
       " 'nutty',\n",
       " 'moment.',\n",
       " 'cyberespionage',\n",
       " 'willkommenskultur',\n",
       " 'chap',\n",
       " 'brandishing',\n",
       " 'weiner',\n",
       " 'friend-of-the-court',\n",
       " '860-435-1029.',\n",
       " 'precision-guided',\n",
       " 'bortles',\n",
       " 'beazley',\n",
       " 'kinney',\n",
       " 'crayons',\n",
       " 'naser',\n",
       " 'snip',\n",
       " 'peragallos',\n",
       " 'measurenyc.com',\n",
       " 'you.',\n",
       " 'blowback',\n",
       " '99-m.p.h',\n",
       " 'decomposing',\n",
       " 'unsatisfying',\n",
       " 'milelong',\n",
       " 'institutions.',\n",
       " 'pointillistic',\n",
       " 'brien',\n",
       " 'submariner',\n",
       " 'lolita',\n",
       " 'gad',\n",
       " 'daniella',\n",
       " 'aspirational',\n",
       " 'involvement.',\n",
       " '3-pointer',\n",
       " 'right.',\n",
       " 'cliftonnj.org',\n",
       " '54.16',\n",
       " 'kiku',\n",
       " 'peps',\n",
       " 'spreadsheet.',\n",
       " 'slattery',\n",
       " 'bosso',\n",
       " 'stoops',\n",
       " 'weaned',\n",
       " 'nation.',\n",
       " 'shybunko',\n",
       " 'stillerman',\n",
       " 'trot',\n",
       " 'guggenheim.org',\n",
       " 'piecemeal',\n",
       " '96-page',\n",
       " 'lengthiest',\n",
       " 'tranquilizer',\n",
       " 'starz',\n",
       " 'yunnanese-style',\n",
       " 'lockette',\n",
       " 'quarter-century-old',\n",
       " 'free.',\n",
       " 'laze',\n",
       " 'a.l.',\n",
       " 'photogenic',\n",
       " '27,500',\n",
       " 'ceramist',\n",
       " 'offense.',\n",
       " 'second-round',\n",
       " 'unscholarly',\n",
       " 'iraqi-american',\n",
       " 'impounds',\n",
       " 'university-sponsored',\n",
       " 'pteropods',\n",
       " 'endearing',\n",
       " 'much-admired',\n",
       " 'goldhammer',\n",
       " 'perches',\n",
       " 'uucroton.org',\n",
       " '693,000',\n",
       " 'tupper',\n",
       " '203-753-0381',\n",
       " 'cat.',\n",
       " 'division-leading',\n",
       " 'telemarketing',\n",
       " 'lightenings',\n",
       " '212-423-3500',\n",
       " 'schlippe',\n",
       " 'k.k.r',\n",
       " 'uniform.',\n",
       " 'knee-jerk',\n",
       " 'hasn',\n",
       " 'groat',\n",
       " 'colusa',\n",
       " 'head-up',\n",
       " 'tomine',\n",
       " 'policy.',\n",
       " 'hirohito',\n",
       " 'sick-leave',\n",
       " 'hoshur',\n",
       " 'branchburg',\n",
       " 'alida',\n",
       " 'shotz',\n",
       " '914-693-0473.',\n",
       " 'unpredictable.',\n",
       " '1.07',\n",
       " 'two-prong',\n",
       " 'akimbo',\n",
       " 'lives.',\n",
       " 'bootstrapped',\n",
       " 'novus',\n",
       " 'lilac',\n",
       " 'facsimiles',\n",
       " 'student-centric',\n",
       " 'tarried',\n",
       " 'backswing',\n",
       " 'stiffed',\n",
       " 'poblenou',\n",
       " 'golfers',\n",
       " '.001',\n",
       " '1,455',\n",
       " 'slugfest',\n",
       " 'ribéry',\n",
       " 'sidle',\n",
       " '25,500',\n",
       " 'non-executive',\n",
       " 'stegner',\n",
       " 'komadougou',\n",
       " 'franchise-studded',\n",
       " '\\xadconsequences',\n",
       " 'croton-on-hudson',\n",
       " 'winterkorn',\n",
       " 'goalie',\n",
       " 'tip-off',\n",
       " 'male.',\n",
       " 'time.',\n",
       " 'cleef',\n",
       " 'arias',\n",
       " 'pudong',\n",
       " 'm.h',\n",
       " 'macculloch',\n",
       " 'kunduz',\n",
       " 'preternaturally',\n",
       " 'hass',\n",
       " 'shiny',\n",
       " '6-foot-6',\n",
       " 'hurriedly',\n",
       " 'matuska',\n",
       " 'back-room',\n",
       " 'straightening',\n",
       " 'eye-to-eye',\n",
       " 'plans.',\n",
       " 'soft-first-story',\n",
       " 'harker',\n",
       " 'moment-by-moment',\n",
       " 'troops.',\n",
       " 'e-book',\n",
       " '1590s',\n",
       " 'antidoping',\n",
       " 'r8',\n",
       " 'family.',\n",
       " 'chondros',\n",
       " 'mauboussin',\n",
       " 'nine-hole',\n",
       " '1-844-698-6397',\n",
       " 'intoxication.',\n",
       " 'bristling',\n",
       " 'equilibrium.',\n",
       " 'absolution',\n",
       " 'decelerating',\n",
       " 'rueben',\n",
       " 'wishbone',\n",
       " 'thrashed',\n",
       " 'medley.',\n",
       " 'crèche',\n",
       " 'velvet-lined',\n",
       " 'lapel',\n",
       " 'marchionne',\n",
       " 'sense.',\n",
       " 'gluttony',\n",
       " 'mellbin',\n",
       " 'babes',\n",
       " '860-405-9052.',\n",
       " 'amelia',\n",
       " 'conversational',\n",
       " 'ricco',\n",
       " 'scrawny',\n",
       " '212-579-0528',\n",
       " 'cey',\n",
       " 'smashingly',\n",
       " '91,000',\n",
       " 'hyperreality',\n",
       " 'idmark',\n",
       " 'low-impact',\n",
       " 'overcook',\n",
       " 'brayden',\n",
       " 'extra-virgin',\n",
       " 'basford',\n",
       " 'boundary-pushing',\n",
       " 'shredding',\n",
       " 'sound-mixing',\n",
       " 'awakens',\n",
       " 'sentencers',\n",
       " 'expressively',\n",
       " 'clumpy',\n",
       " 'accuses',\n",
       " 'season.',\n",
       " 'bohumil',\n",
       " 'non-catholics',\n",
       " 'coast.',\n",
       " 'sfyridou',\n",
       " 'ways.',\n",
       " '25-ton',\n",
       " 'cho',\n",
       " 'low-sodium',\n",
       " 'globalizing',\n",
       " 'electric.',\n",
       " 'stuff.',\n",
       " 'drunken-driving',\n",
       " 'retching',\n",
       " 'odysseys',\n",
       " 'romero',\n",
       " 'toddle',\n",
       " 'hindu-buddhist',\n",
       " 'ahistorically',\n",
       " 'valery',\n",
       " 'vote.',\n",
       " 'pick-and-roll',\n",
       " 'vulgarity',\n",
       " 'clyne',\n",
       " 'hariwati',\n",
       " 'marcille',\n",
       " '888-861-6791',\n",
       " 'scourge',\n",
       " 'arapahoe',\n",
       " 'saccharine',\n",
       " 'well-polished',\n",
       " '212-627-4819',\n",
       " 'dismissals',\n",
       " 'sofia',\n",
       " 'i.i.h.f',\n",
       " 'carwash',\n",
       " 'photo-poetics',\n",
       " 'mccann',\n",
       " 'potbellied',\n",
       " 'peeling',\n",
       " 'dolan',\n",
       " 'dandolo',\n",
       " 'sharpshooter',\n",
       " 'mortified',\n",
       " ...]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df_test_sets[df_test_sets['test_set'] == 'nyt_v1']['distinct_in_test_lower'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'two-bath',\n",
       " 'outspoken.',\n",
       " 'c.i.a.-sponsored',\n",
       " 'mozaffor',\n",
       " 'ethan',\n",
       " 'chipper',\n",
       " 'non-american',\n",
       " '88-page',\n",
       " 'fingerpicked',\n",
       " 'mean.',\n",
       " 'guessing',\n",
       " 'limped',\n",
       " 'retires',\n",
       " 'florida.',\n",
       " 'pulpy',\n",
       " 'h.d.p',\n",
       " 'galeotti',\n",
       " 'mutates',\n",
       " 'fair-market',\n",
       " 'acar',\n",
       " 'gilbertville',\n",
       " 'mcconnell',\n",
       " 'happy-slappy',\n",
       " 'xiaochuan',\n",
       " 'marktwainhouse.org',\n",
       " 'most-searched',\n",
       " 'humanize',\n",
       " 'malty',\n",
       " 'telegraphing',\n",
       " 'meaty',\n",
       " 'well-trodden',\n",
       " 'holtby',\n",
       " 'deavere',\n",
       " 'nonwhite',\n",
       " 'freda',\n",
       " 'mermelstein',\n",
       " 'flagstaff',\n",
       " 'djemel',\n",
       " 'pre-recession',\n",
       " 'balducci',\n",
       " 'crazy.',\n",
       " 'flirty',\n",
       " 'andino',\n",
       " 'bilkent',\n",
       " 'birthrates',\n",
       " 'coy',\n",
       " 'recker',\n",
       " 'hothouse',\n",
       " 'worst-hit',\n",
       " 'trivially',\n",
       " 'sugary',\n",
       " 'ibargüen',\n",
       " 'message.',\n",
       " 'chernow',\n",
       " 'pask',\n",
       " 'expressionless',\n",
       " 'g.m.',\n",
       " 'aksyonov',\n",
       " 'serena',\n",
       " 'disdainful',\n",
       " 'layup',\n",
       " 'inception.',\n",
       " 'desmondfishlibrary.org',\n",
       " 'open-water',\n",
       " 'greater.',\n",
       " 'carmona',\n",
       " 'bread-and-butter',\n",
       " 'goan',\n",
       " 'snows',\n",
       " 'daata',\n",
       " 'bretas',\n",
       " 'nicolai',\n",
       " 'mondella',\n",
       " '€100',\n",
       " 'catsoulis',\n",
       " 'mind-share',\n",
       " '1893-1983',\n",
       " '34,540',\n",
       " 'low-key',\n",
       " 'souchon',\n",
       " 'abiquiu',\n",
       " 'expecting.',\n",
       " '498,000',\n",
       " 'christines-store.com',\n",
       " 'quaint',\n",
       " 'cockadoodleq',\n",
       " 'folkified',\n",
       " 'spinster',\n",
       " '203-222-7070',\n",
       " '211,000',\n",
       " 'self-portrait',\n",
       " '212-505-5181',\n",
       " '3,380',\n",
       " 'soy-lacquered',\n",
       " 'sustainability.',\n",
       " 'gadsby',\n",
       " 'leftovers',\n",
       " 'disheartening',\n",
       " 'made.',\n",
       " 'sopacnow.org',\n",
       " 'last-2',\n",
       " 'primped',\n",
       " 'haven.',\n",
       " 'vidinli',\n",
       " 'asians.',\n",
       " 'bad-mouth',\n",
       " 'lenson.',\n",
       " 'charli',\n",
       " 'much-promoted',\n",
       " 'wisteria',\n",
       " '1-2',\n",
       " 'chesters',\n",
       " 'high.',\n",
       " 'self-revealing',\n",
       " 'song-and-dance',\n",
       " 'montpelier',\n",
       " 'amaury',\n",
       " 'cornejo',\n",
       " 'banged-up',\n",
       " 'sneer',\n",
       " 'fishkill',\n",
       " 'areola',\n",
       " 'sanctions.',\n",
       " 'sternheimer',\n",
       " 'bare-knuckle',\n",
       " 'authenticate',\n",
       " 'mermaid',\n",
       " 'chromecast',\n",
       " 'double-double',\n",
       " 'realistic.',\n",
       " 'fact-obsessed',\n",
       " 'melgen',\n",
       " 'starts-win-place-show',\n",
       " 'treetop',\n",
       " '22-5',\n",
       " 'ellenville',\n",
       " 'system.',\n",
       " 'stagy',\n",
       " 'swindlers',\n",
       " 'catheter',\n",
       " 'slights',\n",
       " 'nuggets',\n",
       " 'musicmountain.org',\n",
       " 'resurrectionists',\n",
       " 'mumbled',\n",
       " 'hypochondriac',\n",
       " 'cobbling',\n",
       " 'estell',\n",
       " 'rafters',\n",
       " 'stanikzai',\n",
       " 'worshipers',\n",
       " 'markov',\n",
       " 'carma',\n",
       " 'shyly',\n",
       " 'chauffeur.',\n",
       " 'encore',\n",
       " 'engendering',\n",
       " 'burfoot',\n",
       " 'gizmodo',\n",
       " 'lamberton',\n",
       " 'tsipras',\n",
       " 'least.',\n",
       " 'beeping',\n",
       " 'heffner',\n",
       " 'matchup',\n",
       " 'tub',\n",
       " 'cinephile',\n",
       " 'lytham',\n",
       " 'anticorruption',\n",
       " 'expletive',\n",
       " 'ganek',\n",
       " 'badstuber',\n",
       " 'overregulation',\n",
       " 'omissions',\n",
       " 'shanleys',\n",
       " 'kathak',\n",
       " 'fleetingly',\n",
       " '240,091',\n",
       " 'deferments',\n",
       " 'esopus',\n",
       " 'banstetter',\n",
       " 'fizzy',\n",
       " 'mennonite',\n",
       " 'evgeni',\n",
       " 'street-prospect',\n",
       " 'cash-and-stock',\n",
       " 'chatting',\n",
       " 'namechecked',\n",
       " 'artists.',\n",
       " 'theaterworks',\n",
       " 'burkburnett',\n",
       " 'challenging.',\n",
       " 'corner.',\n",
       " 'draft-eligible',\n",
       " 'self.',\n",
       " 'carefree',\n",
       " 'thingamajigs',\n",
       " 'incomprehensible',\n",
       " 'lipton',\n",
       " 'gardened',\n",
       " 'cast-metal',\n",
       " 'gibneydance.org',\n",
       " 'woolliness',\n",
       " 'bookends',\n",
       " 'all-cause',\n",
       " 'company.',\n",
       " 'yeses',\n",
       " 'late-october',\n",
       " 'eun-me',\n",
       " 'stretcher',\n",
       " 'everdeen',\n",
       " 'chafee',\n",
       " 'whit',\n",
       " 'mcilroy',\n",
       " 'drags',\n",
       " '11-6-3',\n",
       " 'beef-filled',\n",
       " 'adriane',\n",
       " 'incontinent',\n",
       " 'pagliuca',\n",
       " 'plaats',\n",
       " 'affability',\n",
       " 'caballero',\n",
       " '★',\n",
       " 'giacometti',\n",
       " 'bathers',\n",
       " 'anti-israel',\n",
       " 'cregg',\n",
       " '877-840-0457',\n",
       " 'hand-built',\n",
       " 'mazzoleni',\n",
       " 'o.k.',\n",
       " 'pixar',\n",
       " 'visuals',\n",
       " 'treason.',\n",
       " '5,000-per-unit',\n",
       " 'codpieces',\n",
       " 'mahottari',\n",
       " 'ernesta',\n",
       " 'well-intentioned',\n",
       " 'rezoning',\n",
       " 'redemption.',\n",
       " 'daffy',\n",
       " 'mournfully',\n",
       " 'hrabal',\n",
       " 'motherless',\n",
       " 'flat-screen',\n",
       " 'hollander',\n",
       " 'stromboli',\n",
       " '2:28',\n",
       " 'high-sticking',\n",
       " 'repaved',\n",
       " 'tricky',\n",
       " 'draymond',\n",
       " 'new-media',\n",
       " 'aarp',\n",
       " 'rwby.',\n",
       " 'exhumes',\n",
       " '91-year-old',\n",
       " 'delgado',\n",
       " 'heights.',\n",
       " 'scrubby',\n",
       " 'terranova',\n",
       " 'patted',\n",
       " 'marea',\n",
       " 'andrzej',\n",
       " 'views.',\n",
       " 'trice',\n",
       " 'undercooked',\n",
       " 'moskowitz',\n",
       " 'shape-up',\n",
       " 'vote16usa',\n",
       " 'pappalardo',\n",
       " '914-271-6612.',\n",
       " 'fyi',\n",
       " 'b.y.u',\n",
       " 'memento',\n",
       " 'samour',\n",
       " '2:35',\n",
       " 'adiba',\n",
       " '226,000',\n",
       " 'ripostes',\n",
       " 'aundrea',\n",
       " '6:45.',\n",
       " 'unnaturally',\n",
       " 'spirit.',\n",
       " 'newhavenmuseum.org',\n",
       " 'gold-framed',\n",
       " 'beaumont-la-ronce.',\n",
       " 'water.',\n",
       " 'portia',\n",
       " 'folk-art',\n",
       " 'reeboks',\n",
       " 'shinehee',\n",
       " 'sentlij',\n",
       " 'legit',\n",
       " 'impressionable',\n",
       " 'vengeful',\n",
       " 'talladega',\n",
       " 'haberstock',\n",
       " 'angriness',\n",
       " 'peacekeeper',\n",
       " 're-sign',\n",
       " 'bankruptcies',\n",
       " 'macklin',\n",
       " 'lacava',\n",
       " 'vitello',\n",
       " 'judd',\n",
       " 'year.',\n",
       " 'ratmansky',\n",
       " 'grier',\n",
       " 'sauna',\n",
       " 'stringing',\n",
       " 'oncologist',\n",
       " 'sorrows.',\n",
       " 'tomei',\n",
       " 'macgyver',\n",
       " 'ghani',\n",
       " 'overcost',\n",
       " 'bluedoorartcenter.org',\n",
       " 'satchmo',\n",
       " 'jouret-epstein',\n",
       " 'dispiriting',\n",
       " 'different.',\n",
       " '44.0.',\n",
       " 'bellocq',\n",
       " 'ursula',\n",
       " 'artswestchester',\n",
       " 'accident-prediction',\n",
       " 'set.',\n",
       " 'routers',\n",
       " 'chukwu',\n",
       " '1,087.70',\n",
       " 'tiebreak',\n",
       " 'parlor-gallery.com',\n",
       " 's.u.v',\n",
       " 'one.',\n",
       " 'beast.',\n",
       " 'sidley',\n",
       " 'reassessing',\n",
       " 'moto',\n",
       " 'shabab',\n",
       " 'buzzfeed',\n",
       " 'bests',\n",
       " 'councilman',\n",
       " 'delves',\n",
       " 'biggs',\n",
       " 'lower-body',\n",
       " 'fourth-quarter',\n",
       " 'used-books',\n",
       " 'wander',\n",
       " 'playsam',\n",
       " 'soft-focus',\n",
       " 'life-enhancing',\n",
       " 'barthelme',\n",
       " '802.11n',\n",
       " 'johanna',\n",
       " 'andrii',\n",
       " 'puffy',\n",
       " 'blalock',\n",
       " 'rainey',\n",
       " 'cholesterol-lowering',\n",
       " 'strouse',\n",
       " 'showthe',\n",
       " 'airbag',\n",
       " 'républicain',\n",
       " 'rediscover',\n",
       " 'unmendable',\n",
       " 'bushel',\n",
       " 'naomi',\n",
       " 'disney-owned',\n",
       " '32-34',\n",
       " 'rubinmuseum.org',\n",
       " 'chiquita',\n",
       " 'angelika',\n",
       " 'closets',\n",
       " 'r.f.a.',\n",
       " 'constrict',\n",
       " 'showrunners',\n",
       " 'strode',\n",
       " 'front-desk',\n",
       " 'bharara',\n",
       " 'helpful.',\n",
       " 'operation.',\n",
       " 'for-profits',\n",
       " 'baby.',\n",
       " 'wonks',\n",
       " 'infiltrate',\n",
       " 'republic.',\n",
       " 'tradicionales',\n",
       " 'e.t.f.s',\n",
       " 'church.',\n",
       " '28-0',\n",
       " 'ticked',\n",
       " 'in-person',\n",
       " '201-227-1030',\n",
       " 'haddock',\n",
       " '8-year-old',\n",
       " 'icing',\n",
       " 'attired',\n",
       " 'glassed-in',\n",
       " 'man.',\n",
       " 'sofa',\n",
       " 'level.',\n",
       " 'concussion',\n",
       " 'parthé',\n",
       " 'keyhole',\n",
       " 'fdrlibrary.marist.edu',\n",
       " 'wags',\n",
       " 'yevgeny',\n",
       " '45-year-old',\n",
       " 'marshmallow',\n",
       " '212-924-7771',\n",
       " 'heinrichs',\n",
       " 'wither',\n",
       " 'high-testosterone',\n",
       " 'carbonell',\n",
       " 'openhanded',\n",
       " 'heupel',\n",
       " 'resident./now',\n",
       " 'defenseman',\n",
       " 'granderson',\n",
       " 'quarts',\n",
       " 'jacint',\n",
       " 'prowling',\n",
       " 'véra',\n",
       " 'on.',\n",
       " 'bicker.',\n",
       " 'mid-eastern',\n",
       " '69,260',\n",
       " 'punts',\n",
       " 'riotously',\n",
       " 'snarkiness',\n",
       " 'calif.',\n",
       " 'morgantown',\n",
       " 'interlocking',\n",
       " 'altogether.',\n",
       " 'pavéd',\n",
       " 'entwining',\n",
       " 'are.',\n",
       " 'finney',\n",
       " 'rubel',\n",
       " 'true-to-life',\n",
       " 'fieilo',\n",
       " 'sliminess',\n",
       " 'chettinadu',\n",
       " 'sieff',\n",
       " 'capito',\n",
       " 'disinfect',\n",
       " 'mouthwash',\n",
       " 'toes',\n",
       " 'nook',\n",
       " 'brookfieldplaceny.com/eventscalendar',\n",
       " 'gaines',\n",
       " 'clowney',\n",
       " 'sonatina',\n",
       " 'ketchup',\n",
       " 'neto',\n",
       " 'gesticulations',\n",
       " '914-375-5100',\n",
       " 'kelley',\n",
       " 'rap-star',\n",
       " 'podcaster',\n",
       " 'fix.',\n",
       " 'repeatedly.',\n",
       " 'landon',\n",
       " 'sondos',\n",
       " 'empire.',\n",
       " 'eyebrows',\n",
       " 'geli',\n",
       " 'wayfair.',\n",
       " 'wouter',\n",
       " 'bargained',\n",
       " '\\xadböttger',\n",
       " 'fierstein',\n",
       " 'dollop',\n",
       " 'drumheller',\n",
       " 'unkempt',\n",
       " 'straw-man',\n",
       " 'nazar',\n",
       " 'recasting',\n",
       " 're-emerging',\n",
       " 'bowl-eligible',\n",
       " 'md.',\n",
       " 'enchanting',\n",
       " '0.434',\n",
       " 'chiasson',\n",
       " 'springer',\n",
       " 'abdul-hamed',\n",
       " 'gravy',\n",
       " '616,000',\n",
       " 'responsibility.',\n",
       " 'cuatro',\n",
       " 'uncontrollable',\n",
       " 'catholic-jewish',\n",
       " 'often-derided',\n",
       " 'dollhouses',\n",
       " 'casserole',\n",
       " 'culpable',\n",
       " 'then-record',\n",
       " 'brainstormed',\n",
       " 'unfancy',\n",
       " 'ijmeer',\n",
       " 'résumés',\n",
       " 'dez',\n",
       " 'lasermania',\n",
       " 'batsman',\n",
       " 'satisfactorily',\n",
       " 'foursomes',\n",
       " 'demaryius',\n",
       " 'abby',\n",
       " 'overseas.',\n",
       " 'front-load',\n",
       " 'undress',\n",
       " 'pierre-laurent',\n",
       " 'soybean',\n",
       " '7,135',\n",
       " 'n.s.a.',\n",
       " 'oladipo',\n",
       " 'pamper',\n",
       " 'rejoining',\n",
       " 'ratpac',\n",
       " 'máirtín',\n",
       " 'tex',\n",
       " 'fllac.vassar.edu',\n",
       " 'hildrew',\n",
       " '5.35',\n",
       " '\\xadconfirm',\n",
       " 'asbury',\n",
       " 'towpath',\n",
       " 'hallowed',\n",
       " 'apatow',\n",
       " 'closed-door',\n",
       " 'harf',\n",
       " 'metzitzah',\n",
       " 'schenn',\n",
       " 'audacious',\n",
       " 'moby-dick',\n",
       " 'bitcoins',\n",
       " 'n.b.a',\n",
       " 'wirth',\n",
       " 'half-bird',\n",
       " 'carbapenem-resistant',\n",
       " 'in-room',\n",
       " 'missive',\n",
       " 'yekaterinburg',\n",
       " 'rosenthal',\n",
       " 'fla.',\n",
       " 'ilit',\n",
       " 'azoulay',\n",
       " 'hadera',\n",
       " 'blue-blooded',\n",
       " 'half-dollar-size',\n",
       " 'independent-minded',\n",
       " 'unintelligibly',\n",
       " 'gilead',\n",
       " 'sapphires',\n",
       " '9-month-old',\n",
       " 'scrivener',\n",
       " 'five-bedroom',\n",
       " '28-year-old',\n",
       " 'tokarski',\n",
       " 'unseasonable',\n",
       " 'porzingis',\n",
       " 'premium.',\n",
       " 'vessantara',\n",
       " 'deconstruct',\n",
       " 'bengals',\n",
       " 'hrm.org',\n",
       " 'gamers.',\n",
       " 'paint.',\n",
       " 'debated.',\n",
       " 'race.',\n",
       " 'chaudhry',\n",
       " 'berate',\n",
       " 'pricking',\n",
       " 'mastodons',\n",
       " 'cruised',\n",
       " '111-11',\n",
       " 'rainstick',\n",
       " 'gorman',\n",
       " 'swiveled',\n",
       " '7:30',\n",
       " 'farnham',\n",
       " 'anthologyfilmarchives.org',\n",
       " '518-822-0510.',\n",
       " 'drywall',\n",
       " 'landfills',\n",
       " 'mantel',\n",
       " 'flier',\n",
       " 'harlen',\n",
       " 'khdeir',\n",
       " 'season-long',\n",
       " 'utopianist',\n",
       " 'cryptolocker',\n",
       " 'barely-hidden',\n",
       " 'paneling',\n",
       " 'faranda',\n",
       " 'ill-looking',\n",
       " 'dwindle',\n",
       " 'engage.',\n",
       " 'pentangelo',\n",
       " 'shape-shifting',\n",
       " 'cuddyer',\n",
       " 'hashed',\n",
       " 'concedes',\n",
       " 'd.i.y',\n",
       " 'swaddled',\n",
       " 'marriage.',\n",
       " 'dorsey',\n",
       " 'plaintively',\n",
       " 'cranston',\n",
       " 'summonses',\n",
       " 'trucker',\n",
       " 'sprinted',\n",
       " 'stop-and-frisk',\n",
       " 'engles',\n",
       " 'drawer',\n",
       " 'mosaab',\n",
       " 'reda',\n",
       " 'laudomia',\n",
       " 'purée½',\n",
       " 'happiness.',\n",
       " 'bessire',\n",
       " 'stoudemire',\n",
       " 'conceptualism',\n",
       " 'piranha',\n",
       " 'unlocking',\n",
       " 'raylan',\n",
       " 'code.',\n",
       " 'maryursula',\n",
       " 'field-goal',\n",
       " 'wiseman',\n",
       " 'supermodel',\n",
       " 'valbuena',\n",
       " 'comforted',\n",
       " 'seven-foot',\n",
       " 'joc',\n",
       " 'bluster',\n",
       " 'surt',\n",
       " 'multicolored',\n",
       " 'fraschilla',\n",
       " 'blissed-out',\n",
       " 'stefanelli',\n",
       " 'to12',\n",
       " 'city-designated',\n",
       " 'a2020',\n",
       " 'fahs',\n",
       " 'bezy',\n",
       " '50-year-old',\n",
       " 'anti-poverty',\n",
       " 'dallagnol',\n",
       " '21-minute',\n",
       " 'al-mukarama',\n",
       " 'mindfulness-based',\n",
       " 'kreider',\n",
       " 'slimane',\n",
       " 'movement/we',\n",
       " 'bergoglio',\n",
       " 'kareem',\n",
       " 'looney',\n",
       " 'buffoonish',\n",
       " 'de-stress.',\n",
       " 'malick',\n",
       " 'zeroed',\n",
       " 'salih',\n",
       " 'unassailable',\n",
       " 'chocolate-infused',\n",
       " 'merkin',\n",
       " 'birther',\n",
       " 'héctor',\n",
       " '97-mile-per-hour',\n",
       " 'rankled',\n",
       " 'pekka',\n",
       " 'el-abidine',\n",
       " 'maese',\n",
       " 'girardi',\n",
       " 'history.',\n",
       " 'hill-rom',\n",
       " 'poggenpohl',\n",
       " 'nusra',\n",
       " 'julavits',\n",
       " 'krista',\n",
       " 'yolo',\n",
       " 'tencent',\n",
       " 'boire',\n",
       " 'mommie',\n",
       " 'ragged',\n",
       " 'splendidly',\n",
       " 'sitter',\n",
       " 'gretzky',\n",
       " 'ménage',\n",
       " 'coughlin',\n",
       " 'dannemora',\n",
       " 'mcginley',\n",
       " 'vashti',\n",
       " 'protopopov',\n",
       " 'm.r.i',\n",
       " 'corruption-related',\n",
       " 'almar',\n",
       " 'renovating',\n",
       " 'south.',\n",
       " 'double-minor',\n",
       " 'dilallo',\n",
       " 'lalas',\n",
       " 'internment.',\n",
       " 'whatsapp',\n",
       " 'coalesces',\n",
       " 'khodorkovsky',\n",
       " 'madarious',\n",
       " 'eccentrics',\n",
       " 'hypertrophied',\n",
       " 'vertiginous',\n",
       " 'golgohar',\n",
       " 'roundly',\n",
       " 'dunkers',\n",
       " '0.34',\n",
       " '€135,000',\n",
       " 'live.',\n",
       " 'nivas',\n",
       " '212-423-3337',\n",
       " 'medication.',\n",
       " '914-937-4126',\n",
       " 'cowl-neck',\n",
       " '107-year',\n",
       " 'overconsumption.',\n",
       " 'squalor',\n",
       " 'feisty',\n",
       " 'dropbox',\n",
       " 'squirmy',\n",
       " 'fiala',\n",
       " 'brucemuseum.org',\n",
       " 'time-stamped',\n",
       " '24-14',\n",
       " 'swartz',\n",
       " 'sprawled',\n",
       " 'tragic.',\n",
       " '855-17-543-541',\n",
       " 'status.',\n",
       " 'merrie',\n",
       " 'caftans',\n",
       " 'career-high',\n",
       " 'scoured',\n",
       " 'backboard-shattering',\n",
       " 'irons',\n",
       " 'bluetec',\n",
       " 'cannonball',\n",
       " '.350',\n",
       " 'relative.',\n",
       " 'monza',\n",
       " 'neitzel',\n",
       " 'platter',\n",
       " 'kway',\n",
       " 'rubies',\n",
       " 'nutty',\n",
       " 'moment.',\n",
       " 'cyberespionage',\n",
       " 'willkommenskultur',\n",
       " 'chap',\n",
       " 'brandishing',\n",
       " 'weiner',\n",
       " 'friend-of-the-court',\n",
       " '860-435-1029.',\n",
       " 'precision-guided',\n",
       " 'bortles',\n",
       " 'beazley',\n",
       " 'kinney',\n",
       " 'crayons',\n",
       " 'naser',\n",
       " 'snip',\n",
       " 'peragallos',\n",
       " 'measurenyc.com',\n",
       " 'you.',\n",
       " 'blowback',\n",
       " '99-m.p.h',\n",
       " 'decomposing',\n",
       " 'unsatisfying',\n",
       " 'milelong',\n",
       " 'institutions.',\n",
       " 'pointillistic',\n",
       " 'brien',\n",
       " 'submariner',\n",
       " 'lolita',\n",
       " 'gad',\n",
       " 'daniella',\n",
       " 'aspirational',\n",
       " 'involvement.',\n",
       " '3-pointer',\n",
       " 'right.',\n",
       " 'cliftonnj.org',\n",
       " '54.16',\n",
       " 'kiku',\n",
       " 'peps',\n",
       " 'spreadsheet.',\n",
       " 'slattery',\n",
       " 'bosso',\n",
       " 'stoops',\n",
       " 'weaned',\n",
       " 'nation.',\n",
       " 'shybunko',\n",
       " 'stillerman',\n",
       " 'trot',\n",
       " 'guggenheim.org',\n",
       " 'piecemeal',\n",
       " '96-page',\n",
       " 'lengthiest',\n",
       " 'tranquilizer',\n",
       " 'starz',\n",
       " 'yunnanese-style',\n",
       " 'lockette',\n",
       " 'quarter-century-old',\n",
       " 'free.',\n",
       " 'laze',\n",
       " 'a.l.',\n",
       " 'photogenic',\n",
       " '27,500',\n",
       " 'ceramist',\n",
       " 'offense.',\n",
       " 'second-round',\n",
       " 'unscholarly',\n",
       " 'iraqi-american',\n",
       " 'impounds',\n",
       " 'university-sponsored',\n",
       " 'pteropods',\n",
       " 'endearing',\n",
       " 'much-admired',\n",
       " 'goldhammer',\n",
       " 'perches',\n",
       " 'uucroton.org',\n",
       " '693,000',\n",
       " 'tupper',\n",
       " '203-753-0381',\n",
       " 'cat.',\n",
       " 'division-leading',\n",
       " 'telemarketing',\n",
       " 'lightenings',\n",
       " '212-423-3500',\n",
       " 'schlippe',\n",
       " 'k.k.r',\n",
       " 'uniform.',\n",
       " 'knee-jerk',\n",
       " 'hasn',\n",
       " 'groat',\n",
       " 'colusa',\n",
       " 'head-up',\n",
       " 'tomine',\n",
       " 'policy.',\n",
       " 'hirohito',\n",
       " 'sick-leave',\n",
       " 'hoshur',\n",
       " 'branchburg',\n",
       " 'alida',\n",
       " 'shotz',\n",
       " '914-693-0473.',\n",
       " 'unpredictable.',\n",
       " '1.07',\n",
       " 'two-prong',\n",
       " 'akimbo',\n",
       " 'lives.',\n",
       " 'bootstrapped',\n",
       " 'novus',\n",
       " 'lilac',\n",
       " 'facsimiles',\n",
       " 'student-centric',\n",
       " 'tarried',\n",
       " 'backswing',\n",
       " 'stiffed',\n",
       " 'poblenou',\n",
       " 'golfers',\n",
       " '.001',\n",
       " '1,455',\n",
       " 'slugfest',\n",
       " 'ribéry',\n",
       " 'sidle',\n",
       " '25,500',\n",
       " 'non-executive',\n",
       " 'stegner',\n",
       " 'komadougou',\n",
       " 'franchise-studded',\n",
       " '\\xadconsequences',\n",
       " 'croton-on-hudson',\n",
       " 'winterkorn',\n",
       " 'goalie',\n",
       " 'tip-off',\n",
       " 'male.',\n",
       " 'time.',\n",
       " 'cleef',\n",
       " 'arias',\n",
       " 'pudong',\n",
       " 'm.h',\n",
       " 'macculloch',\n",
       " 'kunduz',\n",
       " 'preternaturally',\n",
       " 'hass',\n",
       " 'shiny',\n",
       " '6-foot-6',\n",
       " 'hurriedly',\n",
       " 'matuska',\n",
       " 'back-room',\n",
       " 'straightening',\n",
       " 'eye-to-eye',\n",
       " 'plans.',\n",
       " 'soft-first-story',\n",
       " 'harker',\n",
       " 'moment-by-moment',\n",
       " 'troops.',\n",
       " 'e-book',\n",
       " '1590s',\n",
       " 'antidoping',\n",
       " 'r8',\n",
       " 'family.',\n",
       " 'chondros',\n",
       " 'mauboussin',\n",
       " 'nine-hole',\n",
       " '1-844-698-6397',\n",
       " 'intoxication.',\n",
       " 'bristling',\n",
       " 'equilibrium.',\n",
       " 'absolution',\n",
       " 'decelerating',\n",
       " 'rueben',\n",
       " 'wishbone',\n",
       " 'thrashed',\n",
       " 'medley.',\n",
       " 'crèche',\n",
       " 'velvet-lined',\n",
       " 'lapel',\n",
       " 'marchionne',\n",
       " 'sense.',\n",
       " 'gluttony',\n",
       " 'mellbin',\n",
       " 'babes',\n",
       " '860-405-9052.',\n",
       " 'amelia',\n",
       " 'conversational',\n",
       " 'ricco',\n",
       " 'scrawny',\n",
       " '212-579-0528',\n",
       " 'cey',\n",
       " 'smashingly',\n",
       " '91,000',\n",
       " 'hyperreality',\n",
       " 'idmark',\n",
       " 'low-impact',\n",
       " 'overcook',\n",
       " 'brayden',\n",
       " 'extra-virgin',\n",
       " 'basford',\n",
       " 'boundary-pushing',\n",
       " 'shredding',\n",
       " 'sound-mixing',\n",
       " 'awakens',\n",
       " 'sentencers',\n",
       " 'expressively',\n",
       " 'clumpy',\n",
       " 'accuses',\n",
       " 'season.',\n",
       " 'bohumil',\n",
       " 'non-catholics',\n",
       " 'coast.',\n",
       " 'sfyridou',\n",
       " 'ways.',\n",
       " '25-ton',\n",
       " 'cho',\n",
       " 'low-sodium',\n",
       " 'globalizing',\n",
       " 'electric.',\n",
       " 'stuff.',\n",
       " 'drunken-driving',\n",
       " 'retching',\n",
       " 'odysseys',\n",
       " 'romero',\n",
       " 'toddle',\n",
       " 'hindu-buddhist',\n",
       " 'ahistorically',\n",
       " 'valery',\n",
       " 'vote.',\n",
       " 'pick-and-roll',\n",
       " 'vulgarity',\n",
       " 'clyne',\n",
       " 'hariwati',\n",
       " 'marcille',\n",
       " '888-861-6791',\n",
       " 'scourge',\n",
       " 'arapahoe',\n",
       " 'saccharine',\n",
       " 'well-polished',\n",
       " '212-627-4819',\n",
       " 'dismissals',\n",
       " 'sofia',\n",
       " 'i.i.h.f',\n",
       " 'carwash',\n",
       " 'photo-poetics',\n",
       " 'mccann',\n",
       " 'potbellied',\n",
       " 'peeling',\n",
       " 'dolan',\n",
       " 'dandolo',\n",
       " 'sharpshooter',\n",
       " 'mortified',\n",
       " ...}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sets.iloc[2]['distinct_in_test_lower']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2553817"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_freqdist.N()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98853"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(squad_freqdist_lower.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3773"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([x for x in squad_freqdist_lower.keys() if x in wordsapi_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3773"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set.intersection(set(squad_freqdist_lower.keys()), set(wordsapi_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://wordsapiv1.p.rapidapi.com/words/{}\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-host': \"wordsapiv1.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': \"\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url.format('food'), headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': 'food',\n",
       " 'results': [{'definition': 'any solid substance (as opposed to liquid) that is used as a source of nourishment',\n",
       "   'partOfSpeech': 'noun',\n",
       "   'synonyms': ['solid food'],\n",
       "   'typeOf': ['solid'],\n",
       "   'hasTypes': ['garden truck',\n",
       "    'green goods',\n",
       "    'green groceries',\n",
       "    'health food',\n",
       "    'junk food',\n",
       "    'leftovers',\n",
       "    'loaf',\n",
       "    'yoghurt',\n",
       "    'convenience food',\n",
       "    'meat',\n",
       "    'cheese',\n",
       "    'dika bread',\n",
       "    'chocolate',\n",
       "    'baked goods',\n",
       "    'fresh foods',\n",
       "    'breakfast food',\n",
       "    'butter',\n",
       "    'fresh food',\n",
       "    'coconut',\n",
       "    'coconut meat',\n",
       "    'yogurt',\n",
       "    'pasta',\n",
       "    'alimentary paste',\n",
       "    'produce',\n",
       "    'fish',\n",
       "    'yoghourt',\n",
       "    'seafood',\n",
       "    'slop'],\n",
       "   'partOf': ['nutrient'],\n",
       "   'examples': ['food and drink']},\n",
       "  {'definition': 'anything that provides mental stimulus for thinking',\n",
       "   'partOfSpeech': 'noun',\n",
       "   'synonyms': ['food for thought', 'intellectual nourishment'],\n",
       "   'typeOf': ['mental object', 'content', 'cognitive content'],\n",
       "   'hasTypes': ['pabulum']},\n",
       "  {'definition': 'any substance that can be metabolized by an animal to give energy and build tissue',\n",
       "   'partOfSpeech': 'noun',\n",
       "   'synonyms': ['nutrient'],\n",
       "   'typeOf': ['substance'],\n",
       "   'hasTypes': ['yolk',\n",
       "    'alimentation',\n",
       "    'beverage',\n",
       "    'chyme',\n",
       "    'comestible',\n",
       "    'comfort food',\n",
       "    'commissariat',\n",
       "    'culture medium',\n",
       "    'drink',\n",
       "    'drinkable',\n",
       "    'eatable',\n",
       "    'edible',\n",
       "    'fare',\n",
       "    'feed',\n",
       "    'food product',\n",
       "    'foodstuff',\n",
       "    'manna',\n",
       "    'manna from heaven',\n",
       "    'medium',\n",
       "    'micronutrient',\n",
       "    'miraculous food',\n",
       "    'nourishment',\n",
       "    'nutriment',\n",
       "    'nutrition',\n",
       "    'pabulum',\n",
       "    'potable',\n",
       "    'provender',\n",
       "    'provisions',\n",
       "    'victuals',\n",
       "    'vitellus',\n",
       "    'water',\n",
       "    'aliment',\n",
       "    'soul food',\n",
       "    'sustenance',\n",
       "    'viands',\n",
       "    'victual'],\n",
       "   'hasParts': ['solid food']}],\n",
       " 'syllables': {'count': 1, 'list': ['food']},\n",
       " 'pronunciation': {'all': 'fud'},\n",
       " 'frequency': 5.27}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'success': False, 'message': 'word not found'},\n",
       " {'success': False, 'message': 'word not found'},\n",
       " {'success': False, 'message': 'word not found'},\n",
       " {'success': False, 'message': 'word not found'},\n",
       " {'success': False, 'message': 'word not found'}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
