{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Stanford CoreNLP server\n",
    "`java -Xmx16g -cp C:\\stanford-corenlp-latest\\stanford-corenlp-4.0.0\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9002 -timeout 600 -threads 5 -maxCharLength 100000 -quiet False -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Justin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from hyphen import Hyphenator\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import stanfordnlp\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Uncomment if needed to fix this error:\n",
    "# OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "NLP_PORT = 9002\n",
    "WORDS_API_KEY = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_PATH = './predictions'\n",
    "TEST_SETS_PATH = './test_sets'\n",
    "MODEL_EVALS_URL = 'https://squad-model-evals.s3-us-west-2.amazonaws.com/model_db.json'\n",
    "\n",
    "#SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT', 'dev-v1.1']\n",
    "SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQuAD Training Lexical Diversity: 0.043178504959439146\n",
      "SQuAD Training Lexical Diversity (lower case): 0.03877918893738047\n"
     ]
    }
   ],
   "source": [
    "# Get the SQuAD training set data for later comparisons\n",
    "\n",
    "with open(TEST_SETS_PATH + '/train-v2.0.json') as f:\n",
    "    s_json = json.load(f)\n",
    "    \n",
    "full_squad_text = ' '.join([x['context'] for y in s_json['data'] for x in y['paragraphs']])    \n",
    "\n",
    "squad_freqdist = nltk.FreqDist(nltk.tokenize.word_tokenize(full_squad_text))\n",
    "squad_freqdist_lower = nltk.FreqDist(nltk.tokenize.word_tokenize(full_squad_text.lower()))\n",
    "\n",
    "# SQuAD lexical diversity\n",
    "print('SQuAD Training Lexical Diversity:', len(set(nltk.tokenize.word_tokenize(full_squad_text)))/len(nltk.tokenize.word_tokenize(full_squad_text)))\n",
    "print('SQuAD Training Lexical Diversity (lower case):', len(set(nltk.tokenize.word_tokenize(full_squad_text.lower())))/len(nltk.tokenize.word_tokenize(full_squad_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Justin\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Use the Words API to get results for each word that appears in one of the test sets or the SQuAD training set\n",
    "\n",
    "# API key available via https://www.wordsapi.com/ and https://rapidapi.com/developer/dashboard\n",
    "# Sample file available at https://s3.amazonaws.com/wordsapi/wordsapi_sample.zip\n",
    "\n",
    "word_api_url = \"https://wordsapiv1.p.rapidapi.com/words/{}\"\n",
    "word_api_headers = {\n",
    "    'x-rapidapi-host': \"wordsapiv1.p.rapidapi.com\",\n",
    "    'x-rapidapi-key': WORDS_API_KEY\n",
    "    }\n",
    "\n",
    "# Words API provided a sample file of some of their results for free. \n",
    "# Don't want to waste API calls requesting results we already have.\n",
    "\n",
    "# Load the sample file into wordsapi_dict to filter on.\n",
    "with open('datafiles\\wordsapi_sample.json', encoding='utf8') as f:\n",
    "    words_api_dict = json.load(f)\n",
    "    \n",
    "# Create a Stanford CoreNLP pipeline for use later.     \n",
    "nlp = stanfordnlp.Pipeline(processors='tokenize', use_gpu=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eval_file(eval_file_path, model_evals_url, overwrite=False):\n",
    "    if (not os.path.exists(eval_file_path)) or overwrite:\n",
    "        r = requests.get(model_evals_url)\n",
    "                        \n",
    "        with open(eval_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(r.text)\n",
    "\n",
    "    else:\n",
    "        print('File Exists')\n",
    "    \n",
    "    \n",
    "\n",
    "def write_output(output_file_path, list_to_write):\n",
    "    fields = list_to_write[0].keys()\n",
    "    \n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, \n",
    "                                    fieldnames=fields,\n",
    "                                    delimiter=',', \n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_MINIMAL )\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(list_to_write)\n",
    "\n",
    "def parse_predictions(prediction_file_path, download=False):\n",
    "    \n",
    "    with open(prediction_file_path) as f:\n",
    "      predictions = json.load(f)\n",
    "\n",
    "\n",
    "    pred_list_test = [{ 'model_display_name': x['name'], \n",
    "      'model_name': x['metadata']['name'], \n",
    "      'description': x['metadata']['description'], \n",
    "      'uuid': x['metadata']['uuid'],\n",
    "      'testbed': x['testbed'],\n",
    "      'predictions': x['predictions']\n",
    "\n",
    "     } for x in predictions]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for r in predictions:\n",
    "\n",
    "      display_name = r['name']\n",
    "      model_name = r['metadata']['name']\n",
    "      description = r['metadata']['description']\n",
    "      uuid = r['metadata']['uuid']\n",
    "      testbed = r['testbed']\n",
    "\n",
    "      for k1, v1 in r['predictions'].items():\n",
    "        if k1 in (SET_NAMES):\n",
    "          if 'bundle' in v1.keys():\n",
    "            test_set = k1\n",
    "            bundle = v1['bundle']\n",
    "\n",
    "            for k2, v2 in v1['data'].items():\n",
    "              qid = k2\n",
    "              predicted_answer = v2\n",
    "              exact_match = v1['scores'][qid]['exact_match']\n",
    "              f1 = v1['scores'][qid]['f1']\n",
    "\n",
    "              pred_list.append( {\n",
    "                'display_name': display_name,\n",
    "                'model_name': model_name,\n",
    "                'description': description,\n",
    "                'uuid': uuid,\n",
    "                'testbed': testbed,\n",
    "                'test_set': test_set,\n",
    "                'qid': qid,\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': float(f1)\n",
    "              })\n",
    "   \n",
    "    return pred_list\n",
    "\n",
    "def load_data(input_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        return [r for r in csv_reader]\n",
    "\n",
    "def parse_answers(answer_file_path):\n",
    "    test_set_answers = [a for a in os.listdir(answer_file_path) if not os.path.isdir('/'.join([answer_file_path, a]))]\n",
    "    answers_list = []\n",
    "    \n",
    "    for f in test_set_answers:\n",
    "      with open('/'.join([TEST_SETS_PATH, f])) as fh:\n",
    "          test_set = f.split('.')[0]\n",
    "          \n",
    "          answers = json.load(fh)['data']\n",
    "          for x in answers:\n",
    "              title = x['title']\n",
    "    \n",
    "              for p in x['paragraphs']:\n",
    "                  context = p['context']\n",
    "    \n",
    "                  for qa in p['qas']:\n",
    "                      question = qa['question']\n",
    "                      question_id = qa['id']\n",
    "    \n",
    "                      for a in qa['answers']:\n",
    "                          answers_list.append(\n",
    "                                  {\n",
    "                                      'test_set': test_set,\n",
    "                                      'question_id': question_id,\n",
    "                                      'title': title,\n",
    "                                      'context': context,\n",
    "                                      'question_text': question,\n",
    "                                      'answer_text': a['text'],\n",
    "                                      'answer_start': a['answer_start']\n",
    "                                  }\n",
    "                              )\n",
    "    return answers_list\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(question_id, predicted_answer, all_answers):\n",
    "    gold_answers = [normalize_answer(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    return max((int(normalize_answer(predicted_answer) == a) for a in gold_answers))\n",
    "\n",
    "def compute_f1(question_id, predicted_answer, all_answers):\n",
    "    gold_toks = [get_tokens(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    pred_toks = get_tokens(predicted_answer)\n",
    "    \n",
    "    f1s = []\n",
    "  \n",
    "    for answer_toks in gold_toks:\n",
    "        common = collections.Counter(answer_toks) & collections.Counter(pred_toks)\n",
    "        num_same = sum(common.values())\n",
    "      \n",
    "        if len(answer_toks) == 0 or len(pred_toks) == 0:\n",
    "            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "            f1s.append(float(int(answer_toks == pred_toks)))\n",
    "            continue\n",
    "        if num_same == 0:\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(answer_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return float(max(f1s))\n",
    "\n",
    "def print_answer(qid, all_answers):\n",
    "    question = [q for q in all_answers if q['question_id'] == qid]\n",
    "    answers = [a['answer_text'] for a in question]\n",
    "    \n",
    "    if question:\n",
    "        print('Test Set:', question[0]['test_set'])\n",
    "        print('Context:', question[0]['context'])\n",
    "        print('Question:', question[0]['question_text'])\n",
    "        print('Answers:', answers)\n",
    "        \n",
    "def get_all_stanford_metrics(txt):\n",
    "    subtree_value = ''\n",
    "    ners = []\n",
    "    ners_count = 0\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        with CoreNLPClient(endpoint='http://localhost:{}'.format(NLP_PORT), start_server=False, timeout=30000) as client:\n",
    "\n",
    "            ann = client.annotate(txt)\n",
    "            \n",
    "            sentence_count = len(ann.sentence)\n",
    "            words = [x.word for s in ann.sentence for x in s.token if x.word not in string.punctuation]\n",
    "            word_count = len(words)\n",
    "            character_count = sum([len(x) for x in words])\n",
    "            \n",
    "            for s in ann.sentence:\n",
    "                    if s.mentions:\n",
    "                        for m in s.mentions:\n",
    "                            ners.append(m.entityType)\n",
    "                            ners_count += 1\n",
    "            \n",
    "            constituency_parse = sentence.parseTree\n",
    "            subtree_value = constituency_parse.child[0].value\n",
    "        \n",
    "        return subtree_value, ners, ners_count, sentence_count, word_count, character_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0], e.args[0], e.args[0], e.args[0], e.args[0], e.args[0]\n",
    "    \n",
    "def get_stanford_counts(txt):\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "        sentence_count = len(doc.sentences)\n",
    "        words = [w.text for s in doc.sentences for w in s.words if w.text not in string.punctuation]\n",
    "        word_count = len(words)\n",
    "        character_count = sum([len(x) for x in words])\n",
    "        \n",
    "        return sentence_count, word_count, character_count, words\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0], e.args[0], e.args[0]\n",
    "        \n",
    "\n",
    "# Take a list of words, filter out any that were in the sample, and sent to Words API.\n",
    "# All results are aggregated into a list of dicts objects.\n",
    "\n",
    "def get_wordsapi_result(word_list):\n",
    "    results = []\n",
    "    word_set = set(word_list).difference(set(wordsapi_dict.keys()))\n",
    "    for w in word_set:\n",
    "        try:\n",
    "            response = requests.request(\"GET\", word_api_url.format(w), headers=word_api_headers)\n",
    "            result_json = response.json()\n",
    "            result_json['word'] = w\n",
    "            results.append(result_json)\n",
    "        except Exception as e:\n",
    "            response = {'word': w, 'success':  False, 'message': str(e)}\n",
    "            print(w)\n",
    "    return results        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running. Found 2 sentences\n"
     ]
    }
   ],
   "source": [
    "# Test Server\n",
    "try:\n",
    "    txt = 'This is a test sentence. So is this.'\n",
    "    with CoreNLPClient(endpoint='http://localhost:{}'.format(NLP_PORT), start_server=False, timeout=30000) as client:\n",
    "        ann = client.annotate(txt)\n",
    "        print('Server running. Found {} sentences'.format(len(ann.sentence)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n"
     ]
    }
   ],
   "source": [
    "# Download the model_db.json file that contains all the pre-evaluated and scored questions\n",
    "# from the previous groups' work, if it doesn't exist yet.\n",
    "\n",
    "fetch_eval_file(PREDICTION_PATH + '/model_db.json', MODEL_EVALS_URL, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If predictions and/or answer files don't exist, uncomment these to recreate them\n",
    "\n",
    "predictions = parse_predictions(PREDICTION_PATH + '/model_db.json')\n",
    "answers = parse_answers(TEST_SETS_PATH)\n",
    "\n",
    "write_output(PREDICTION_PATH + '/all_predictions.csv', predictions)\n",
    "write_output(PREDICTION_PATH + '/all_answers.csv', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, load from files\n",
    "predictions = load_data(PREDICTION_PATH + '/all_predictions.csv')\n",
    "answers = load_data(PREDICTION_PATH + '/all_answers.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into Pandas dataframes\n",
    "\n",
    "df_pred = pd.DataFrame(predictions)\n",
    "df_answers = pd.DataFrame(answers)\n",
    "\n",
    "df_pred = df_pred.astype({'f1': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set</th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>91</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193716</th>\n",
       "      <td>train-v2</td>\n",
       "      <td>5735cc33012e2f140011a06c</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Sikhism is practiced primarily in Gurudwara at...</td>\n",
       "      <td>About how many Christian houses of worship exi...</td>\n",
       "      <td>170</td>\n",
       "      <td>728</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193719</th>\n",
       "      <td>train-v2</td>\n",
       "      <td>5735d0026c16ec1900b92817</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Institute of Medicine, the central college of ...</td>\n",
       "      <td>When did the Institute of Medicine begin to of...</td>\n",
       "      <td>1978</td>\n",
       "      <td>219</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193733</th>\n",
       "      <td>train-v2</td>\n",
       "      <td>5735d1a86c16ec1900b92832</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>The main international airport serving Kathman...</td>\n",
       "      <td>Starting in the center of Kathmandu, how many ...</td>\n",
       "      <td>6</td>\n",
       "      <td>134</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193734</th>\n",
       "      <td>train-v2</td>\n",
       "      <td>5735d1a86c16ec1900b92833</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>The main international airport serving Kathman...</td>\n",
       "      <td>How many airlines use Tribhuvan International ...</td>\n",
       "      <td>22</td>\n",
       "      <td>297</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193740</th>\n",
       "      <td>train-v2</td>\n",
       "      <td>5735d259012e2f140011a0a0</td>\n",
       "      <td>Kathmandu</td>\n",
       "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
       "      <td>In what year did Kathmandu create its initial ...</td>\n",
       "      <td>1975</td>\n",
       "      <td>199</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11464 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_set               question_id               title  \\\n",
       "66      amazon_reviews_v1  5dd4661fcc027a086d65bc77  Amazon_Reviews_530   \n",
       "67      amazon_reviews_v1  5dd4661fcc027a086d65bc77  Amazon_Reviews_530   \n",
       "68      amazon_reviews_v1  5dd4661fcc027a086d65bc77  Amazon_Reviews_530   \n",
       "151     amazon_reviews_v1  5dd4673dcc027a086d65bcec  Amazon_Reviews_295   \n",
       "152     amazon_reviews_v1  5dd4673dcc027a086d65bcec  Amazon_Reviews_295   \n",
       "...                   ...                       ...                 ...   \n",
       "193716           train-v2  5735cc33012e2f140011a06c           Kathmandu   \n",
       "193719           train-v2  5735d0026c16ec1900b92817           Kathmandu   \n",
       "193733           train-v2  5735d1a86c16ec1900b92832           Kathmandu   \n",
       "193734           train-v2  5735d1a86c16ec1900b92833           Kathmandu   \n",
       "193740           train-v2  5735d259012e2f140011a0a0           Kathmandu   \n",
       "\n",
       "                                                  context  \\\n",
       "66      i wanted an electric kettle, but landed up ord...   \n",
       "67      i wanted an electric kettle, but landed up ord...   \n",
       "68      i wanted an electric kettle, but landed up ord...   \n",
       "151     I ordered these sheets and must say was a bit ...   \n",
       "152     I ordered these sheets and must say was a bit ...   \n",
       "...                                                   ...   \n",
       "193716  Sikhism is practiced primarily in Gurudwara at...   \n",
       "193719  Institute of Medicine, the central college of ...   \n",
       "193733  The main international airport serving Kathman...   \n",
       "193734  The main international airport serving Kathman...   \n",
       "193740  Kathmandu Metropolitan City (KMC), in order to...   \n",
       "\n",
       "                                            question_text answer_text  \\\n",
       "66                        How many irritations are there?           2   \n",
       "67                        How many irritations are there?           2   \n",
       "68                        How many irritations are there?           2   \n",
       "151               what is the thread count on the sheets?        1500   \n",
       "152               what is the thread count on the sheets?        1500   \n",
       "...                                                   ...         ...   \n",
       "193716  About how many Christian houses of worship exi...         170   \n",
       "193719  When did the Institute of Medicine begin to of...        1978   \n",
       "193733  Starting in the center of Kathmandu, how many ...           6   \n",
       "193734  How many airlines use Tribhuvan International ...          22   \n",
       "193740  In what year did Kathmandu create its initial ...        1975   \n",
       "\n",
       "       answer_start  is_numeric  \n",
       "66              169        True  \n",
       "67              169        True  \n",
       "68              169        True  \n",
       "151              91        True  \n",
       "152             126        True  \n",
       "...             ...         ...  \n",
       "193716          728        True  \n",
       "193719          219        True  \n",
       "193733          134        True  \n",
       "193734          297        True  \n",
       "193740          199        True  \n",
       "\n",
       "[11464 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers['is_numeric'] = df_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)\n",
    "df_answers[df_answers['is_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_answers = pd.DataFrame({'answer_text': df_answers['answer_text'].unique()})\n",
    "df_distinct_answers[['first_parse', 'ners', 'ners_count', 'sentence_count', 'word_count', 'word_character_count', ]] = df_distinct_answers.apply(lambda row: get_all_stanford_metrics(row['answer_text']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_text</th>\n",
       "      <th>first_parse</th>\n",
       "      <th>ners_count</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_character_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ners</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CoreNLP request timed out. Your document may be too long.</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name 'sentence' is not defined</th>\n",
       "      <td>114789</td>\n",
       "      <td>114789</td>\n",
       "      <td>114789</td>\n",
       "      <td>114789</td>\n",
       "      <td>114789</td>\n",
       "      <td>114789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    answer_text  first_parse  \\\n",
       "ners                                                                           \n",
       "CoreNLP request timed out. Your document may be...            2            2   \n",
       "name 'sentence' is not defined                           114789       114789   \n",
       "\n",
       "                                                    ners_count  \\\n",
       "ners                                                             \n",
       "CoreNLP request timed out. Your document may be...           2   \n",
       "name 'sentence' is not defined                          114789   \n",
       "\n",
       "                                                    sentence_count  \\\n",
       "ners                                                                 \n",
       "CoreNLP request timed out. Your document may be...               2   \n",
       "name 'sentence' is not defined                              114789   \n",
       "\n",
       "                                                    word_count  \\\n",
       "ners                                                             \n",
       "CoreNLP request timed out. Your document may be...           2   \n",
       "name 'sentence' is not defined                          114789   \n",
       "\n",
       "                                                    word_character_count  \n",
       "ners                                                                      \n",
       "CoreNLP request timed out. Your document may be...                     2  \n",
       "name 'sentence' is not defined                                    114789  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct_answers.fillna(value = {'ners':'_NO_NER'}).groupby(['ners']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context = df_answers[['test_set','context']].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context[['sentence_count', 'word_count', 'word_character_count', 'words']] = df_distinct_context.apply(lambda row: get_stanford_counts(row['context']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Words API to Get Better Syllable Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>syllables</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in-between</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>yo-yo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>twenty-third</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>clean-living</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>tip-off</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50013</th>\n",
       "      <td>anti-catholicism</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50058</th>\n",
       "      <td>anti-french</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50061</th>\n",
       "      <td>hurdy-gurdy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50186</th>\n",
       "      <td>child-bearing</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50279</th>\n",
       "      <td>off-color</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>925 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   word  syllables\n",
       "1            in-between          2\n",
       "14                yo-yo          1\n",
       "77         twenty-third          2\n",
       "114        clean-living          2\n",
       "166             tip-off          1\n",
       "...                 ...        ...\n",
       "50013  anti-catholicism          5\n",
       "50058       anti-french          2\n",
       "50061       hurdy-gurdy          3\n",
       "50186     child-bearing          2\n",
       "50279         off-color          2\n",
       "\n",
       "[925 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word lists are already in the distinct_in_test_lower column of the df_test_sets dataframe from above.\n",
    "# Iterate through all of those and write the results to files.\n",
    "\n",
    "### Commented out to avoid accidental re-run\n",
    "\n",
    "# for test_set in df_test_sets['test_set'].unique():\n",
    "#     word_list = list(df_test_sets[df_test_sets['test_set'] == test_set]['distinct_in_test_lower'].values[0])\n",
    "#     results = get_wordsapi_result(word_list)\n",
    "#     with open('datafiles/wordsapi_{}.json'.format(test_set), 'w', encoding='utf8') as f_out:\n",
    "#         json.dump(results, f_out)\n",
    "\n",
    "# After all the API calls complete, read the output files and combine the results into a new dataframe: syll_df\n",
    "\n",
    "combined_results = []\n",
    "for f in os.listdir('datafiles/'):\n",
    "    \n",
    "    # Filter out the sample json file originally provided (it's formatted a little differently).\n",
    "    # Also filtering out the squad results... not necessary the first time this runs, given it wouldn't yet exist.\n",
    "    \n",
    "    if f not in ('wordsapi_sample.json', 'wordsapi_squad.json'):\n",
    "        with open('datafiles/{}'.format(f), 'r', encoding='utf8') as f_open:\n",
    "            x = json.load(f_open)\n",
    "            combined_results += x\n",
    "            \n",
    "syll_df_2 = pd.DataFrame(x)\n",
    "\n",
    "# For this, we only care about the syllables data, so we filter out any rows that didn't have it\n",
    "# (Words API claims that 44% of their words have syllable info)\n",
    "\n",
    "# We also just pull the word and syllables columns, then replace the existing syllables columns with just the syllable count\n",
    "syll_df_2 = syll_df_2[~syll_df_2['syllables'].isna()][['word','syllables']].reset_index(drop=True)\n",
    "syll_df_2['syllables'] = syll_df_2.apply(lambda row: row['syllables']['count'], axis = 1)            \n",
    "\n",
    "# Parse and filter the original sample file. The data here is formatted a little differently, so we don't reuse the previous logic.\n",
    "\n",
    "with open('datafiles/{}'.format('wordsapi_sample.json'), 'r', encoding='utf8') as f_open:\n",
    "    x = json.load(f_open)\n",
    "\n",
    "syll_df_2 = pd.concat([syll_df_2, pd.DataFrame([{'word': k, 'syllables': v['syllables']['count']} for k, v in x.items() if 'syllables' in x[k].keys()])]).reset_index(drop=True)   \n",
    "\n",
    "# Now to get the SQuAD words we need. Again, don't want to waste API calls on words we've already checked, so we filter those out first.\n",
    "\n",
    "# We have the list FreqDist of SQuAD words already in squad_freqdist_lower from before, so we can use that to get the list of words. \n",
    "squad_minus_checked = list(set(squad_freqdist_lower.keys()).difference(set(syll_df_2['word'].to_list())))\n",
    "\n",
    "# Then we can pass that same list to our get_wordsapi_result() function from before... we'll write the results to a file again.\n",
    "\n",
    "### Commented out to avoid accidental re-run\n",
    "\n",
    "# results = get_wordsapi_result(squad_minus_checked)\n",
    "# with open('datafiles/wordsapi_squad.json', 'w', encoding='utf8') as f_out:\n",
    "#     json.dump(results, f_out)\n",
    "\n",
    "# We can pull the data back in using the same method used for the other API results, and concat it with the existing dataframe.\n",
    "\n",
    "with open('datafiles/wordsapi_{}.json'.format('squad'), 'r', encoding='utf8') as f_open:\n",
    "    x = json.load(f_open)\n",
    "    \n",
    "df_squad_syll = pd.DataFrame(x)\n",
    "df_squad_syll = df_squad_syll[~df_squad_syll['syllables'].isna()][['word','syllables']].reset_index(drop=True)\n",
    "df_squad_syll['syllables'] = df_squad_syll.apply(lambda row: row['syllables']['count'], axis = 1)        \n",
    "\n",
    "syll_df_2 = pd.concat([syll_df_2, df_squad_syll]).reset_index(drop=True) \n",
    "\n",
    "# Finally, we found that the Words API consistently counts 1 syllable too few for hyphenated words...\n",
    "display(syll_df_2[syll_df_2['word'].str.contains('-')])\n",
    "\n",
    "# ... so we increment their counts by one to compensate.\n",
    "\n",
    "syll_df_2.loc[syll_df_2['word'].str.contains('-'), 'syllables'] = syll_df_2['syllables'] + 1 \n",
    "\n",
    "# And we write the results so we don't have to do this again...\n",
    "syll_df_2 = syll_df_2.drop_duplicates().reset_index(drop=True)\n",
    "syll_df_2.to_csv('./syllable_counts.csv', index=False)\n",
    "\n",
    "# Now we need to update the df_distinct_context syllables counts and re-populate any dependent metrics\n",
    "syll_df_2 = pd.read_csv('./syllable_counts.csv', keep_default_na=False)\n",
    "syll_2_dict = syll_df_2.set_index('word').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_syllables(word):\n",
    "    if word in syll_2_dict.keys():\n",
    "        syll_count =  syll_2_dict['word']['syllables']\n",
    "    elif len(word) < 100:\n",
    "        syll_count = len(Hyphenator('en_US').syllables(word))\n",
    "    else:\n",
    "        syll_count = 0\n",
    "    \n",
    "    return syll_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.map_partitions(lambda df: df.apply(lambda row: [max(1, len(Hyphenator('en_US').syllables(x))) if len(str(x)) < 100 else -1 for x in row['words'] ], axis = 1)) \\\n",
    "\n",
    "syll_df = dd.from_pandas(df_distinct_context, npartitions = 2*multiprocessing.cpu_count()) \\\n",
    "            .map_partitions(lambda df: df.apply(lambda row: [get_syllables(x) for x in row['words'] ], axis = 1)) \\\n",
    "            .compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['syllables_per_word'] = syll_df\n",
    "\n",
    "df_distinct_context['polysyllable_count'] = df_distinct_context.apply(lambda row: len([x for x in row['syllables_per_word'] if x > 1]), axis = 1)\n",
    "df_distinct_context['avg_word_length'] = df_distinct_context.apply(lambda row: sum([len(x) for x in row['words']])/row['word_count'], axis = 1)\n",
    "\n",
    "df_distinct_context['avg_sentence_length_in_words'] = df_distinct_context['word_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['context_character_count'] = df_distinct_context.apply(lambda row: len(row['context']), axis=1)\n",
    "df_distinct_context['avg_sentence_length_in_characters'] = df_distinct_context['context_character_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['syllables_per_word'] = df_distinct_context.apply(lambda row: sum([x for x in row['syllables_per_word'] if x > 0])/ len([x for x in row['syllables_per_word'] if x > 0]) , axis=1)\n",
    "df_distinct_context['flesch-kincaid_grade_level'] = df_distinct_context.apply(lambda row: (0.39 * row['avg_sentence_length_in_words']) + (11.8 * row['syllables_per_word']) - 15.59, axis=1)\n",
    "\n",
    "df_distinct_context['coleman-liau'] = df_distinct_context.apply(lambda row: (0.0588 * (row['avg_word_length']) * 100) - (0.296 * (100/row['avg_sentence_length_in_words'])) - 15.8, axis=1)\n",
    "df_distinct_context['gunning-fog'] = df_distinct_context.apply(lambda row: 0.4 * ((row['word_count'] / row['sentence_count']) + ((row['polysyllable_count'] / row['word_count']) * 100)), axis=1)\n",
    "df_distinct_context['automated-readability'] = df_distinct_context.apply(lambda row: 4.71 * (row['context_character_count'] / row['word_count']) + 0.5 * (row['word_count'] / row['sentence_count']) - 21.43, axis=1)\n",
    "\n",
    "df_distinct_context['lexical_diversity'] = df_distinct_context.apply(lambda row: len(set(row['words']))/row['word_count'], axis=1)\n",
    "df_distinct_context['lexical_diversity_lower'] = df_distinct_context.apply(lambda row: len(set([x.lower() for x in row['words']]))/len([x.lower() for x in row['words']]), axis=1)\n",
    "\n",
    "df_distinct_context['nltk_sentence_count'] = df_distinct_context.apply(lambda row: len(nltk.FreqDist(nltk.tokenize.sent_tokenize(row['context'].lower()))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers = df_answers.merge(df_distinct_answers, on=['answer_text'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers['is_numeric'] = df_merged_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers_and_context = df_merged_answers.merge(df_distinct_context, on=['context'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context = df_pred.merge(df_merged_answers_and_context, left_on=['qid'], right_on=['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context['exact_match'] = df_pred_answers_context['exact_match'].map({'True':True, 'False':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers.to_csv('answers.csv', index=False)\n",
    "df_distinct_answers.to_csv('distinct_answers.csv', index=False)\n",
    "df_distinct_context.to_csv('distinct_context.csv', index=False)\n",
    "df_merged_answers.to_csv('merged_answers.csv', index=False)\n",
    "df_merged_answers_and_context.to_csv('merged_answers_and_context.csv', index = False)\n",
    "df_pred_answers_context.to_csv('pred_answers_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_answers\n",
    "del df_distinct_answers\n",
    "del df_distinct_context\n",
    "del df_merged_answers\n",
    "del df_merged_answers_and_context\n",
    "del df_pred_answers_context\n",
    "del df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_answers = pd.read_csv('answers.csv')\n",
    "# df_distinct_answers = pd.read_csv('distinct_answers.csv')\n",
    "# df_distinct_context = pd.read_csv('distinct_context.csv')\n",
    "# df_merged_answers = pd.read_csv('merged_answers.csv')\n",
    "# df_merged_answers_and_context = pd.read_csv('merged_answers_and_context.csv')\n",
    "# df_pred_answers_context = pd.read_csv('pred_answers_context.csv')\n",
    "# df_pred = pd.DataFrame(load_data(PREDICTION_PATH + '/all_predictions.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
