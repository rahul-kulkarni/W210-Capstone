{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Stanford CoreNLP server\n",
    "`java -Xmx16g -cp C:\\stanford-corenlp-latest\\stanford-corenlp-4.0.0\\* edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9001 -timeout 600 -threads 5 -maxCharLength 100000 -quiet False -preload tokenize,ssplit,pos,lemma,ner,parse,depparse,coref`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import collections\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import json\n",
    "from hyphen import Hyphenator\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import multiprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import stanfordnlp\n",
    "from stanfordnlp.server import CoreNLPClient\n",
    "\n",
    "# Uncomment if needed to fix this error:\n",
    "# OMP: Error #15: Initializing libiomp5md.dll, but found libiomp5md.dll already initialized.\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_PATH = './predictions'\n",
    "TEST_SETS_PATH = './test_sets'\n",
    "MODEL_EVALS_URL = 'https://squad-model-evals.s3-us-west-2.amazonaws.com/model_db.json'\n",
    "\n",
    "#SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT', 'dev-v1.1']\n",
    "SET_NAMES = ['Amazon', 'Reddit', 'New-Wiki', 'NYT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_eval_file(eval_file_path, model_evals_url, overwrite=False):\n",
    "    if (not os.path.exists(eval_file_path)) or overwrite:\n",
    "        r = requests.get(model_evals_url)\n",
    "                        \n",
    "        with open(eval_file_path, 'w', encoding='utf-8') as outfile:\n",
    "            outfile.write(r.text)\n",
    "\n",
    "    else:\n",
    "        print('File Exists')\n",
    "    \n",
    "    \n",
    "\n",
    "def write_output(output_file_path, list_to_write):\n",
    "    fields = list_to_write[0].keys()\n",
    "    \n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        csv_writer = csv.DictWriter(csv_file, \n",
    "                                    fieldnames=fields,\n",
    "                                    delimiter=',', \n",
    "                                    quotechar='\"',\n",
    "                                    quoting=csv.QUOTE_MINIMAL )\n",
    "        csv_writer.writeheader()\n",
    "        csv_writer.writerows(list_to_write)\n",
    "\n",
    "def parse_predictions(prediction_file_path, download=False):\n",
    "    \n",
    "    with open(prediction_file_path) as f:\n",
    "      predictions = json.load(f)\n",
    "\n",
    "\n",
    "    pred_list_test = [{ 'model_display_name': x['name'], \n",
    "      'model_name': x['metadata']['name'], \n",
    "      'description': x['metadata']['description'], \n",
    "      'uuid': x['metadata']['uuid'],\n",
    "      'testbed': x['testbed'],\n",
    "      'predictions': x['predictions']\n",
    "\n",
    "     } for x in predictions]\n",
    "\n",
    "    pred_list = []\n",
    "\n",
    "    for r in predictions:\n",
    "\n",
    "      display_name = r['name']\n",
    "      model_name = r['metadata']['name']\n",
    "      description = r['metadata']['description']\n",
    "      uuid = r['metadata']['uuid']\n",
    "      testbed = r['testbed']\n",
    "\n",
    "      for k1, v1 in r['predictions'].items():\n",
    "        if k1 in (SET_NAMES):\n",
    "          if 'bundle' in v1.keys():\n",
    "            test_set = k1\n",
    "            bundle = v1['bundle']\n",
    "\n",
    "            for k2, v2 in v1['data'].items():\n",
    "              qid = k2\n",
    "              predicted_answer = v2\n",
    "              exact_match = v1['scores'][qid]['exact_match']\n",
    "              f1 = v1['scores'][qid]['f1']\n",
    "\n",
    "              pred_list.append( {\n",
    "                'display_name': display_name,\n",
    "                'model_name': model_name,\n",
    "                'description': description,\n",
    "                'uuid': uuid,\n",
    "                'testbed': testbed,\n",
    "                'test_set': test_set,\n",
    "                'qid': qid,\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'exact_match': exact_match,\n",
    "                'f1': float(f1)\n",
    "              })\n",
    "   \n",
    "    return pred_list\n",
    "\n",
    "def load_data(input_file_path):\n",
    "    with open(input_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_reader = csv.DictReader(csv_file)\n",
    "        return [r for r in csv_reader]\n",
    "\n",
    "def parse_answers(answer_file_path):\n",
    "    test_set_answers = [a for a in os.listdir(answer_file_path) if not os.path.isdir('/'.join([answer_file_path, a]))]\n",
    "    answers_list = []\n",
    "    \n",
    "    for f in test_set_answers:\n",
    "      with open('/'.join([TEST_SETS_PATH, f])) as fh:\n",
    "          test_set = f.split('.')[0]\n",
    "          \n",
    "          answers = json.load(fh)['data']\n",
    "          for x in answers:\n",
    "              title = x['title']\n",
    "    \n",
    "              for p in x['paragraphs']:\n",
    "                  context = p['context']\n",
    "    \n",
    "                  for qa in p['qas']:\n",
    "                      question = qa['question']\n",
    "                      question_id = qa['id']\n",
    "    \n",
    "                      for a in qa['answers']:\n",
    "                          answers_list.append(\n",
    "                                  {\n",
    "                                      'test_set': test_set,\n",
    "                                      'question_id': question_id,\n",
    "                                      'title': title,\n",
    "                                      'context': context,\n",
    "                                      'question_text': question,\n",
    "                                      'answer_text': a['text'],\n",
    "                                      'answer_start': a['answer_start']\n",
    "                                  }\n",
    "                              )\n",
    "    return answers_list\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  \n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s: return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(question_id, predicted_answer, all_answers):\n",
    "    gold_answers = [normalize_answer(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    return max((int(normalize_answer(predicted_answer) == a) for a in gold_answers))\n",
    "\n",
    "def compute_f1(question_id, predicted_answer, all_answers):\n",
    "    gold_toks = [get_tokens(x['answer_text']) for x in all_answers if x['question_id'] == question_id]\n",
    "    pred_toks = get_tokens(predicted_answer)\n",
    "    \n",
    "    f1s = []\n",
    "  \n",
    "    for answer_toks in gold_toks:\n",
    "        common = collections.Counter(answer_toks) & collections.Counter(pred_toks)\n",
    "        num_same = sum(common.values())\n",
    "      \n",
    "        if len(answer_toks) == 0 or len(pred_toks) == 0:\n",
    "            # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "            f1s.append(float(int(answer_toks == pred_toks)))\n",
    "            continue\n",
    "        if num_same == 0:\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "            \n",
    "        precision = 1.0 * num_same / len(pred_toks)\n",
    "        recall = 1.0 * num_same / len(answer_toks)\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "        f1s.append(f1)\n",
    "\n",
    "    return float(max(f1s))\n",
    "\n",
    "def print_answer(qid, all_answers):\n",
    "    question = [q for q in all_answers if q['question_id'] == qid]\n",
    "    answers = [a['answer_text'] for a in question]\n",
    "    \n",
    "    if question:\n",
    "        print('Test Set:', question[0]['test_set'])\n",
    "        print('Context:', question[0]['context'])\n",
    "        print('Question:', question[0]['question_text'])\n",
    "        print('Answers:', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CoreNLP request timed out. Your document may be too long.\n"
     ]
    }
   ],
   "source": [
    "# Test Server\n",
    "try:\n",
    "    txt = 'This is a test sentence. So is this.'\n",
    "    with CoreNLPClient(endpoint='http://localhost:9001', start_server=False, timeout=30000) as client:\n",
    "        ann = client.annotate(txt)\n",
    "        print('Server running. Found {} sentences'.format(len(ann.sentence)))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Exists\n"
     ]
    }
   ],
   "source": [
    "# Download the model_db.json file that contains all the pre-evaluated and scored questions\n",
    "# from the previous groups' work, if it doesn't exist yet.\n",
    "\n",
    "fetch_eval_file(PREDICTION_PATH + '/model_db.json', MODEL_EVALS_URL, overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If predictions and/or answer files don't exist, uncomment these to recreate them\n",
    "\n",
    "predictions = parse_predictions(PREDICTION_PATH + '/model_db.json')\n",
    "answers = parse_answers(TEST_SETS_PATH)\n",
    "\n",
    "write_output(PREDICTION_PATH + '/all_predictions.csv', predictions)\n",
    "write_output(PREDICTION_PATH + '/all_answers.csv', answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Otherwise, load from files\n",
    "predictions = load_data(PREDICTION_PATH + '/all_predictions.csv')\n",
    "answers = load_data(PREDICTION_PATH + '/all_answers.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load into Pandas dataframes\n",
    "\n",
    "df_pred = pd.DataFrame(predictions)\n",
    "df_answers = pd.DataFrame(answers)\n",
    "\n",
    "df_pred = df_pred.astype({'f1': 'float'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set</th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4661fcc027a086d65bc77</td>\n",
       "      <td>Amazon_Reviews_530</td>\n",
       "      <td>i wanted an electric kettle, but landed up ord...</td>\n",
       "      <td>How many irritations are there?</td>\n",
       "      <td>2</td>\n",
       "      <td>169</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>91</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd4673dcc027a086d65bcec</td>\n",
       "      <td>Amazon_Reviews_295</td>\n",
       "      <td>I ordered these sheets and must say was a bit ...</td>\n",
       "      <td>what is the thread count on the sheets?</td>\n",
       "      <td>1500</td>\n",
       "      <td>126</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106622</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9c9bbd8ae5305bc982f410</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>Sales Professional Development Help? Hello All...</td>\n",
       "      <td>How many people does the company that I work f...</td>\n",
       "      <td>8</td>\n",
       "      <td>302</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106688</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9ca3b18ae5305bc982f454</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...</td>\n",
       "      <td>How many did he play in?</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106689</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9ca3b18ae5305bc982f454</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...</td>\n",
       "      <td>How many did he play in?</td>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106760</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9cae448ae5305bc982f492</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>[AMA Request] A scalper / secondary-market tic...</td>\n",
       "      <td>What's the total number of questions the autho...</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106761</th>\n",
       "      <td>reddit_v1</td>\n",
       "      <td>5d9cae448ae5305bc982f492</td>\n",
       "      <td>Filtered_Reddit_Comments</td>\n",
       "      <td>[AMA Request] A scalper / secondary-market tic...</td>\n",
       "      <td>What's the total number of questions the autho...</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4795 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 test_set               question_id                     title  \\\n",
       "66      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "67      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "68      amazon_reviews_v1  5dd4661fcc027a086d65bc77        Amazon_Reviews_530   \n",
       "151     amazon_reviews_v1  5dd4673dcc027a086d65bcec        Amazon_Reviews_295   \n",
       "152     amazon_reviews_v1  5dd4673dcc027a086d65bcec        Amazon_Reviews_295   \n",
       "...                   ...                       ...                       ...   \n",
       "106622          reddit_v1  5d9c9bbd8ae5305bc982f410  Filtered_Reddit_Comments   \n",
       "106688          reddit_v1  5d9ca3b18ae5305bc982f454  Filtered_Reddit_Comments   \n",
       "106689          reddit_v1  5d9ca3b18ae5305bc982f454  Filtered_Reddit_Comments   \n",
       "106760          reddit_v1  5d9cae448ae5305bc982f492  Filtered_Reddit_Comments   \n",
       "106761          reddit_v1  5d9cae448ae5305bc982f492  Filtered_Reddit_Comments   \n",
       "\n",
       "                                                  context  \\\n",
       "66      i wanted an electric kettle, but landed up ord...   \n",
       "67      i wanted an electric kettle, but landed up ord...   \n",
       "68      i wanted an electric kettle, but landed up ord...   \n",
       "151     I ordered these sheets and must say was a bit ...   \n",
       "152     I ordered these sheets and must say was a bit ...   \n",
       "...                                                   ...   \n",
       "106622  Sales Professional Development Help? Hello All...   \n",
       "106688  David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...   \n",
       "106689  David Pastrnak's WJC is over. 19 SOG, 1 G, 3 A...   \n",
       "106760  [AMA Request] A scalper / secondary-market tic...   \n",
       "106761  [AMA Request] A scalper / secondary-market tic...   \n",
       "\n",
       "                                            question_text answer_text  \\\n",
       "66                        How many irritations are there?           2   \n",
       "67                        How many irritations are there?           2   \n",
       "68                        How many irritations are there?           2   \n",
       "151               what is the thread count on the sheets?        1500   \n",
       "152               what is the thread count on the sheets?        1500   \n",
       "...                                                   ...         ...   \n",
       "106622  How many people does the company that I work f...           8   \n",
       "106688                           How many did he play in?           4   \n",
       "106689                           How many did he play in?           4   \n",
       "106760  What's the total number of questions the autho...           5   \n",
       "106761  What's the total number of questions the autho...           5   \n",
       "\n",
       "       answer_start  is_numeric  \n",
       "66              169        True  \n",
       "67              169        True  \n",
       "68              169        True  \n",
       "151              91        True  \n",
       "152             126        True  \n",
       "...             ...         ...  \n",
       "106622          302        True  \n",
       "106688           99        True  \n",
       "106689           99        True  \n",
       "106760           60        True  \n",
       "106761           60        True  \n",
       "\n",
       "[4795 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers['is_numeric'] = df_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)\n",
    "df_answers[df_answers['is_numeric']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: gpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\Justin\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(processors='tokenize', use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stanford_metrics(txt):\n",
    "    subtree_value = ''\n",
    "    ner = '_NO_NER'\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        with CoreNLPClient(endpoint='http://localhost:9001', start_server=False, timeout=30000) as client:\n",
    "\n",
    "            ann = client.annotate(txt)\n",
    "            \n",
    "            sentence_count = len(ann.sentence)\n",
    "            words = [x.word for s in ann.sentence for x in s.token if x.word not in string.punctuation]\n",
    "            word_count = len(words)\n",
    "            character_count = sum([len(x) for x in words])\n",
    "            \n",
    "            sentence = ann.sentence[0]\n",
    "            if sentence.mentions:\n",
    "                ner = sentence.mentions[0].entityType\n",
    "            \n",
    "            constituency_parse = sentence.parseTree\n",
    "            subtree_value = constituency_parse.child[0].value\n",
    "        \n",
    "        return subtree_value, ner, sentence_count, word_count, character_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0],e.args[0], e.args[0], e.args[0],e.args[0]\n",
    "    \n",
    "def get_stanford_counts(txt):\n",
    "    sentence_count = 0\n",
    "    word_count = 0 \n",
    "    character_count = 0\n",
    "    \n",
    "    try:\n",
    "        doc = nlp(txt)\n",
    "        sentence_count = len(doc.sentences)\n",
    "        words = [w.text for s in doc.sentences for w in s.words if w.text not in string.punctuation]\n",
    "        word_count = len(words)\n",
    "        character_count = sum([len(x) for x in words])\n",
    "        \n",
    "        return sentence_count, word_count, character_count, words\n",
    "        \n",
    "    except Exception as e:\n",
    "        return e.args[0], e.args[0], e.args[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_answers = pd.DataFrame({'answer_text': df_answers['answer_text'].unique()})\n",
    "df_distinct_answers[['first_parse', 'first_ner', 'sentence_count', 'word_count', 'word_character_count', ]] = df_distinct_answers.apply(lambda row: get_all_stanford_metrics(row['answer_text']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_text</th>\n",
       "      <th>first_parse</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>word_character_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first_ner</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CAUSE_OF_DEATH</th>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "      <td>386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CITY</th>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "      <td>581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COUNTRY</th>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIMINAL_CHARGE</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CoreNLP request timed out. Your document may be too long.</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <td>1933</td>\n",
       "      <td>1933</td>\n",
       "      <td>1933</td>\n",
       "      <td>1933</td>\n",
       "      <td>1933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DURATION</th>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "      <td>1231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HANDLE</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IDEOLOGY</th>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOCATION</th>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONEY</th>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NATIONALITY</th>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NUMBER</th>\n",
       "      <td>4522</td>\n",
       "      <td>4522</td>\n",
       "      <td>4522</td>\n",
       "      <td>4522</td>\n",
       "      <td>4522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORDINAL</th>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORGANIZATION</th>\n",
       "      <td>1760</td>\n",
       "      <td>1760</td>\n",
       "      <td>1760</td>\n",
       "      <td>1760</td>\n",
       "      <td>1760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT</th>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "      <td>369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERSON</th>\n",
       "      <td>4894</td>\n",
       "      <td>4894</td>\n",
       "      <td>4894</td>\n",
       "      <td>4894</td>\n",
       "      <td>4894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RELIGION</th>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SET</th>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STATE_OR_PROVINCE</th>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIME</th>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TITLE</th>\n",
       "      <td>1751</td>\n",
       "      <td>1751</td>\n",
       "      <td>1751</td>\n",
       "      <td>1751</td>\n",
       "      <td>1751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URL</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_NO_NER</th>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "      <td>31029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>list index out of range</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    answer_text  first_parse  \\\n",
       "first_ner                                                                      \n",
       "CAUSE_OF_DEATH                                              386          386   \n",
       "CITY                                                        581          581   \n",
       "COUNTRY                                                     604          604   \n",
       "CRIMINAL_CHARGE                                             166          166   \n",
       "CoreNLP request timed out. Your document may be...           19           19   \n",
       "DATE                                                       1933         1933   \n",
       "DURATION                                                   1231         1231   \n",
       "HANDLE                                                        6            6   \n",
       "IDEOLOGY                                                    264          264   \n",
       "LOCATION                                                    610          610   \n",
       "MISC                                                        635          635   \n",
       "MONEY                                                       640          640   \n",
       "NATIONALITY                                                 676          676   \n",
       "NUMBER                                                     4522         4522   \n",
       "ORDINAL                                                     504          504   \n",
       "ORGANIZATION                                               1760         1760   \n",
       "PERCENT                                                     369          369   \n",
       "PERSON                                                     4894         4894   \n",
       "RELIGION                                                    140          140   \n",
       "SET                                                         182          182   \n",
       "STATE_OR_PROVINCE                                           302          302   \n",
       "TIME                                                        214          214   \n",
       "TITLE                                                      1751         1751   \n",
       "URL                                                           3            3   \n",
       "_NO_NER                                                   31029        31029   \n",
       "list index out of range                                       1            1   \n",
       "\n",
       "                                                    sentence_count  \\\n",
       "first_ner                                                            \n",
       "CAUSE_OF_DEATH                                                 386   \n",
       "CITY                                                           581   \n",
       "COUNTRY                                                        604   \n",
       "CRIMINAL_CHARGE                                                166   \n",
       "CoreNLP request timed out. Your document may be...              19   \n",
       "DATE                                                          1933   \n",
       "DURATION                                                      1231   \n",
       "HANDLE                                                           6   \n",
       "IDEOLOGY                                                       264   \n",
       "LOCATION                                                       610   \n",
       "MISC                                                           635   \n",
       "MONEY                                                          640   \n",
       "NATIONALITY                                                    676   \n",
       "NUMBER                                                        4522   \n",
       "ORDINAL                                                        504   \n",
       "ORGANIZATION                                                  1760   \n",
       "PERCENT                                                        369   \n",
       "PERSON                                                        4894   \n",
       "RELIGION                                                       140   \n",
       "SET                                                            182   \n",
       "STATE_OR_PROVINCE                                              302   \n",
       "TIME                                                           214   \n",
       "TITLE                                                         1751   \n",
       "URL                                                              3   \n",
       "_NO_NER                                                      31029   \n",
       "list index out of range                                          1   \n",
       "\n",
       "                                                    word_count  \\\n",
       "first_ner                                                        \n",
       "CAUSE_OF_DEATH                                             386   \n",
       "CITY                                                       581   \n",
       "COUNTRY                                                    604   \n",
       "CRIMINAL_CHARGE                                            166   \n",
       "CoreNLP request timed out. Your document may be...          19   \n",
       "DATE                                                      1933   \n",
       "DURATION                                                  1231   \n",
       "HANDLE                                                       6   \n",
       "IDEOLOGY                                                   264   \n",
       "LOCATION                                                   610   \n",
       "MISC                                                       635   \n",
       "MONEY                                                      640   \n",
       "NATIONALITY                                                676   \n",
       "NUMBER                                                    4522   \n",
       "ORDINAL                                                    504   \n",
       "ORGANIZATION                                              1760   \n",
       "PERCENT                                                    369   \n",
       "PERSON                                                    4894   \n",
       "RELIGION                                                   140   \n",
       "SET                                                        182   \n",
       "STATE_OR_PROVINCE                                          302   \n",
       "TIME                                                       214   \n",
       "TITLE                                                     1751   \n",
       "URL                                                          3   \n",
       "_NO_NER                                                  31029   \n",
       "list index out of range                                      1   \n",
       "\n",
       "                                                    word_character_count  \n",
       "first_ner                                                                 \n",
       "CAUSE_OF_DEATH                                                       386  \n",
       "CITY                                                                 581  \n",
       "COUNTRY                                                              604  \n",
       "CRIMINAL_CHARGE                                                      166  \n",
       "CoreNLP request timed out. Your document may be...                    19  \n",
       "DATE                                                                1933  \n",
       "DURATION                                                            1231  \n",
       "HANDLE                                                                 6  \n",
       "IDEOLOGY                                                             264  \n",
       "LOCATION                                                             610  \n",
       "MISC                                                                 635  \n",
       "MONEY                                                                640  \n",
       "NATIONALITY                                                          676  \n",
       "NUMBER                                                              4522  \n",
       "ORDINAL                                                              504  \n",
       "ORGANIZATION                                                        1760  \n",
       "PERCENT                                                              369  \n",
       "PERSON                                                              4894  \n",
       "RELIGION                                                             140  \n",
       "SET                                                                  182  \n",
       "STATE_OR_PROVINCE                                                    302  \n",
       "TIME                                                                 214  \n",
       "TITLE                                                               1751  \n",
       "URL                                                                    3  \n",
       "_NO_NER                                                            31029  \n",
       "list index out of range                                                1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distinct_answers.fillna(value = {'first_ner':'_NO_NER'}).groupby(['first_ner']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context = df_answers[['test_set','context']].drop_duplicates().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context[['sentence_count', 'word_count', 'word_character_count', 'words']] = df_distinct_context.apply(lambda row: get_stanford_counts(row['context']), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "syll_df = dd.from_pandas(df_distinct_context, npartitions = 2*multiprocessing.cpu_count()) \\\n",
    "            .map_partitions(lambda df: df.apply(lambda row: [max(1, len(Hyphenator('en_US').syllables(x))) if len(str(x)) < 100 else -1 for x in row['words'] ], axis = 1)) \\\n",
    "            .compute(scheduler='processes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['syllables_per_word'] = syll_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['polysyllable_count'] = df_distinct_context.apply(lambda row: len([x for x in row['syllables_per_word'] if x > 1]), axis = 1)\n",
    "df_distinct_context['avg_word_length'] = df_distinct_context.apply(lambda row: sum([len(x) for x in row['words']])/row['word_count'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['avg_sentence_length_in_words'] = df_distinct_context['word_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['context_character_count'] = df_distinct_context.apply(lambda row: len(row['context']), axis=1)\n",
    "df_distinct_context['avg_sentence_length_in_characters'] = df_distinct_context['context_character_count']/df_distinct_context['sentence_count']\n",
    "df_distinct_context['syllables_per_word'] = df_distinct_context.apply(lambda row: sum([x for x in row['syllables_per_word'] if x > 0])/ len([x for x in row['syllables_per_word'] if x > 0]) , axis=1)\n",
    "df_distinct_context['flesch-kincaid_grade_level'] = df_distinct_context.apply(lambda row: (0.39 * row['avg_sentence_length_in_words']) + (11.8 * row['syllables_per_word']) - 15.59, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distinct_context['coleman-liau'] = df_distinct_context.apply(lambda row: (0.0588 * (row['avg_word_length']) * 100) - (0.296 * (100/row['avg_sentence_length_in_words'])) - 15.8, axis=1)\n",
    "df_distinct_context['gunning-fog'] = df_distinct_context.apply(lambda row: 0.4 * ((row['word_count'] / row['sentence_count']) + ((row['polysyllable_count'] / row['word_count']) * 100)), axis=1)\n",
    "df_distinct_context['automated-readability'] = df_distinct_context.apply(lambda row: 4.71 * (row['context_character_count'] / row['word_count']) + 0.5 * (row['word_count'] / row['sentence_count']) - 21.43, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers = df_answers.merge(df_distinct_answers, on=['answer_text'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers['is_numeric'] = df_merged_answers.apply(lambda row: row['answer_text'].isnumeric(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged_answers_and_context = df_merged_answers.merge(df_distinct_context, on=['context'])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context = df_pred.merge(df_merged_answers_and_context, left_on=['qid'], right_on=['question_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_answers_context['exact_match'] = df_pred_answers_context['exact_match'].map({'True':True, 'False':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_answers.to_csv('answers.csv', index=False)\n",
    "df_distinct_answers.to_csv('distinct_answers.csv', index=False)\n",
    "df_distinct_context.to_csv('distinct_context.csv', index=False)\n",
    "df_merged_answers.to_csv('merged_answers.csv', index=False)\n",
    "df_merged_answers_and_context.to_csv('merged_answers_and_context.csv', index = False)\n",
    "df_pred_answers_context.to_csv('pred_answers_context.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_pred_answers_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3e1f12d4feb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_pred_answers_context\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df_pred_answers_context' is not defined"
     ]
    }
   ],
   "source": [
    "df_pred_answers_context[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justin\\Anaconda3\\envs\\w210_capstone\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (20,21,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df_answers = pd.read_csv('answers.csv')\n",
    "df_distinct_answers = pd.read_csv('distinct_answers.csv')\n",
    "df_distinct_context = pd.read_csv('distinct_context.csv')\n",
    "df_merged_answers = pd.read_csv('merged_answers.csv')\n",
    "df_merged_answers_and_context = pd.read_csv('merged_answers_and_context.csv')\n",
    "df_pred_answers_context = pd.read_csv('pred_answers_context.csv')\n",
    "df_pred = pd.DataFrame(load_data(PREDICTION_PATH + '/all_predictions.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_set</th>\n",
       "      <th>question_id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question_text</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>is_numeric</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>Amazon_Reviews_2030</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>What size is the holder?</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>Amazon_Reviews_2030</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>What size is the holder?</td>\n",
       "      <td>not too big and not too small</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd465dacc027a086d65bc6c</td>\n",
       "      <td>Amazon_Reviews_2030</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>What size is the holder?</td>\n",
       "      <td>too big and not too small</td>\n",
       "      <td>30</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd465dacc027a086d65bc6d</td>\n",
       "      <td>Amazon_Reviews_2030</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>What does it fit nicely?</td>\n",
       "      <td>any lipstick, lip gloss, chapstick</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>amazon_reviews_v1</td>\n",
       "      <td>5dd465dacc027a086d65bc6d</td>\n",
       "      <td>Amazon_Reviews_2030</td>\n",
       "      <td>It's a very nice holder - not too big and not ...</td>\n",
       "      <td>What does it fit nicely?</td>\n",
       "      <td>any lipstick, lip gloss, chapstick, etc</td>\n",
       "      <td>65</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            test_set               question_id                title  \\\n",
       "0  amazon_reviews_v1  5dd465dacc027a086d65bc6c  Amazon_Reviews_2030   \n",
       "1  amazon_reviews_v1  5dd465dacc027a086d65bc6c  Amazon_Reviews_2030   \n",
       "2  amazon_reviews_v1  5dd465dacc027a086d65bc6c  Amazon_Reviews_2030   \n",
       "3  amazon_reviews_v1  5dd465dacc027a086d65bc6d  Amazon_Reviews_2030   \n",
       "4  amazon_reviews_v1  5dd465dacc027a086d65bc6d  Amazon_Reviews_2030   \n",
       "\n",
       "                                             context  \\\n",
       "0  It's a very nice holder - not too big and not ...   \n",
       "1  It's a very nice holder - not too big and not ...   \n",
       "2  It's a very nice holder - not too big and not ...   \n",
       "3  It's a very nice holder - not too big and not ...   \n",
       "4  It's a very nice holder - not too big and not ...   \n",
       "\n",
       "              question_text                              answer_text  \\\n",
       "0  What size is the holder?            not too big and not too small   \n",
       "1  What size is the holder?            not too big and not too small   \n",
       "2  What size is the holder?                too big and not too small   \n",
       "3  What does it fit nicely?       any lipstick, lip gloss, chapstick   \n",
       "4  What does it fit nicely?  any lipstick, lip gloss, chapstick, etc   \n",
       "\n",
       "   answer_start  is_numeric  \n",
       "0            26       False  \n",
       "1            26       False  \n",
       "2            30       False  \n",
       "3            65       False  \n",
       "4            65       False  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
