{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Testing Tutorial\n",
    "\n",
    "Welcome to the Python Testing Tutorial notebook. This will be the resource we use to both learn, and test what we learned. This notebook is designed to be loaded to a Jupyter Lab instance that has `pytest` and `ipython_pytest` installed. In a new virtual environment, do\n",
    "\n",
    "```\n",
    "$ pip install pytest ipython_pytest jupyterlab\n",
    "```\n",
    "\n",
    "When this is done, launch Jupyter\n",
    "\n",
    "```\n",
    "$ jupyter lab\n",
    "```\n",
    "\n",
    "Click on the upload icon, and upload this notebook.\n",
    "\n",
    "The next step will be to load the `ipython_test` Jupyter extension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext ipython_pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any output from this step. If an error occured saying it is not installed, make sure the virtual environment has `ipython_test` installed.\n",
    "\n",
    "Next, we will run one test, just to see what failure looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_something():\n",
    "    assert [1] == [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** Try to run it in your environment: go into the notebook, and\n",
    "\n",
    "* Execute the `load_ext` cell, by clicking in and pressing Shift-Enter\n",
    "* Execute the `pytest` cell, by clicking in and pressing Shift-Enter.\n",
    "\n",
    "You should get a failure that looks much like the following one:\n",
    "\n",
    "```\n",
    "    def test_something():\n",
    ">       assert [1] == [2]\n",
    "E       assert [1] == [2]\n",
    "E         At index 0 diff: 1 != 2\n",
    "E         Use -v to get the full diff\n",
    "\n",
    "_ipytesttmp.py:3: AssertionError\n",
    "```\n",
    "\n",
    "If you did not, then something might not be installed and configured properly. Check again that `pytest` and `ipython_pytest` are properly installed in your virtual environment.\n",
    "\n",
    "This is our first hands-on exercise. The next few will be a little more challenging, but this one is just to make sure we have our environment set up. We will have 5 minutes to finish it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Step Back\n",
    "\n",
    "* Unit tests as code guidance\n",
    "* Unit tests as regression tests\n",
    "* pytest as a test runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \"Traditional\" unit tests\n",
    "import unittest\n",
    "\n",
    "def test(klass):\n",
    "    loader = unittest.TestLoader()\n",
    "    suite=loader.loadTestsFromTestCase(klass)\n",
    "    runner = unittest.TextTestRunner()\n",
    "    runner.run(suite)\n",
    "\n",
    "class TestSimple(unittest.TestCase):\n",
    "    def test_wrong(self):\n",
    "        self.assertEqual(1, 2)\n",
    "\n",
    "test(TestSimple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests\n",
    "\n",
    "The mechanics of running tests differ between environments. Usually, you will be running the `pytest` command-line -- but quite often, not directly. You might be using a `tox` runner, run the tests in a Continuous Integration environment, or maybe inside a Docker container. Regardless, most of the effort will be spent writing tests and looking at test failures -- and this is going to be what this tutorial will cover.\n",
    "\n",
    "Pytest uses the Python keyword `assert` in order to check conditions in tests. It internally processes this statement to help failures look nicer. We already saw that it does a checks the difference with a list of one element. Let's see a few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_missing():\n",
    "    assert [1] == []\n",
    "    \n",
    "def test_extra():\n",
    "    assert [1] == [1, 2]\n",
    "    \n",
    "def test_different():\n",
    "    assert [1, 2, 3] == [1, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All tests failed here -- but read the output carefully to see *how* they failed. When writing tests for new code, much of your work will be analyzing test failures to understand how they failed, and whether the test code or product code is broken. If the tests are doing their job to prevent regressions, much of your work will be looking at test failures to understand whether you need to fix the code or modify an out-of-date test.\n",
    "\n",
    "The most interesting thing about any test framework is how test failures look.\n",
    "\n",
    "Let's see how `pytest` handles other equality failures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_string():\n",
    "    assert \"hello\\nworld\" == \"goodbye\\nworld\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For strings, `pytest` will do a line-by-line diff to highlight differences. However, it will not do inside-line-diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def test_set():\n",
    "    assert set([1,2]) == set([1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sets, it will check for spurious elements on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more code we put on the assertion line, the more detailed the output will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def add(x, y):\n",
    "    return x+y\n",
    "    \n",
    "def test_add():\n",
    "    assert add(1, 2) == 4\n",
    "    \n",
    "def test_add_lessinformation():\n",
    "    n = add(1, 2)\n",
    "    assert n == 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sometimes equality is too strict: after all many functions we write, it's hard to know what the output will be equal to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import random\n",
    "\n",
    "def test_random():\n",
    "    assert random.uniform(0, 1) > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check for set membership:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import random\n",
    "\n",
    "def test_choice():\n",
    "    assert random.choice([1, 2, 3]) in [4, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing edge-cases of code is particularly important. Specifically, we might want to test whether the code raises the right exception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_exception():\n",
    "    with pytest.raises(ValueError):\n",
    "        1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** It is time to fix some tests. In each cell, only change the line that says \"fix this line\". We will have 10 minutes to finish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def add(a, b):\n",
    "    return a # fix this line\n",
    "\n",
    "def test_add():\n",
    "    assert add(1, 2) == 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def append(a, b):\n",
    "    pass # ix this line\n",
    "    \n",
    "def test_append():\n",
    "    a = [1, 2, 3]\n",
    "    append(a, 4)\n",
    "    assert a == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def safe_remove(a, b):\n",
    "    pass # fix this line\n",
    "\n",
    "def test_safe_remove_no():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 3)\n",
    "    assert 1 in things\n",
    "\n",
    "def test_safe_remove_yes():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 2)\n",
    "    assert 2 not in things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def get_min_max(a, b):\n",
    "    return a, b # fix this line\n",
    "\n",
    "def test_min_max_high():\n",
    "    a, b = get_min_max(2, 1)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b\n",
    "\n",
    "def test_min_max_low():\n",
    "    a, b = get_min_max(1, 2)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_no_element():\n",
    "    thing = {} # fix this line\n",
    "    with pytest.raises(IndexError):\n",
    "        thing[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "* Tests check whether things are true\n",
    "* pytest uses `assert`\n",
    "* Put calculations on the `assert` line to get more information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mocking\n",
    "\n",
    "Sometimes, using real objects is hard, ill-advised, or complicated.\n",
    "\n",
    "For example, a `requests.Session` connects to real websites:\n",
    "using it in your unittests invites a...lot...of problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unittest import mock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mocks\" are a unittest concept: they produce objects that are substitutes for the real ones.\n",
    "There's a whole cottage industry that will explain that \"mock\", \"fake\", and \"stub\" are all subtly different.\n",
    "We'll use all of those interchangably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular = mock.Mock()\n",
    "\n",
    "def do_something(o):\n",
    "    return o.something(5)\n",
    "\n",
    "do_something(regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks have \"all the methods\". The methods will usually return another Mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read words from a pathlib.Path\n",
    "import contextlib\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    with contextlib.closing(path_obj.open()) as fpin:\n",
    "        result = fpin.read()\n",
    "    return result.split()\n",
    "\n",
    "words_from_path(regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want it to return a \"real\" string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import contextlib\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    with contextlib.closing(path_obj.open()) as fpin:\n",
    "        result = fpin.read()\n",
    "    return result.split()\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "my_path = mock.Mock()\n",
    "my_path.open.return_value.read.return_value = \"\"\"\\\n",
    "In winter, when the fields are white,\n",
    "I sing this song for your delight.\n",
    "In spring, when woods are getting green,\n",
    "I'll try and tell you what I mean.\n",
    "In summer, when the days are long,\n",
    "Perhaps you'll understand the song.\n",
    "In autumn, when the leaves are brown,\n",
    "Take pen and ink, and write it down.\n",
    "\"\"\"\n",
    "\n",
    "def test_jabberwocky():\n",
    "    result = words_from_path(my_path)\n",
    "    assert \"winter\" in result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks, however, lack the so-called \"magic\" methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_magic():\n",
    "    a = mock.Mock()\n",
    "    b = mock.Mock()\n",
    "    a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to support special operation like arithmetic operators, indexing, or any other syntax,\n",
    "we will need `MagicMock`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_magic():\n",
    "    a = mock.MagicMock()\n",
    "    b = mock.MagicMock()\n",
    "    assert a + b == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want the test to pass, we need to make sure `__add__` returns the right value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_correct_magic():\n",
    "    a = mock.MagicMock()\n",
    "    b = mock.MagicMock()\n",
    "    a.__add__.return_value = 2\n",
    "    assert a + b == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are not careful, `Mock` (and `MagicMock`) can be too \"loose\". They will let some code pass, though it should not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import contextlib\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    fpin = path_obj.open()\n",
    "    try:\n",
    "        result = fpin.read()\n",
    "    finally:\n",
    "        fpin.cloze()\n",
    "    return result.split()\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "my_path = mock.Mock()\n",
    "my_path.open.return_value.read.return_value = \"\"\"\\\n",
    "In winter, when the fields are white,\n",
    "I sing this song for your delight.\n",
    "In spring, when woods are getting green,\n",
    "I'll try and tell you what I mean.\n",
    "In summer, when the days are long,\n",
    "Perhaps you'll understand the song.\n",
    "In autumn, when the leaves are brown,\n",
    "Take pen and ink, and write it down.\n",
    "\"\"\"\n",
    "\n",
    "def test_jabberwocky():\n",
    "    result = words_from_path(my_path)\n",
    "    assert \"when\" in result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This test should not pass -- we misspelled `.close()` as `.cloze()`. But since `Mock` has all possible methods, and nobody looks at the result of `.close()` this buggy code does not pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from io import TextIOBase\n",
    "\n",
    "def words_from_path(path_obj):\n",
    "    fpin = path_obj.open()\n",
    "    try:\n",
    "        result = fpin.read()\n",
    "    finally:\n",
    "        fpin.cloze()\n",
    "    return result.split()\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "my_path = mock.Mock()\n",
    "mock_file = my_path.open.return_value = mock.Mock(spec=TextIOBase)\n",
    "mock_file.read.return_value = \"\"\"\\\n",
    "In winter, when the fields are white,\n",
    "I sing this song for your delight.\n",
    "In spring, when woods are getting green,\n",
    "I'll try and tell you what I mean.\n",
    "In summer, when the days are long,\n",
    "Perhaps you'll understand the song.\n",
    "In autumn, when the leaves are brown,\n",
    "Take pen and ink, and write it down.\n",
    "\"\"\"\n",
    "\n",
    "def test_jabberwocky():\n",
    "    result = words_from_path(my_path)\n",
    "    assert \"when\" in result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to give Mocks names. This is useful when we have more than one mock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def do_stuff(session, file):\n",
    "    from_internet = session.get(\"http://example.com\").read()\n",
    "    from_file = file.read()\n",
    "    return from_internet, from_file\n",
    "\n",
    "def test_hard_to_read():\n",
    "    result = do_stuff(mock.MagicMock(), mock.MagicMock())\n",
    "    assert result == (1, 1)\n",
    "\n",
    "def test_easy_to_read():\n",
    "    result = do_stuff(mock.MagicMock(name=\"session\"), mock.MagicMock(name=\"file\"))\n",
    "    assert result == (1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hands on**: Fix this test! Only change the marked line to get the test to pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def analyze_website(session):\n",
    "    ret = session.get(\"http://example.com\")\n",
    "    return ret[10:15]\n",
    "\n",
    "def test_analyze():\n",
    "    session = mock.MagicMock(name=\"session\")\n",
    "    pass # fix this line\n",
    "    assert analyze_website(session) == \"hello\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test-Driven Bug Fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def most_popular_word(text):\n",
    "    counter = collections.Counter(text.split())\n",
    "    return counter.most_common(n=1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_popular_word(\"hello world hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(most_popular_word(\"Hello world hello world hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import collections\n",
    "\n",
    "def most_popular_word(text):\n",
    "    counter = collections.Counter(text.split())\n",
    "    return counter.most_common(n=1)[0][0]\n",
    "\n",
    "def test_simple():\n",
    "    assert most_popular_word(\"hello world hello\") == \"hello\"\n",
    "    \n",
    "def test_mixed_case():\n",
    "    assert most_popular_word(\"Hello world hello world hello\") == \"hello\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "import collections\n",
    "\n",
    "def most_popular_word(text):\n",
    "    counter = collections.Counter(text.lower().split())\n",
    "    return counter.most_common(n=1)[0][0]\n",
    "\n",
    "def test_simple():\n",
    "    assert most_popular_word(\"hello world hello\") == \"hello\"\n",
    "    \n",
    "def test_mixed_case():    \n",
    "    assert most_popular_word(\"Hello world hello world hello\") == \"hello\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest \n",
    "\n",
    "def gaussian_formula(n):\n",
    "    return (n * (n+1)) // 2\n",
    "\n",
    "def test_guassian_formula_for_4():\n",
    "    assert sum(range(4 + 1)) == gaussian_formula(4)\n",
    "    \n",
    "def test_guassian_formula_for_5():\n",
    "    assert sum(range(5 + 1)) == gaussian_formula(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest \n",
    "\n",
    "def gaussian_formula(n):\n",
    "    return ((n+1) * n) // 2\n",
    "\n",
    "def test_guassian_formula_for_4():\n",
    "    assert sum(range(4 + 1)) == gaussian_formula(4)\n",
    "    \n",
    "def test_guassian_formula_for_5():\n",
    "    assert sum(range(5 + 1)) == gaussian_formula(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HANDS ON** Write a failing test and fix the bug\n",
    "\n",
    "Hint: what if the word is not in the text?\n",
    "\n",
    "We will have 10 minutes for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%pytest\n",
    "\n",
    "def remove_word(text, word):\n",
    "    idx = text.find(word)\n",
    "    return text[:idx] + text[idx+len(word):]\n",
    "\n",
    "def test_remove_word_simple():\n",
    "    text = \"hello friends goodbye\"\n",
    "    new_text = remove_word(text, \"friends\")\n",
    "    assert new_text == \"hello  goodbye\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* pytest\n",
    "* assertions\n",
    "* mocking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
